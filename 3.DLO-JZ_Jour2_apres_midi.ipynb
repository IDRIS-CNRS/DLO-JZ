{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLO-JZ Profiler, DataLoader - Jour 2 - après midi \n",
    "\n",
    "![car](./images/noun-car-repair-32305.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objet du notebook\n",
    "\n",
    "Le but de ce *notebook* est d'optimiser la *DataLoader* afin de ne pas ralentir la boucle d'apprentissage. L'étude de la performance des solutions optimisées se fera en visualisant les traces du *profiler* :\n",
    "* **TP 3** : Profiler\n",
    "* **TP 4** : Optimisation du DataLoader\n",
    "\n",
    "Les cellules dans ce *notebook* ne sont pas prévues pour être modifiées, sauf rares exceptions indiquées dans les commentaires. Les TP se feront en modifiant le code `dlojz.py`.\n",
    "\n",
    "Les directives de modification seront marquées par l'étiquette **TODO :** dans le *notebook* suivant.\n",
    " \n",
    "Les solutions sont présentes dans le répertoire `solutions`.\n",
    "\n",
    "*Notebook rédigé par l'équipe assistance IA de l'IDRIS, juin 2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un module PyTorch doit avoir été chargé pour le bon fonctionnement de ce Notebook. **Nécessairement**, le module `pytorch-gpu/py3/1.11.0` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions *python* de gestion de queue SLURM dévelopées par l'IDRIS et les fonctions dédiées à la formation DLO-JZ sont à importer.\n",
    "\n",
    "Le module d'environnement pour les *jobs* et la taille des images sont fixés pour ce *notebook*.\n",
    "\n",
    "**TODO :** choisir un *pseudonyme* (maximum 5 caractères) pour vous différencier dans la queue SLURM et dans les outils collaboratifs pendant la formation et la compétition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools import controle_technique, compare, GPU_underthehood, plot_accuracy, lrfind_plot, imagenet_starter, turbo_profiler\n",
    "MODULE = 'pytorch-gpu/py3/1.11.0'\n",
    "account = 'for@v100'\n",
    "name = 'pseudo'   ## Pseudonyme à choisir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion de la queue SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie permet d'afficher et de gérer la queue SLURM.\n",
    "\n",
    "Pour afficher toute la queue *utilisateur* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**: Cette fonction utilisée plusieurs fois dans ce *notebook* permet d'afficher la queue de manière dynamique, rafraichie toutes les 5 secondes. Cependant elle ne s'arrête que lorsque la queue est vide. Si vous désirez reprendre la main sur le *notebook*, il vous suffira d'arrêter manuellement la cellule avec le bouton *stop*. Cela a bien sûr aucun impact sur le *scheduler* SLURM. Les *jobs* ne seront pas arrêtés.\n",
    "\n",
    "Si vous voulez annuler un *job* dans votre queue, décommenter la ligne suivante et remplacer le numéro du *job*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!scancel 2088207"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie *debug* permet d'afficher les fichiers de sortie et les fichiers d'erreur du *job*.\n",
    "\n",
    "Il est nécessaire dans la cellule suivante d'indiquer le *jobid* correspondant sous le format donné.\n",
    "\n",
    "***Remarque*** : dans ce notebook, lorsque vous soumettrez un *job*, vous recevrez en retour le numéro du job dans le format suivant : `jobid = ['123456']`. La cellule ci-dessous peut ainsi être facilement actualisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['2088207']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier de sortie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier d'erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Différence entre deux scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le *debug* ou pour comparer son code avec les solutions mises à disposition, la fonction suivante permet d'afficher une page html contenant un différentiel de fichiers texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"dlojz.py\"\n",
    "s2 = \"./solutions/dlojz2_1.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le résultat du différentiel de fichiers sur la page suivante (attention au spoil !) :\n",
    "\n",
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Garage - Mise à niveau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe le *batch size* et la taille d'image pour ce TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_optim = 512\n",
    "image_size = 176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO :** Comparer votre script `dlojz.py` avec ce qu'il devrait être actuellement. Si il y a des divergences, veuillez les corriger (par exemple en copiant-collant la solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"dlojz.py\"\n",
    "s2 = \"./solutions/dlojz2_2.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le résultat du différentiel de fichiers sur la page suivante :\n",
    "\n",
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copier/coller la solution si nécessaire\n",
    "#!cp solutions/dlojz2_2.py dlojz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2_3 : Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du profiler PyTorch\n",
    "**TODO** : implémenter le profiler natif PyTorch dans le script `dlojz.py` :\n",
    "* Importer les fonctionnalités liées au *Profiler Pytorch*.\n",
    "\n",
    "```python\n",
    "   \tfrom torch.profiler import profile, tensorboard_trace_handler, ProfilerActivity, schedule\n",
    "```\n",
    "\n",
    "* Dans la définition des arguments du script, ajouter l'option `--prof`.\n",
    "\n",
    "```python\n",
    "    parser.add_argument('--prof', default=False, action='store_true', help='PROF implementation')\n",
    "```\n",
    "\n",
    "* Avant la boucle d'apprentissage, définir un *context* `prof` avec les paramètres du *profiler*. `contextlib.nullcontext()` définit un *context null* lorsque l'on utilise pas le *Profiler*.\n",
    "\n",
    "```python\n",
    "    # Pytorch profiler setup\n",
    "\tprof =  profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                    schedule=schedule(wait=1, warmup=1, active=12, repeat=1),\n",
    "                    on_trace_ready=tensorboard_trace_handler('./profiler/' + os.environ['SLURM_JOB_NAME'] \n",
    "                                               + '_' + os.environ['SLURM_JOBID'] + '_bs' +\n",
    "                                               str(mini_batch_size)  + '_is' + str(args.image_size)),\n",
    "                    profile_memory=True,\n",
    "                    record_shapes=False, \n",
    "                    with_stack=False,\n",
    "                    with_flops=False\n",
    "                    ) if args.prof else contextlib.nullcontext()\n",
    "```\n",
    "\n",
    "* Puis englober toute la boucle d'apprentissage (validation comprise) dans le *context* `prof`.\n",
    "\n",
    "```python\n",
    "    #### TRAINING ############\n",
    "    with prof:\n",
    "        for epoch in range(args.epochs):\n",
    "            ...\n",
    "```\n",
    "\n",
    "* Indiquer au *profiler* la fin de chaque itération d'apprentissage (avant la validation).\n",
    "```python\n",
    "    # profiler update\n",
    "    if args.prof: prof.step()\n",
    "```\n",
    "\n",
    "### Génération d'une trace profiler\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "__Remarques__ : \n",
    "* le profilage étant effectué sur une douzaine de *steps*, nous n'exécutons l'entraînement que sur 15 steps grâce à l'argument `--test-nsteps=15`\n",
    "* les arguments `--num-workers 0 --no-persistent-workers --no-pin-memory --no-non-blocking --prefetch-factor 2` utilisés dans la commande ci-dessous servent à supprimer certaines optimisations déjà présentes dans le script `dlojz.py`. Ces optimisations seront détaillées dans le prochain chapitre du cours.\n",
    "\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --test-nsteps 15 --prof'\n",
    "command += f' --num-workers 0 --no-persistent-workers --no-pin-memory --no-non-blocking --prefetch-factor 2'\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1586932']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : vérifier qu'une trace a bien été générée dans le répertoire `profiler/<name>_<jobid>_bs512_is176/` sous la forme d'un fichier `.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls profiler/{name}_{jobid[0]}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des traces profiler avec TensorBoard <a id=\"visu_tensorboard_gpu\"></a>\n",
    "\n",
    "**TODO** : visualiser cette trace grâce à l'application TensorBoard en suivant les étapes suivantes :\n",
    "* ouvrir [jupyterhub.idris.fr](https://jupyterhub.idris.fr) dans un nouvel onglet du navigateur\n",
    "* ouvrir une nouvelle instance JupyterHub en cliquant sur **Add New JupyterLab Instance**\n",
    "* sélectionner **Spawn server on SLURM node** (on va réserver un GPU)\n",
    "* sélectionner **Tensorboard** dans le menu **Frontend**\n",
    "* définir le chemin des logs **$WORK/DLO-JZ/profiler** dans **TensorBoard logs directory**\n",
    "\n",
    "<div><img src=\"images/new_spawn_tensorboard.png\" width=\"550\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque__ : le premier démarrage de TensorBoard peut prendre un peu de temps. Il faut parfois faire preuve d'un peu de patience lorsqu'on utilise cet outil mais ça en vaut la peine :)\n",
    "\n",
    "**TODO** : en naviguant dans les différents onglets du TensorBoard, chercher à répondre aux questions suivantes :\n",
    "* le GPU est-il bien utilisé ? (mémoire max utilisée, *occupancy*, *efficiency*)\n",
    "* la mémoire CPU est-elle saturée ?\n",
    "* les *TensorCores* sont-ils bien sollicités grâce à l'implémentation de la *mixed precision* ?\n",
    "* quelle partie de l'entraînement est la plus gourmande en temps ? se déroule-t-elle sur le CPU ou le GPU ?\n",
    "* essayer de repérer les grandes étapes de calcul sur la *timeline* de l'exécution (onglet *Trace*)\n",
    "\n",
    "**IMPORTANT** : une fois le TP terminé, penser à quitter l'instance JupyterHub pour **libérer le GPU** ( *> Hub Control Panel > Cancel* )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Garage](images/stop.png \"Arrêtez-vous ici! Une présentation vous attend avant le prochain TP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TP2_4 : Optimisation du DataLoader\n",
    "\n",
    "### Contrôle technique (version sous-optimisée)\n",
    "\n",
    "**TODO** : lancer l'exécution sur 50 itérations (`--test-nsteps 50`) sans profiling pour passer un contrôle technique qui servira de référence. **Cette exécution va prendre quelques minutes, vous pouvez passer à la suite du TP sans attendre la fin de l'exécution.**\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --test-nsteps 50'\n",
    "command += f' --num-workers 0 --no-persistent-workers --no-pin-memory --no-non-blocking --prefetch-factor 2'\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1587014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Découverte de turbo_profiler\n",
    "Pour ce TP, nous avons implémenté un profiler maison léger `turbo_profiler` basé sur l'outil `Chronometer` pour visualiser le temps passé sur CPU (DataLoader) et sur GPU (le reste de l'itération). Ce profiler est moins précis mais cela nous permettra de désactiver le profiler PyTorch pour ne pas dégrader les performances et éviter de devoir ouvrir l'outil graphique TensorBoard à chaque fois pour visualiser les informations qui nous intéressent.\n",
    "\n",
    "**TODO** :  relancer l'exécution précédente en désactivant le profiler PyTorch (sans l'argument `--prof`) et découvrir le profiler `turbo_profiler`.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --test-nsteps 15'\n",
    "command += f' --num-workers 0 --no-persistent-workers --no-pin-memory --no-non-blocking --prefetch-factor 2'\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1587014']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : visualiser la sortie de `turbo_profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo_profiler(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des paramètres d'optimisation du DataLoader\n",
    "L'objectif de ce TP est de réduire le temps passé sur CPU par le DataLoader.\n",
    "\n",
    "Les différentes optimisations proposées par le DataLoader de PyTorch sont accessibles dans le script `dlojz.py` via les arguments :\n",
    "* `--num-workers <num_workers>` (défaut à `10`)\n",
    "* `--persistent-workers` (défaut) ou `--no-persistent-workers`\n",
    "* `--pin-memory` (défaut) ou `--no-pin-memory`\n",
    "* `--non-blocking` (défaut) ou `--no-non-blocking`\n",
    "* `--prefetch-factor <prefetch_factor>` (défaut à `3`)\n",
    "* `--drop-last` ou `--no-drop-last` (défaut)\n",
    "\n",
    "**TODO** : faire varier ces différents paramètres et observer leurs effets grâce au profiler `turbo_profiler`\n",
    "\n",
    "**Remarque** : pour cette étude, on ne lance les exécutions que sur 15 itérations (`--test-nsteps 15`) pour avancer plus rapidement. \n",
    "\n",
    "Les différents essais seront stockés dans une *DataFrame* `dataloader_trials` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataloader_trials = pd.DataFrame({\"jobid\":pd.Series([],dtype=str),\n",
    "                                  \"num_workers\":pd.Series([],dtype=int),\n",
    "                                  \"persistent_workers\":pd.Series([],dtype=str),\n",
    "                                  \"pin_memory\":pd.Series([],dtype=str),\n",
    "                                  \"non_blocking\":pd.Series([],dtype=str),\n",
    "                                  \"prefetch_factor\":pd.Series([],dtype=int),\n",
    "                                  \"drop_last\":pd.Series([],dtype=str),\n",
    "                                  \"loading_time\":pd.Series([],dtype=float),\n",
    "                                  \"training_time\":pd.Series([],dtype=float)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --test-nsteps 15'\n",
    "\n",
    "# paramètres d'entrée correspondant aux optimisations du DataLoader\n",
    "command += ' --num-workers 0' \n",
    "command += ' --no-persistent-workers'\n",
    "command += ' --no-pin-memory'\n",
    "command += ' --no-non-blocking'\n",
    "command += ' --prefetch-factor 2'\n",
    "command += ' --no-drop-last'\n",
    "\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1508799']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call turbo_profiler\n",
    "dataloader_trial = turbo_profiler(jobid,dataloader_info=True)\n",
    "# store result in \"dataloader_trials\" DataFrame\n",
    "dataloader_trials = pd.concat([dataloader_trials,dataloader_trial], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher le tableau récapitulatif, trier par ordre croissant du LOADING_TIME\n",
    "dataloader_trials.sort_values(\"loading_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher le tableau récapitulatif, trier par ordre croissant du TRAINING_TIME\n",
    "dataloader_trials.sort_values(\"training_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des traces profiler avec TensorBoard (version optimisée)\n",
    "**TODO** : après avoir choisi un lot de paramètres optimal, relancer le job en réactivant le profiler PyTorch (argument d'entrée `--prof`) afin de visualiser les traces sous TensorBoard, et les comparer avec la version non optimale étudiée dans le TP2_0.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --prof --test-nsteps 15'\n",
    "\n",
    "# définir ici les paramètres optimaux \n",
    "command += ' --num-workers 0' \n",
    "command += ' --no-persistent-workers'\n",
    "command += ' --no-pin-memory'\n",
    "command += ' --no-non-blocking'\n",
    "command += ' --prefetch-factor 2'\n",
    "command += ' --no-drop-last'\n",
    "\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'éviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1732254']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : vérifier qu'une trace a bien été générée dans le répertoire `profiler/<name>_<jobid>_bs512_is176/` sous la forme d'un fichier `.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls profiler/{name}_{jobid[0]}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : visualiser cette trace grâce à l'application TensorBoard ([retrouver la procédure](#visu_tensorboard_gpu)). \n",
    "\n",
    "**IMPORTANT** : une fois le TP terminé, penser à quitter l'instance JupyterHub pour **libérer le GPU** ( *> Hub Control Panel > Cancel* )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Contrôle technique (version optimisée)\n",
    "\n",
    "**TODO** : lancer l'exécution sur 50 itérations (`--test-nsteps 50`) sans profiling pour passer un nouveau contrôle technique, à comparer avec celui de référence.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test --test-nsteps 50'\n",
    "\n",
    "# définir ici les paramètres optimaux \n",
    "command += ' --num-workers 0' \n",
    "command += ' --no-persistent-workers'\n",
    "command += ' --no-pin-memory'\n",
    "command += ' --no-non-blocking'\n",
    "command += ' --prefetch-factor 2'\n",
    "command += ' --no-drop-last'\n",
    "\n",
    "n_gpu = 1\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1587014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Garage](images/stop.png \"Arrêtez-vous ici! Une présentation vous attend avant le prochain TP.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-1.11.0_py3.9.12",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-1.11.0_py3.9.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
