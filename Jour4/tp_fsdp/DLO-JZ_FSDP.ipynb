{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cc9e94-d3b5-4360-a4e9-7e4af709fcc7",
   "metadata": {},
   "source": [
    "# DLO-JZ Fully Sharded Data parallelism - Jour 4\n",
    "\n",
    "Utilisation de la FSDP sur un modèle de langue **Llama 3.2 3B**.\n",
    "\n",
    "![Monstertruck](./images/MonsterTruck.png)\n",
    "\n",
    "\n",
    "## Objet du notebook\n",
    "\n",
    "Le but de ce *notebook* est d'optimiser un code d'apprentissage d'un modèle *Llama 3.2* sur un dataset de roleplay *Imagenet* :\n",
    "* Passage de DDP à FSDP\n",
    "* FSDP optimisée grâce au wrap récursif\n",
    "* Bonus : Application de la compilation PyTorch par dessus la FSDP\n",
    "\n",
    "\n",
    "Les cellules dans ce *notebook* ne sont pas prévues pour être modifiées, sauf rares exceptions indiquées dans les commentaires. Les TP se feront en modifiant le code `fsdp.py`.\n",
    "\n",
    "Les directives de modification seront marquées par l'étiquette **TODO :** dans le *notebook* suivant.\n",
    " \n",
    "Les solutions sont présentes dans le répertoire `solutions/`.\n",
    "\n",
    "*Notebook rédigé par l'équipe assistance IA de l'IDRIS, octobre 2024*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0e25c-e6f3-4cb3-bd3a-26bc6844b3ed",
   "metadata": {},
   "source": [
    "### Environnement de calcul\n",
    "\n",
    "Les fonctions *python* de gestion de queue Slurm dévelopées par l'IDRIS et les fonctions dédiées à la formation DLO-JZ sont à importer.\n",
    "\n",
    "Le module d'environnement pour les *jobs* et la taille des images sont fixés pour ce *notebook*.\n",
    "\n",
    "**TODO :** choisir un *pseudonyme* pour vous différencier dans la queue Slurm et dans les outils collaboratifs pendant la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bc232-591b-4a83-871d-bc74c790eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools import controle_technique, compare, GPU_underthehood, plot_accuracy, lrfind_plot, pipe_memory, turbo_profiler, comm_profiler\n",
    "MODULE = 'pytorch-gpu/py3/2.4.0'\n",
    "account = 'for@a100'\n",
    "name = 'NATHOUNET'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a5881-581b-4a11-bb36-12643907ef6a",
   "metadata": {},
   "source": [
    "---\n",
    "### Gestion de la queue Slurm\n",
    "\n",
    "Cette partie permet d'afficher et de gérer la queue Slurm.\n",
    "\n",
    "Pour afficher toute la queue *utilisateur* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d8f6e-bd59-4bbb-a461-7c8ad4826706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2838a0-7287-4aa7-ac1b-992345421e6b",
   "metadata": {},
   "source": [
    "**Remarque**: Cette fonction utilisée plusieurs fois dans ce *notebook* permet d'afficher la queue de manière dynamique, rafraichie toutes les 5 secondes. Cependant elle ne s'arrête que lorsque la queue est vide. Si vous désirez reprendre la main sur le *notebook*, il vous suffira d'arrêter manuellement la cellule avec le bouton *stop*. Cela a bien sûr aucun impact sur le *scheduler* Slurm. Les *jobs* ne seront pas arrêtés.\n",
    "\n",
    "Si vous voulez arrêter des *jobs* dans la queue :\n",
    "* Annuler tous vos *jobs* dans la queue (décommenter la ligne suivante)\n",
    "* Annuler un *job* dans votre queue (décommenter la ligne suivante et ajouter le numéro du *job* à la fin de la ligne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783ac8b-5cc8-4c52-8f3b-e619d3b203b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!scancel -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96240c33-d1bd-4797-8c6b-8da35488ed54",
   "metadata": {},
   "source": [
    "---\n",
    "### Différence de scripts <a id='diff_scripts'></a>\n",
    "\n",
    "Pour le *debug* ou pour comparer son code avec les solutions mises à disposition, la fonction suivante permet d'afficher une page html contenant un différentiel de fichiers texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5d033-0857-4fa9-92a1-85672c801bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"fsdp.py\"\n",
    "s2 = \"./solutions/fsdp_2.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58981a7e-234d-4d27-95a3-a4f2b60e712c",
   "metadata": {},
   "source": [
    "Voir le résultat du différentiel de fichiers sur la page suivante (attention au spoil !) :\n",
    "\n",
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6476e0-960e-4ed6-b488-9951ba400fe2",
   "metadata": {},
   "source": [
    "---\n",
    "# Première exécution en DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3783d86-19b6-4153-9d20-f2da8543bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp solutions/fsdp_0.py fsdp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f4636-2e4a-461a-975a-ff5c38c813e6",
   "metadata": {},
   "source": [
    "Prenez connaissance du script **fsdp.py**. C'est un fine-tuning d'un modèle de langue Llama 3.2 à 3 milliards de paramètres sur un dataset de Roleplay récupéré sur HuggingFace.\n",
    "\n",
    "La structure entre ce script et celui lié à la computer vision des autres jours est très similaire.\n",
    "\n",
    "_Note : on utilise les mêmes visualisations que dans les autres TPs donc les schémas peuvent parler d'images, mais dans notre contexte de NLP, la dimension du batch ne fait pas référence à des images mais plutôt aux nombres de séquences._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db8fdd-79cd-4f6e-a87e-9f7fc30333d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "command = f'fsdp.py --batch-size 4 --num-workers 2 --seq-len 512 --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d61e05-8dca-4cd2-91ec-05003c54275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb9101-e895-4078-9bfc-1d38eb016fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['902284']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc4736-28cb-4c08-92ca-d3fd6d1b0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4ac62-cf79-4337-96e6-1f375a3f4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo_profiler(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa93df7-12f2-4c58-a887-627d510efcc5",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff5fdb2-9663-42e4-99bd-57d2fc8e9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "command = [f'fsdp.py --batch-size {batch_size} --num-workers 2 --seq-len 512 --test'\n",
    "          for batch_size in batch_sizes]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='a100', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b9803-073a-4ec8-9ce2-5903f9a229cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42417d2-efe8-4df7-87a6-19a41013bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151860f-2415-4373-a320-da0c8e31cf98",
   "metadata": {},
   "source": [
    "---\n",
    "# Passage au Fully Sharded Data Parallelism\n",
    "\n",
    "**TODO**: Remplacez le Distributed Data Parallelism par le Fully Sharded Data Parallelism. Ce n'est qu'un simple wrapper et demande peu de modifications. Indice : ctrl-F de \"#### Distribute the Model\" pour repérer l'endroit où faire ça.\n",
    "\n",
    "**Important**: Dans la documentation, vous pouvez voir un paramètre _auto_wrap_policy_, il est abordé dans la section suivante donc laissez le non précisé pour l'instant.\n",
    "\n",
    "FSDP est un wrapper très haut niveau qui fait toutes les communications de manière cachée pour faciliter son utilisation. C'est son grand avantage par rapport à DeepSpeed.\n",
    "\n",
    "![fsdp](images/fsdp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a257738-ef88-41d6-a645-ec170c8f6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "command = f'fsdp.py --batch-size 4 --num-workers 2 --seq-len 512 --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220a952-bcef-4301-911d-7f6c89866fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e3b77-9481-42d8-8c69-370a0ae56663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['902284']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f6b80-f651-4d4e-9a0d-a64087c389e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955fd62-c743-45cb-9bbb-7eedf1cd7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_memory(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4518f5-4334-4bcd-ab1e-8a0aa9f70bf6",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687271d-99da-4134-bba0-0a8ee1196f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "batch_sizes = [2, 4, 8, 16]\n",
    "command = [f'fsdp.py --batch-size {batch_size} --num-workers 2 --seq-len 512 --test'\n",
    "          for batch_size in batch_sizes]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='a100', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b30134-a64d-4f8a-a7d3-55972010e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['299137', '299138', '299139', '299140', '299141']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6df6c1-7396-4aed-b5f2-6dc7f4338587",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b66b87-7bec-41c4-b99e-e637c0dd07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd70a2e-ed19-4beb-b171-6cca9eef9d03",
   "metadata": {},
   "source": [
    "### Contrôle technique de la configuration optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd84d8-b061-4d68-b977-e832db7642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique([jobids[-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151958a-5128-467a-bd31-6ffcd55394b9",
   "metadata": {},
   "source": [
    "---\n",
    "# Optimisation de la FSDP en l'appliquant récursivement\n",
    "\n",
    "En utilisant la FSDP sans l'argument _auto_wrap_policy_, alors tous les paramètres du modèle sont transmis en une seule fois plutôt que quand ils sont nécessaires. On pourrait imaginer transmettre d'abord les paramètres des premières couches (dont on a besoin rapidement), puis ensuite les autres.\n",
    "\n",
    "Le paramètre _auto_wrap_policy_ permet de faire ça. Il est puissant car il permet de maximiser le recouvrement entre les calculs et les communications et minimise la mémoire nécessaire.\n",
    "\n",
    "**TODO**: Ajoutez le paramètre _auto_wrap_policy_. Indice : Pour les transformers, il est de coutume de wrapper chaque block de transformer indépendamment (signature de la méthode _transformer_auto_wrap_policy_ [ici](https://github.com/pytorch/pytorch/blob/8f3efb8797b7a2dbd958bf625374985793ed5035/torch/distributed/fsdp/wrap.py#L307))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79d093-ec7b-4928-a98e-4081ab4bb0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "command = f'fsdp.py --batch-size 4 --num-workers 2 --seq-len 512 --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1387e58-14e3-4283-b152-60c4144683a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456bbc8-7c37-48f0-aa9c-77960bd334a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['902284']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dca950-ea3c-43c1-ac11-0c9c676e2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858cddfb-adae-4899-b728-260a3c01764a",
   "metadata": {},
   "source": [
    "Vous pouvez consulter les logs de l'entraînement. On affiche le modèle, et on peut par conséquent voir l'impact de l'argument _auto_wrap_policy_.\n",
    "\n",
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfd3e0-d3b2-4048-be6e-2e832353baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "batch_sizes = [4, 8, 12, 16, 32]\n",
    "command = [f'fsdp.py --batch-size {batch_size} --num-workers 2 --seq-len 512 --test'\n",
    "          for batch_size in batch_sizes]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='a100', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b700d-1f67-4d91-aa65-52b0727d709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['300073', '300074', '300075', '300076', '300077']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef40184-ccb1-466a-97b9-207dcece6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e8c8a-f95b-4d75-9aaf-d69f0ddd2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4d187-02ec-4234-9dd7-b7780ca0d963",
   "metadata": {},
   "source": [
    "### Contrôle technique de la configuration optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d90e27-d726-4e81-a1d4-a5061c9e47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobids[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc945e3-c83f-4a02-ad2e-3a8b68df8006",
   "metadata": {},
   "source": [
    "---\n",
    "# Bonus : torch.compile par dessus FSDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44353501-1a3e-4552-b652-652f82720d1e",
   "metadata": {},
   "source": [
    "**TODO**: Appliquez la compilation par PyTorch de votre modèle.\n",
    "\n",
    "**Indice**: ctrl-F de \"#### JIT\" pour trouver où faire ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb98f9-72b4-44a4-8c49-9c90b6943d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "command = f'fsdp.py --batch-size 16 --num-workers 2 --seq-len 512 --test --compile'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca220b-5ac3-4e5b-a186-9fcee0bd5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['902284']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bba9d3-5ccf-41b3-8512-3e6d90682aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367d59a-1148-4bb4-83a3-e3eba55f4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b35b39-6ac5-40d6-99f9-41491c3f91ce",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c43692-1507-47e3-8ca1-15c61d6dbe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 4\n",
    "batch_sizes = [8, 12, 16, 20, 24]\n",
    "command = [f'fsdp.py --batch-size {batch_size} --num-workers 2 --seq-len 512 --test --compile'\n",
    "          for batch_size in batch_sizes]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='a100', qos=\"qos_gpu_a100-dev\")\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead77b51-4ffe-47cb-a0f7-1f5e17144db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['300403', '300404', '300406', '300407', '300408', '300409']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4523853-c969-4bf2-b5c0-ac32731fb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb61de7-d7a5-4e54-a083-a4fd40889ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435a7b8-20ca-42db-b9be-01e752cc8902",
   "metadata": {},
   "source": [
    "### Contrôle technique de la configuration optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7064c05-e593-47a7-8cea-4136c9c7225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobids[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce526d83-52cd-4f5d-9bd0-4f2ede2baf56",
   "metadata": {},
   "source": [
    "_torch.compile_ est encore très nouveau et il peut arriver qu'un modèle ne puisse pas être converti. Plusieurs backends sont disponibles (voir documentation officielle). Dans les cas des modèles les plus exotiques, la compilation peut tout simplement échoué. C'est tellement bas niveau qu'il est bien possible qu'on ne puisse rien y faire, c'est juste lié au fait que _torch.compile_ est relativement nouveau. À garder à l'esprit cependant, car cela peut augmenter de 50%, voire parfois 100% le throughput de votre modèle.\n",
    "\n",
    "![Commentaires](images/cedez.png \"La suite correspond aux annexes, vous etes arrivé à bout du TP, BRAVO\")\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.4.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.4.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
