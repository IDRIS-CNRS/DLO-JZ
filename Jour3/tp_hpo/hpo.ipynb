{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d9624",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **TP HPO : Hyperparameter Optimization in Deep Learning**\n",
    "\n",
    "* Author : IDRIS-CNRS, Guerda.K, Hunout.L\n",
    "* Version : 2024-03\n",
    "* License : CC BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5206ac",
   "metadata": {},
   "source": [
    "## **Initial Setup and Instructions**\n",
    "This section prepares the notebook environment for our exercises and introduces key tools we'll use throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45606e98",
   "metadata": {},
   "source": [
    "### Auto-Reloading\n",
    "\n",
    "To ensure any changes in imported modules are automatically updated in the notebook, we enable auto-reloading. This is particularly useful during development, as it saves time that would otherwise be spent restarting the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7bfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018aec11",
   "metadata": {},
   "source": [
    "### Understanding `ToDo`\n",
    "\n",
    "Throughout this notebook, you'll encounter placeholders marked with `ToDo`. This indicates a section where you, the student, need to complete or modify the code. The `ToDo` class is designed to remind you of tasks that need to be addressed to proceed with the exercises. It's a prompt for active participation in the learning process.\n",
    "\n",
    "Whenever you see a `ToDo` in the code, it's your cue to engage with the material by writing or modifying code. This interactive approach helps reinforce learning through practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6076720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToDo:\n",
    "    def __init__(self, message=None):\n",
    "        if message is None:\n",
    "            self.message = \"This part of the code needs to be completed by the student.\"\n",
    "        else:\n",
    "            self.message = message\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(\"TODO: Complete this part of the code.\")\n",
    "        raise NotImplementedError(self.message)\n",
    "\n",
    "    # Example of an additional method that could be useful in the future\n",
    "    def hint(self):\n",
    "        print(f\"Hint: {self.message}\")\n",
    "\n",
    "# Usage\n",
    "todo = ToDo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bc1d",
   "metadata": {},
   "source": [
    "## **Introduction to Hyperparameter Optimization**\n",
    "\n",
    "Hyperparameter Optimization (HPO) is a critical process in developing deep learning models. It involves finding the most effective combination of hyperparameters, which are the configuration settings used to structure the learning process of a model. Unlike model parameters, which are learned during training, hyperparameters are set prior to the learning process and can significantly impact the performance of the model.\n",
    "\n",
    "### Goals of HPO\n",
    "- **Maximize Performance**: Optimize model accuracy, precision, recall, or any other relevant metric.\n",
    "- **Efficiency**: Reduce training time and computational resources without compromising performance.\n",
    "- **Generalization**: Enhance the model's ability to perform well on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3e6de",
   "metadata": {},
   "source": [
    "## Overview of HPO Frameworks\n",
    "\n",
    "In the realm of machine learning and deep learning, several frameworks are available for hyperparameter optimization (HPO). Each of these frameworks offers unique features and capabilities. Among the most notable are Optuna, Ray Tune, Hyperopt, and Scikit-Optimize. However, in this notebook, we will primarily focus on Optuna and Ray Tune due to their specific advantages and relevance in different scenarios.\n",
    "\n",
    "Optuna\n",
    "\n",
    "- **Overview**: Optuna is a versatile and user-friendly open-source optimization framework specifically tailored for machine learning. Known for its efficiency and ease of use, Optuna is particularly well-suited for individuals and teams starting their journey with HPO. \n",
    "- **Why We Focus on Optuna**: We choose Optuna for its intuitive API, efficient optimization algorithms, and excellent visualization capabilities, making it ideal for educational purposes and straightforward HPO tasks.\n",
    "- [Optuna Website](https://optuna.org/)\n",
    "\n",
    "Ray Tune\n",
    "\n",
    "- **Overview**: Ray Tune is a powerful component of the Ray ecosystem, designed for distributed hyperparameter tuning. It is especially useful for handling large-scale, computationally intensive HPO tasks.\n",
    "- **Why Ray Tune is an Interesting Choice**: We include Ray Tune due to its scalability, ability to leverage distributed computing resources effectively, and integration with various machine learning frameworks, making it suitable for more advanced, large-scale HPO scenarios.\n",
    "- [Ray Tune Website](https://docs.ray.io/en/latest/tune/)\n",
    "\n",
    "Other Notable Frameworks\n",
    "\n",
    "- **Hyperopt**: A popular tool for optimizing over awkward search spaces with real-valued, discrete, and conditional dimensions. [Hyperopt Website](http://hyperopt.github.io/hyperopt/)\n",
    "- **Scikit-Optimize**: A library for sequential model-based optimization that is built on top of Scikit-Learn. It's particularly straightforward for those already familiar with the Scikit-Learn ecosystem. [Scikit-Optimize Website](https://scikit-optimize.github.io/stable/)\n",
    "\n",
    "While there are other excellent frameworks available, Optuna and Ray Tune stand out for their distinct advantages in specific use cases. Optuna's user-friendly nature makes it an excellent teaching tool for understanding the basics and intricacies of HPO, while Ray Tune's scalability and advanced features make it a robust choice for tackling more complex, resource-intensive HPO tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a928c4",
   "metadata": {},
   "source": [
    "## Installing the Frameworks\n",
    "\n",
    "Before diving into practical examples, you may need to install Optuna and Ray Tune. These libraries can be easily installed using pip. However, they might already be installed in your environment, especially if you are using a pre-configured setup like Google Colab or a managed Jupyter environment.\n",
    "\n",
    "To check if these libraries are already installed, you can import them in your notebook. If they are not installed, you can install them using the following pip commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b19adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import optuna\n",
    "    print(\"Optuna is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Optuna is not installed.\")\n",
    "\n",
    "try:\n",
    "    import ray.tune\n",
    "    print(\"Ray Tune is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Ray Tune is not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1c828",
   "metadata": {},
   "source": [
    "If you find that Optuna or Ray Tune is not installed, you can install them using the following commands:\n",
    "\n",
    "\n",
    "Remember to restart the kernel of your Jupyter notebook after installing the libraries to ensure that the changes take effect.\n",
    "\n",
    "```bash\n",
    "# To install Optuna\n",
    "!pip install optuna --user\n",
    "\n",
    "# To install Ray Tune\n",
    "!pip install ray[tune] --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d72610",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## **Synthetic Optimization with a Simple Function**\n",
    "\n",
    "To grasp the fundamentals of hyperparameter optimization (HPO) without the complexity of training models, we'll start with a synthetic example. This approach involves a straightforward function that simulates loss based on two hyperparameters and a modeâ€”either 'simple' or 'complex'. This synthetic function facilitates exploration of the HPO process efficiently.\n",
    "\n",
    "### Setting Up the Simulation\n",
    "\n",
    "Our goal with the `synthetic_loss_function` is to explore how different combinations of hyperparameters affect the loss, depending on the mode of operation. This function demonstrates the impact of hyperparameters on a model's loss landscape in a simplified manner.\n",
    "\n",
    "### Implementing the Synthetic Loss Function\n",
    "\n",
    "The task involves implementing a single `synthetic_loss_function` capable of operating in two distinct modes. This design allows us to examine different types of loss landscapes: one straightforward with a single minimum and another more intricate with multiple minima.\n",
    "\n",
    "> **Task**: Implement the `synthetic_loss_function` that behaves differently based on the `mode` parameter:\n",
    "- The `mode='simple'` loss, will be a variant of the famous [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) (a=b=1) :\n",
    "  $$\\text{f(x,y)} = (a - x)^2 + b(y - x^2)^2$$\n",
    "  \n",
    "- The `mode='complex'` loss, we will use the [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function). It's featuring multiple minima and a more dynamic landscape :\n",
    "  $$\\text{f(x,y)} = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bf8f6-7c9e-4604-8166-99fbfda02ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def synthetic_loss_function(hyperparam1, hyperparam2, mode='simple'):\n",
    "    # Implementation based on mode\n",
    "    if mode == 'simple':\n",
    "        # Calculate the loss for the simple mode\n",
    "        return todo() #Implement Rosenbrock\n",
    "    \n",
    "    elif mode == 'complex':\n",
    "        # Calculate the loss for the complex mode\n",
    "        return todo() #Implement Himmelblaus\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified. Choose either 'simple' or 'complex'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ea15a",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "Here's the completed `synthetic_loss_function`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def synthetic_loss_function(hyperparam1, hyperparam2, mode='simple'):\n",
    "    # Implementation based on mode\n",
    "    if mode == 'simple':\n",
    "        # Calculate the loss for the simple mode\n",
    "        return ((1 - hyperparam1)**2 + (hyperparam2 - hyperparam1**2)**2)\n",
    "    elif mode == 'complex':\n",
    "        # Calculate the loss for the complex mode\n",
    "        return (hyperparam1**2+hyperparam2-11)**2+(hyperparam1+hyperparam2**2-7)**2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified. Choose either 'simple' or 'complex'.\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7100",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizing a Synthetic Loss Landscape\n",
    "\n",
    "In practice, the loss landscape of deep learning models is intricate and high-dimensional, making direct visualization challenging. For this exercise, we simplify the concept by using a synthetic loss function. This visualization serves as a conceptual tool to illustrate the effects of hyperparameter adjustments on model performance, rather than a practical approach to navigating real-world loss landscapes.\n",
    "\n",
    "This simplified exercise aims to provide insight into the optimization process in a visual and intuitive manner.\n",
    "\n",
    "> **Task**: Generate a meshgrid for parameter values and use `synthetic_loss_function` to calculate the loss. Then, create a contour plot to visualize the loss landscape. Ensure the range for parameters is broad enough to effectively visualize the landscapes for both modes (-5 to 5 both axes).\n",
    "\n",
    "\n",
    "> **Task**: Apply the `synthetic_loss_function` to calculate the loss across the meshgrid for both 'simple' and 'complex' modes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcce4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range for both parameters using np.linspace and that accommodates both 'simple' and 'complex' modes\n",
    "hyperparam1_range = todo()\n",
    "hyperparam2_range = todo()\n",
    "\n",
    "# Create a meshgrid for the parameter values\n",
    "hyperparam1, hyperparam2 =  np.meshgrid(hyperparam1_range,hyperparam2_range)\n",
    "\n",
    "# Calculate the loss for each combination of param1 and param2 for both modes\n",
    "# Hint: Use the synthetic_loss_function you defined earlier with 'simple' and 'complex' modes\n",
    "loss_simple = todo()\n",
    "loss_complex = todo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c827b",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define the range for both parameters using np.linspace and that accommodates both 'simple' and 'complex' modes\n",
    "hyperparam1_range = np.linspace(-5,5,100)\n",
    "hyperparam2_range = np.linspace(-5,5,100)\n",
    "\n",
    "\n",
    "# Create a meshgrid for the parameter values\n",
    "hyperparam1, hyperparam2 =  np.meshgrid(hyperparam1_range,hyperparam2_range)\n",
    "\n",
    "# Calculate the loss for each combination of param1 and param2 for both modes\n",
    "# Hint: Use the synthetic_loss_function you defined earlier with 'simple' and 'complex' modes\n",
    "loss_simple = synthetic_loss_function(hyperparam1,hyperparam2)\n",
    "loss_complex = synthetic_loss_function(hyperparam1,hyperparam2,mode='complex')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46d201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting the loss landscape for 'simple' mode\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_simple, levels=50, cmap='viridis') # add norm=colors.LogNorm() to see global minimum\n",
    "plt.colorbar()\n",
    "plt.title('Simple Mode Loss Landscape')\n",
    "plt.xlabel('Hyperparameter 1')\n",
    "plt.ylabel('Hyperparameter 2')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_simple, levels=50, cmap='viridis', norm=colors.LogNorm())\n",
    "plt.colorbar()\n",
    "\n",
    "# Plotting the loss landscape for 'complex' mode\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_complex, levels=50, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Complex Mode Loss Landscape')\n",
    "plt.xlabel('Hyperparameter 1')\n",
    "plt.ylabel('Hyperparameter 2')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_complex, levels=50, cmap='viridis', norm=colors.LogNorm())\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc9eb5-8144-4199-86f0-7eefb8eddcd6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## **HPO with Optuna**\n",
    "\n",
    "Hyperparameter Optimization (HPO) plays a crucial role in enhancing the performance of machine learning models by efficiently finding the best set of hyperparameters. It bridges the gap between theoretical understanding and practical application, moving beyond trial-and-error to systematic and automated search strategies. Optuna stands out in the HPO landscape, offering a user-friendly interface and efficient algorithms for exploring complex hyperparameter spaces. Its versatility makes it suitable for a wide range of applications, from tuning simple models to optimizing sophisticated deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee498f-b05a-4575-9e2e-6ba9169c92f7",
   "metadata": {},
   "source": [
    "### Define the Objective Function\n",
    "\n",
    "The cornerstone of using Optuna for HPO is the objective function. This function evaluates how well a set of hyperparameters performs against a predefined metric, typically the loss or accuracy of a model. Within Optuna, a `trial` object suggests hyperparameters, allowing the objective function to be dynamically adjusted based on the trial's performance. \n",
    "\n",
    "> **Task**: Implement an `objective` function for Optuna, designed to work with both 'simple' and 'complex' modes of a synthetic loss function. This entails defining hyperparameter ranges suitable for both modes and incorporating a mode selector. Utilize Optuna's `suggest_float` for continuous hyperparameters and `suggest_categorical` for selecting the operational mode. This exercise will introduce you to defining hyperparameter spaces and optimizing them within the Optuna framework. For detailed guidance, consult the documentation for [`suggest_float`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float) and [`suggest_categorical`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f703f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Use trial.suggest_float for hyperparam1. Think about the appropriate range.\n",
    "    hyperparam1 = todo()\n",
    "    \n",
    "    # Use trial.suggest_float for hyperparam2. Consider what range might be best.\n",
    "    hyperparam2 = todo()\n",
    "    \n",
    "    # Use trial.suggest_categorical to choose between 'simple' and 'complex' modes.\n",
    "    mode = todo()\n",
    "    \n",
    "    return synthetic_loss_function(hyperparam1, hyperparam2, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67864e27",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Use trial.suggest_float for hyperparam1. Think about the appropriate range.\n",
    "    hyperparam1 = trial.suggest_float('hyperparam1',-5,5)\n",
    "    \n",
    "    # Use trial.suggest_float for hyperparam2. Consider what range might be best.\n",
    "    hyperparam2 = trial.suggest_float('hyperparam1',-5,5)\n",
    "    \n",
    "    # Use trial.suggest_categorical to choose between 'simple' and 'complex' modes.\n",
    "    mode = trial.suggest_categorical(\"mode\",[\"simple\",\"complex\"])\n",
    "    \n",
    "    return synthetic_loss_function(hyperparam1, hyperparam2, mode)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8d315",
   "metadata": {},
   "source": [
    "### Exploring Optimization Strategies with Optuna\n",
    "\n",
    "Optuna supports a variety of hyperparameter optimization strategies, each offering distinct advantages. This section will guide you through the creation and optimization of studies using three key strategies: Grid Search, Bayesian Optimization (TPE), and Random Sampling.\n",
    "\n",
    "#### Grid Search\n",
    "> **Task**: Initialize an Optuna study with the [`GridSampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.GridSampler.html) to perform an exhaustive search over a predefined grid of hyperparameter values. This method is thorough, ensuring no potential combination is overlooked, though it may be computationally demanding.\n",
    "\n",
    "#### Bayesian Optimization (TPE)\n",
    "> **Task**: Employ the default [`TPESampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html) for Bayesian Optimization. This approach uses a probabilistic model to intelligently propose hyperparameter sets, optimizing in complex, high-dimensional spaces efficiently.\n",
    "\n",
    "#### Random Sampling\n",
    "> **Task**: Utilize the [`RandomSampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.RandomSampler.html) for a stochastic exploration of the hyperparameter space. Random Sampling provides a baseline by selecting hyperparameters without prior assumptions, offering a chance to identify good parameters early in less complex spaces.\n",
    "\n",
    "For each strategy:\n",
    "- **Setting Up the Study**: Create an Optuna study specifying the chosen sampler. This setup defines the optimization strategy for your hyperparameter search.\n",
    "  \n",
    "- **Running the Optimization**: Optimize your study by invoking the `optimize` method with your objective function. The number of trials can be adjusted to balance thorough exploration with computational resource constraints.\n",
    "\n",
    "- **Analyzing the Results**: Upon completion, use `study.best_params` to review the most effective hyperparameters identified through each strategy, providing insights into their performance and suitability for your specific problem.\n",
    "\n",
    "This exploration provides a hands-on comparison of different optimization strategies within Optuna, demonstrating their utility and effectiveness across a range of optimization scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69cfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "\n",
    "# Adjust Optuna's logging level to WARN to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "random_study = todo()  # Initialize Random Sampling study\n",
    "tpe_study = todo()  # Initialize TPE study\n",
    "\n",
    "# Specify the number of trials\n",
    "n_trials = todo()\n",
    "\n",
    "# Optimize the studies\n",
    "grid_study.optimize(objective, n_trials=n_trials)\n",
    "todo()  # Optimize Random Sampling study\n",
    "todo()  # Optimize TPE study\n",
    "\n",
    "# Print the best parameters found by each study\n",
    "print(f\"Grid Search Best parameters found: {grid_study.best_params}\")\n",
    "todo()  # Print best parameters for Random Sampling study\n",
    "todo()  # Print best parameters for TPE study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ff694",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Basic Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "\n",
    "# Adjust Optuna's logging level to WARN to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Assuming 'objective' is defined elsewhere in your notebook\n",
    "# Example objective function\n",
    "def objective(trial):\n",
    "    hyperparam1 = trial.suggest_float(\"hyperparam1\", -5, 5)\n",
    "    hyperparam2 = trial.suggest_float(\"hyperparam2\", -5, 5)\n",
    "    mode = trial.suggest_categorical(\"mode\", [\"simple\", \"complex\"])\n",
    "    # Example evaluation logic\n",
    "    return (hyperparam1 - 1)**2 + (hyperparam2 - 2)**2\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "tpe_study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='TPESamplerStudy')\n",
    "random_study = optuna.create_study(direction='minimize', sampler=RandomSampler(), study_name='RandomSamplerStudy')\n",
    "\n",
    "# Number of trials\n",
    "n_trials = 100  # Specify the number of trials for demonstration\n",
    "\n",
    "# Optimize the studies\n",
    "grid_study.optimize(objective, n_trials=n_trials)\n",
    "tpe_study.optimize(objective, n_trials=n_trials)\n",
    "random_study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# Print the best parameters found by each study\n",
    "print(f\"Grid Search Best parameters found: {grid_study.best_params}\")\n",
    "print(f\"TPE Best parameters found: {tpe_study.best_params}\")\n",
    "print(f\"Random Sampling Best parameters found: {random_study.best_params}\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Advanced Solution with Progress Bar and less print (click to reveal)</summary>\n",
    "\n",
    "This advanced solution incorporates `tqdm` for progress tracking, providing visual feedback during the optimization process.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Adjust Optuna's logging level to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "tpe_study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='TPESamplerStudy')\n",
    "random_study = optuna.create_study(direction='minimize', sampler=RandomSampler(), study_name='RandomSamplerStudy')\n",
    "\n",
    "# Define a callback for the progress bar update\n",
    "def progress_bar_callback(study, trial):\n",
    "    pbar.update(1)\n",
    "\n",
    "# Number of trials\n",
    "n_trials = 50\n",
    "\n",
    "# Initialize tqdm progress bar for each study and optimize\n",
    "for study in [grid_study, tpe_study, random_study]:\n",
    "    with tqdm(total=n_trials, desc=f\"Optimizing {study.study_name}\", unit=\"trial\") as pbar:\n",
    "        study.optimize(objective, n_trials=n_trials, callbacks=[progress_bar_callback])\n",
    "        pbar.close()  # Ensure the progress bar is closed after optimization\n",
    "\n",
    "    # Print the best parameters found by each study\n",
    "    print(f\"{study.study_name} Best parameters found: {study.best_params}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb9713",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Visualizing Comparisons of Optimization Strategies with Optuna\n",
    "\n",
    "Optuna's visualization module provides a comprehensive toolkit for analyzing the outcomes of hyperparameter optimization experiments. These visual tools offer deep insights into the performance and dynamics of different optimization strategies, including Grid Search, TPE (Bayesian Optimization), and Random Sampling, highlighting the nuances of each approach in navigating the hyperparameter space.\n",
    "\n",
    "- **[`plot_parallel_coordinate`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_parallel_coordinate.html)**: Illustrates the interplay between hyperparameters and objective values, useful for comparing the influence of parameters across optimization strategies.\n",
    "\n",
    "- **[`plot_optimization_history`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_optimization_history.html)**: Chronicles the progression of objective value improvements across trials, offering insights into the efficiency and exploration depth of each strategy.\n",
    "\n",
    "- **[`plot_param_importances`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_param_importances.html)**: Highlights which hyperparameters significantly impact the objective value, aiding in strategy refinement for future optimizations.\n",
    "\n",
    "- **[`plot_contour`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_contour.html)**: Explores the synergy between pairs of hyperparameters and their collective effect on the objective, facilitating the identification of optimal parameter interactions.\n",
    "\n",
    "- **[`plot_edf`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_edf.html)**: Displays the empirical distribution of objective values across all trials, providing a macroscopic view of the optimization landscape influenced by different sampling strategies.\n",
    "\n",
    "- **[`plot_intermediate_values`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_intermediate_values.html)**: Reveals the trajectory of intermediate objective values, shedding light on the iterative progress within trials.\n",
    "\n",
    "- **[`plot_slice`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)**: Demystifies how variations in individual hyperparameters impact the objective, highlighting parameter sensitivity.\n",
    "\n",
    "- **[`plot_timeline`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_timeline.html)**: Visualizes the timeline of trial execution, underscoring the operational efficiency and parallelization capabilities of each strategy.\n",
    "\n",
    "- **[`plot_rank`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_rank.html)**: Assesses the comparative performance of trials, accentuating the effectiveness of different optimization strategies.\n",
    "\n",
    "> **Task**: Examine the visualizations to discern the distinct characteristics and efficacy of **Grid Search**, **Random Sampling** , and **TPE** strategies. Utilize insights from plots like hyperparameter importance and optimization history to devise nuanced future tuning approaches and enhance model optimization outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f2f9c-d656-4d08-b9b7-2326cc4b9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import optuna.visualization as vis\n",
    "from plotly.io import to_html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_optuna_visualizations(study):\n",
    "    \"\"\"\n",
    "    Display a \"beautiful\" series of Optuna visualization plots for a given study.\n",
    "\n",
    "    Parameters:\n",
    "    - study: The Optuna study to visualize.\n",
    "    \"\"\"\n",
    "    # List of visualization functions to display, with a brief description as a comment\n",
    "    visualization_functions = [\n",
    "        vis.plot_parallel_coordinate,  # Hyperparameter Relationship Plot\n",
    "        vis.plot_optimization_history, # Optimization History\n",
    "        vis.plot_param_importances,    # Hyperparameter Importance\n",
    "        vis.plot_contour,              # Contour Plot of Parameter Interactions\n",
    "        vis.plot_edf,                  # EDF (Empirical Distribution Function)\n",
    "        vis.plot_intermediate_values,  # Intermediate Values of All Trials\n",
    "        vis.plot_slice,                # Slice Plot in a Study\n",
    "        vis.plot_timeline,             # Timeline of a Study\n",
    "        vis.plot_rank                  # Rank Plot\n",
    "    ]\n",
    "    grid_html = \"<div style='display: grid; grid-template-columns: repeat( 3 , 1fr); gap: 10px;'>\"\n",
    "    for viz in visualization_functions:\n",
    "        content = to_html(viz(study).update_layout(autosize=True), full_html=False, include_plotlyjs='cdn')\n",
    "        grid_html += \"<div>\" + content + \"</div>\"\n",
    "    grid_html += \"</div>\"\n",
    "    display(HTML(grid_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78d2f6-bd80-461f-a689-53da8188656b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>WebGL Fix Firefox (click to reveal)</summary>\n",
    "\n",
    "You can have a WebGL error on the training computer, they don't have any gpu.\n",
    "Force WebGL on CPU :\n",
    "* go to about:config\n",
    "* search for webgl.force-enabled and make sure this preference is set to true\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7a2ce-cdae-473a-8ff7-df730f4aad2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Alternative with matplotlib (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "import sys, io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as vis #note .matplotlib\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def display_optuna_visualizations(study):\n",
    "    \"\"\"\n",
    "    Display a series of Optuna visualization plots for a given study.\n",
    "\n",
    "    Parameters:\n",
    "    - study: The Optuna study to visualize.\n",
    "    \"\"\"\n",
    "    # List of visualization functions to display, with a brief description as a comment\n",
    "    visualization_functions = [\n",
    "        vis.plot_parallel_coordinate,  # Hyperparameter Relationship Plot\n",
    "        vis.plot_optimization_history, # Optimization History\n",
    "        vis.plot_param_importances,    # Hyperparameter Importance\n",
    "        vis.plot_contour,              # Contour Plot of Parameter Interactions\n",
    "        vis.plot_edf,                  # EDF (Empirical Distribution Function)\n",
    "        vis.plot_intermediate_values,  # Intermediate Values of All Trials\n",
    "        vis.plot_slice,                # Slice Plot in a Study\n",
    "        vis.plot_timeline,             # Timeline of a Study\n",
    "        vis.plot_rank                  # Rank Plot\n",
    "    ]\n",
    "    \n",
    "    # Create a figure and axes for the grid of plots\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(25, 20))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Redirect stdout to a buffer\n",
    "    stdout_buffer = io.StringIO()\n",
    "    cell_stdout = sys.stdout\n",
    "    sys.stdout = stdout_buffer\n",
    "\n",
    "    # Call your plotting functions\n",
    "    for i, ax in enumerate(axs):\n",
    "        visualization_functions[i](study)\n",
    "\n",
    "        # Capture the plot generated by the function\n",
    "        captured_fig = plt.gcf()\n",
    "        captured_ax = plt.gca()\n",
    "        # Close the current figure to clear the plot\n",
    "        plt.close(captured_fig)\n",
    "\n",
    "        # Convert the captured plot to an image array\n",
    "        buffer_ = io.BytesIO()\n",
    "        captured_fig.savefig(buffer_, format='png', bbox_inches='tight')\n",
    "        buffer_.seek(0)\n",
    "        image = Image.open(buffer_)\n",
    "        image= image.crop((0, 0, image.width, min(image.width,image.height)))\n",
    "\n",
    "        # Plot the captured plot->image onto the current subplot\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    sys.stdout = cell_stdout \n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f4798",
   "metadata": {},
   "source": [
    "#### Grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(grid_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c751c6e",
   "metadata": {},
   "source": [
    "#### Random search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c167a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(random_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f549f5",
   "metadata": {},
   "source": [
    "#### TPE search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(tpe_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de77833",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Leveraging Optuna Pruners for Efficient Optimization\n",
    "\n",
    "Optuna pruners enhance optimization by terminating unpromising trials early, conserving valuable computational resources.\n",
    "#### Pruners in Action\n",
    "\n",
    "- **[`SuccessiveHalvingPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html)**: An implementation of **ASHA**. This pruner evaluates trials at intervals, halving less promising ones, focusing resources on those with the most short potential.\n",
    "- **[`HyperbandPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html)**: An implementation of **BOHB**. This pruner evaluate trials at different step intervals, halving less promissing ones, focusing resources on those with the most potential.\n",
    "\n",
    "#### Objective Function Adjustments for Pruning\n",
    "\n",
    "To utilize pruning:\n",
    "- **Intermediate Reporting**: The objective function must periodically report interim results using `trial.report()`.\n",
    "- **Checkpoints for Pruning**: Incorporate `trial.should_prune()` checks after reporting, halting trials early with `optuna.TrialPruned()` if deemed non-promising.\n",
    "\n",
    "#### Why Adapt Our Synthetic Function\n",
    "\n",
    "Adjusting our synthetic function to report intermediate results allows pruners to make informed decisions on trial continuation. This step is crucial for pruning to effectively reduce computational load and improve the optimization process's overall efficiency.\n",
    "\n",
    "Pruning with Optuna signifies a strategic layer to hyperparameter optimization, ensuring a smarter allocation of computational effort towards the most promising trials.\n",
    "\n",
    "> **Note**: Effective use of pruners like `SuccessiveHalvingPruner` or `HyberbandPruner` necessitates modifications for intermediate evaluations within the objective function, enabling a dynamic and resource-efficient optimization workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def objective_with_pruning(trial):\n",
    "    # Define hyperparameters\n",
    "    hyperparam1 = trial.suggest_float(\"hyperparam1\", -5, 5)\n",
    "    hyperparam2 = trial.suggest_float(\"hyperparam2\", -5, 5)\n",
    "    mode = trial.suggest_categorical(\"mode\", [\"simple\", \"complex\"])\n",
    "    \n",
    "    # Simulate a step-wise evaluation process\n",
    "    accumulated_loss = 0\n",
    "    steps = 10 \n",
    "    best_loss = float('+inf')\n",
    "    for step in range(1, steps + 1):\n",
    "        \n",
    "        end_loss = synthetic_loss_function(hyperparam1, hyperparam2 , mode)\n",
    "        intermediate_loss =  end_loss * (1.1-np.tanh(step/3-1))\n",
    "        \n",
    "        if intermediate_loss < best_loss : best_loss = intermediate_loss\n",
    "        \n",
    "        # Report intermediate objective value\n",
    "        trial.report(intermediate_loss, step)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac465be5",
   "metadata": {},
   "source": [
    "> **Task**: Create and optimize an Optuna study with the [`SuccessiveHalvingPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html). Initialize the study to minimize the objective, using `objective_with_pruning`. Observe the pruning effect on trial completions and optimization efficiency. Analyze the best parameters found. Refer to Optuna's documentation on [creating a study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html) and [optimizing it](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c46bc6-0024-4f97-a427-04de9047cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_pruner = todo()\n",
    "\n",
    "asha_study = optuna.create_study(direction='minimize', \n",
    "                                 pruner=asha_pruner)\n",
    "\n",
    "start_asha = time.time()\n",
    "asha_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_asha = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf7b98-428a-427d-b74f-25681972f923",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', \n",
    "                                                     reduction_factor=4,  \n",
    "                                                     min_early_stopping_rate=0, \n",
    "                                                     bootstrap_count=0)\n",
    "\n",
    "asha_study = optuna.create_study(direction='minimize', \n",
    "                                 pruner=asha_pruner)\n",
    "\n",
    "start_asha = time.time()\n",
    "asha_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_asha = time.time()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659734ea-021e-4399-b4ec-5c14f80d7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ASHA,\\n Best result : {asha_study.best_trial.value}\\n Best config : {asha_study.best_params}, in {end_asha-start_asha}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e954da-4eee-489b-8a00-8e874b7aa72f",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Task**: Create and optimize an Optuna study with the [`HyberbandPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyberbandPruner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35bae6-b749-4ccc-883e-4efcc8a03ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bohb_pruner = todo()\n",
    "\n",
    "bohb_study = optuna.create_study(direction=\"minimize\",\n",
    "                                 pruner=bohb_pruner)\n",
    "\n",
    "start_bohb = time.time()\n",
    "bohb_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_bohb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b1d05-6588-41c1-8076-fe5dbbf5b581",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "bohb_pruner = optuna.pruners.HyperbandPruner(min_resource=1, \n",
    "                                             max_resource=\"auto\", \n",
    "                                             reduction_factor=3)\n",
    "\n",
    "bohb_study = optuna.create_study(direction=\"minimize\",\n",
    "                                 pruner=bohb_pruner)\n",
    "\n",
    "start_bohb = time.time()\n",
    "bohb_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_bohb = time.time()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f71d3-3bac-45e5-96f1-e1d56a561c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BOHB,\\n Best result : {bohb_study.best_trial.value}\\n Best config : {bohb_study.best_params}, in {end_bohb-start_bohb}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(asha_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd389-9ef3-4bd7-a9c9-7a24d2bca6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_optuna_visualizations(bohb_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457a961-6518-4226-bb69-eab3ff4b2444",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa043f-2148-4c35-ab1d-166f3d417e08",
   "metadata": {},
   "source": [
    "## **HPO with Ray Tune (multi-workers)**\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a distributed hyperparameter tuning library that excels in optimizing machine learning models by efficiently managing and distributing trials. Its integration with the broader [Ray](https://ray.io/) ecosystem allows for scalable and parallel experimentation across clusters, making it ideal for tackling large-scale optimization tasks.\n",
    "\n",
    " Key Features:\n",
    "- **Scalability**: Handles large-scale experiments across multiple CPUs or GPUs with ease.\n",
    "- **Framework Agnostic**: Compatible with many ML frameworks like TensorFlow, PyTorch, and scikit-learn.\n",
    "- **Advanced Algorithms**: Supports advanced search algorithms and scheduling techniques, including Bayesian optimization and HyperBand.\n",
    "- **Integration with Optuna**: Ray Tune can leverage Optuna for its optimization algorithms, combining Ray Tune's efficient execution with Optuna's sophisticated sampling methods.\n",
    "\n",
    "Why Ray Tune?\n",
    "\n",
    "Choose Ray Tune for **complex, resource-intensive models** where parallel execution and efficient resource management are crucial. Its ability to integrate with Optuna combines the best of both worlds: Ray Tune's distributed computing capabilities with Optuna's powerful sampling strategies.\n",
    "\n",
    "The integration with Optuna can be achieved by using Optuna's samplers and search algorithms within Ray Tune's framework, offering a nuanced approach to hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c223dd-5096-4bb1-94d0-ad8eff93d214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray import train\n",
    "from ray.tune.search.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6957bc-34e3-4f8b-b16d-6a04f7acf262",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 16:50:12,726\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "#init ray cluster\n",
    "ray.init(num_cpus=10, ignore_reinit_error=True, log_to_driver=False) #20 logicals CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48199fc8-81f3-47a5-9507-45877e3320bf",
   "metadata": {},
   "source": [
    "Ray Tune is part of the Ray ecosystem. This therefore uses the master/slave/worker mechanisms present in [Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html). You can test ray Core by passing the following cell in *code* :"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0083958-b0d1-41b3-b667-543491565387",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define a function that will be executed remotely\n",
    "@ray.remote\n",
    "def worker_function(worker_id):\n",
    "    print(f\"Worker {worker_id} is starting...\")\n",
    "    time.sleep(2)  # Simulate some work\n",
    "    print(f\"Worker {worker_id} is done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize Ray\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    # Start two worker processes\n",
    "    worker1 = worker_function.remote(1)\n",
    "    worker2 = worker_function.remote(2)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Wait for both workers to finish\n",
    "    ray.get([worker1, worker2])\n",
    "\n",
    "    # Shutdown Ray\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b31cb02b-6440-471a-9e23-b48ad8c06184",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Do it once ;-)\n",
    "# Fix default ray_results PATH to $WORK\n",
    "# Avoid filling out the $HOME\n",
    "! mv $HOME/ray_resultsl $WORK\n",
    "! ln -s $WORK/ray_results $HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95cf099-91ba-4224-a5eb-42a4797c0583",
   "metadata": {},
   "source": [
    "### Define the Trainable Function\n",
    "\n",
    "Ray Tune use like Optuna *an objective* which is called here *a trainable*.\n",
    "The trainable can be a class or a callable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3025a-acc8-4a1d-8532-040ef9518a2e",
   "metadata": {},
   "source": [
    "> **Task**: Implement a simple trainable, reuse our previous synthetic loss function. You can check this documentation [`Tune trainable`](https://docs.ray.io/en/releases-2.9.2/tune/key-concepts.html#ray-tune-trainables). We will optimize on `loss` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fda5e4-1311-4c6d-a65a-108123585aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainable(config):\n",
    "    loss = todo()\n",
    "    time.sleep(5)\n",
    "    return {\"loss\":loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50326c-ee39-4077-8922-180676e5810d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "def trainable(config):\n",
    "    # Simulated model training and evaluation\n",
    "    loss = synthetic_loss_function(config[\"hyperparam1\"],config[\"hyperparam2\"],config[\"mode\"])\n",
    "    # Simalated training time\n",
    "    time.sleep(5)\n",
    "    return {\"loss\":loss}\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe3a37-6286-4f36-b4dc-2bbbb74f7c28",
   "metadata": {},
   "source": [
    "### Quick example with ray.run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699fd7c-828d-4bbf-afd3-0260208d9e22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    config={\n",
    "        \"hyperparam1\": tune.uniform(-5, 5),\n",
    "        \"hyperparam2\": tune.uniform(-5, 5),\n",
    "        \"mode\": tune.choice([\"simple\",\"complex\"])\n",
    "    },\n",
    "    num_samples=50,\n",
    "    max_concurrent_trials=10,\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902670c-3628-4729-a089-fefbfa3faac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Best loss = \" , analysis.best_result['loss'],\"with config: \", analysis.get_best_config(metric=\"loss\", mode=\"min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb51ba-6489-4448-b1af-487cb84c7ed6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c78f9f-818e-4902-8386-b94897a32d09",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploring tuning configurations with ray.tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d89dc8-57fb-44e2-a262-584bcc59ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_with_pruning(config):\n",
    "    accumulated_loss = 0\n",
    "    steps = 10 \n",
    "    best_loss = float('+inf')\n",
    "    \n",
    "    for step in range(1, steps + 1):\n",
    "        # we keep the same hyperparameters landscape\n",
    "        end_loss = synthetic_loss_function(config[\"hyperparam1\"],config[\"hyperparam2\"],config[\"mode\"])\n",
    "        # but we scale it to simulate a gradient descent !\n",
    "        intermediate_loss =  end_loss * (1.1-np.tanh(step/3-1))\n",
    "        \n",
    "        if intermediate_loss < best_loss : best_loss = intermediate_loss\n",
    "        \n",
    "        # Report intermediate objective value\n",
    "        train.report({\"iterations\": step, \"loss\" : intermediate_loss})\n",
    "        \n",
    "        # Simulate complex step training (full training = 0.4x10 = 4s)\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "    return {\"loss\" : best_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefe2b4-6290-49f4-8d45-c5e0e102bb51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_space = {\"hyperparam1\": tune.uniform(-5, 5),\n",
    "                \"hyperparam2\": tune.uniform(-5, 5),\n",
    "                \"mode\": tune.choice([\"simple\",\"complex\"])\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7d968-4715-43b6-8232-5a4c204f3662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import optuna\n",
    "from ray import train, tune\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "# Need pip install :\n",
    "#from ray.tune.search.hyperopt import HyperOptSearch\n",
    "#from ray.tune.search.bayesopt import BayesOptSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0097f20f-0fe9-4be3-a23b-278020ac3d00",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Task**: Set a Tune scheduler (=pruner) : [AsyncHyperband](https://docs.ray.io/en/releases-2.9.2/tune/api/doc/ray.tune.schedulers.AsyncHyperBandScheduler.html)\n",
    "\n",
    "> **Task**: Set a Optuna pruner with Tune : [OptunaSearch](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.search.optuna.OptunaSearch.html#ray.tune.search.optuna.OptunaSearch)\n",
    "\n",
    "We use a [TuneConfig](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.TuneConfig.html#ray.tune.TuneConfig) here to be cleaner :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb992cb4-1eec-42e3-a80f-0f7f9ed51a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = todo()\n",
    "algo = todo()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        scheduler=scheduler,\n",
    "        num_samples=100,\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable = trainable_with_pruning,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc0e8c-f7ca-4ed8-b4e1-82fd7887668f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "scheduler = AsyncHyperBandScheduler()\n",
    "algo = OptunaSearch(sampler= optuna.samplers.TPESampler())\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=10)\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        num_samples=30,\n",
    "        scheduler=scheduler)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable = trainable,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune_config,\n",
    ")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f429eb-9c65-4f34-855d-5f70a57324ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16e015-eca5-4f80-ba0e-64e5a6d0a6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Best result : {results.get_best_result().metrics['loss']} \\nBest config : {results.get_best_result().config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a234ae-f086-41df-a585-c926d1db57de",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4815f-6364-4031-98cf-57f0df68871b",
   "metadata": {},
   "source": [
    "### Performance Comparisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac16d6-5361-4b94-b2c3-5f6a5b12eeac",
   "metadata": {},
   "source": [
    "Until now we have used Ray as a simple wrapper to use Optuna without taking advantage of its experience parallelism functionality.\n",
    "We will now see the differences in performance as well as the possible gains to be made with Ray Tune parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f0339c-6951-42f2-b4b0-a2f8cfd4e6b9",
   "metadata": {},
   "source": [
    "> **Task**: Run the next cells & compare performance between Optuna & Ray (mono/multi-worker).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f0459-b24f-44e2-a814-a6ba3389f040",
   "metadata": {},
   "source": [
    "**mono-worker (Optuna)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3207e2-1759-4afe-8da7-c303b9d3b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_perf(trial):\n",
    "    time.sleep(4)\n",
    "    return objective_with_pruning(trial)\n",
    "\n",
    "\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', \n",
    "                                                     reduction_factor=4,  \n",
    "                                                     min_early_stopping_rate=0, \n",
    "                                                     bootstrap_count=0)\n",
    "\n",
    "asha_study = optuna.create_study(direction='minimize', \n",
    "                                 #pruner=asha_pruner,\n",
    "                                )\n",
    "\n",
    "start_asha = time.time()\n",
    "asha_study.optimize(objective_perf, n_trials=30,show_progress_bar=True)\n",
    "end_asha = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ad2ca-e951-4628-98e9-d709a5ce450c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"ASHA,\\n Best result : {asha_study.best_trial.value}\\n Best config : {asha_study.best_params}\\n HPO time : {end_asha-start_asha}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93839d6b-9bd9-4eaf-83e7-1c6b98ac810b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**mono-worker (Tune)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e12273-6b22-4855-a30f-e87d9941a072",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = AsyncHyperBandScheduler()\n",
    "algo = OptunaSearch(sampler= optuna.samplers.TPESampler())\n",
    "\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=1)\n",
    "tune_config = tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "        #scheduler=scheduler,\n",
    "        num_samples=30,\n",
    "        max_concurrent_trials=1\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable = trainable_with_pruning,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune_config,\n",
    ")\n",
    "\n",
    "start_mono = time.time()\n",
    "results_mono = tuner.fit()\n",
    "end_mono = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a8056f-6604-47e3-97a2-baadfd874820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Ray mono-worker,\\n Best result : {results_mono.get_best_result().metrics['loss']} \\n Best config : {results_mono.get_best_result().config} \\n HPO time : {end_mono-start_mono}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972457e-e33a-4744-888f-b6c9979f0eb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "**multi-worker (Tune)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58815326-2f44-4e23-a4bf-4fe7ba2b3aaa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = AsyncHyperBandScheduler()\n",
    "algo = OptunaSearch(sampler= optuna.samplers.TPESampler())\n",
    "\n",
    "trainable_with_rsc = tune.with_resources(trainable_with_pruning, {\"cpu\": 1})\n",
    "algo = ConcurrencyLimiter(algo, max_concurrent=2)\n",
    "tune_config = tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=30,\n",
    "        search_alg=algo,\n",
    "        #scheduler=scheduler,\n",
    "        #max_concurrent_trials=4\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable = trainable_with_rsc,\n",
    "    param_space = search_space,\n",
    "    tune_config = tune_config,\n",
    ")\n",
    "\n",
    "start_multi = time.time()\n",
    "results_multi = tuner.fit()\n",
    "end_multi = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bb1e5-b8f9-468f-9a2e-f3d705fa12da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Ray multi-worker,\\n Best result : {results_multi.get_best_result().metrics['loss']} \\n Best config : {results_multi.get_best_result().config} \\n HPO time : {end_multi-start_multi}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3636be8-2b7f-486d-90a1-991793fb86bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "If you have time left:\n",
    "> **Task**: Run the preceding cells by modifying certain values having an impact on the HPO temporal performance (as for example *the num_sample*, *the max_concurrent* or *the time.sleep() in trainable_with_pruning*)\n",
    "\n",
    "> **Task**: Same, by modifying sampler & pruner used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af1bd7-854e-467a-bf7d-4ce4b0a1569a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6076c2-a0d7-485c-8ea9-8118d30a8257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Optuna distributed throught Dask (Bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f40ab51-6a25-4bbf-89e0-7bfc598ec325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import optuna\n",
    "import optuna_distributed\n",
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -100, 100)\n",
    "    y = trial.suggest_categorical(\"y\", [-1, 0, 1])\n",
    "    # Some expensive model fit happens here...\n",
    "    time.sleep(1)\n",
    "    return x**2 + y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cluster = LocalCluster(n_workers=1, threads_per_worker=1)\n",
    "    client = Client(cluster)\n",
    "    \n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "\n",
    "    study = optuna_distributed.from_study(optuna.create_study(), client=client)\n",
    "    \n",
    "    start = time.time()\n",
    "    study.optimize(objective, n_trials=160)\n",
    "    print(f\"done in {time.time()-start}\")\n",
    "    print(study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8dafe-12e9-4df4-9043-bb00f7c4719b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Optuna distributed throught database (Bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f36c1-b6d0-4439-9269-1908681f5841",
   "metadata": {},
   "source": [
    "We create a new database with **SQLite**, easy to use, possible to use but bad performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595ce8e-743f-48c5-848a-85c3d9d31934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sqlite3 optuna_dist.db \".databases\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af93c1-eb6e-44ce-86b5-a49e3faefe02",
   "metadata": {},
   "source": [
    "Now, we have to create a new study to initialize our trials database. Here we are using cmdline, we can do it directly with python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bb36b-fa47-419d-a5d0-491663cef3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!optuna create-study --study-name \"distributed-example\" --storage \"sqlite:///optuna_dist.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29fd9-2a99-451e-a2e1-d9b0608c63f3",
   "metadata": {},
   "source": [
    "For this demonstration, we use a toy (and useless XD) HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaae7b8-6c57-4e33-9562-649cbe5c02f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile optuna_worker.py\n",
    "import optuna\n",
    "import time\n",
    "import random\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -10, 10)\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.load_study(\n",
    "        study_name=\"distributed-example\", storage=\"sqlite:///optuna_dist.db\"\n",
    "    )\n",
    "    start = time.time()\n",
    "    study.optimize(objective, n_trials=60)\n",
    "    print(f\"process run for {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaaa87-f530-4596-a6ce-afaee2a97297",
   "metadata": {},
   "source": [
    "We have everythings to run multiple process for our HPO. This line generate 4 processes which run the same HPO code. Each output color line mean a different process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa1b2a-1d32-4870-8023-ece54a59d184",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!seq 4 | xargs -I{} -P 4 sh -c 'python optuna_worker.py 2>&1 | while read -r line; do if [[ \"$line\" == *[* ]]; then echo -e \"\\033[3{}m${line}\\033[0m\"; else echo \"$line\"; fi; done'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee66c5-d992-4ecd-ae56-f7944def6558",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Optuna Visualisation time ! (doesn't work right now on jupyterhub)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772fa71-1aba-4924-acce-cb920d2c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/.local/bin/optuna-dashboard --port=\"8080\"  --server='wsgiref' sqlite:///optuna_dist.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888e83-b65a-4c69-be86-10cb41cae95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#f'jupyterhub.idris.fr{os.environ[\"JUPYTERHUB_SERVICE_PREFIX\"]}proxy/8080'\n",
    "import optuna_dashboard\n",
    "optuna_dashboard.run_server(storage=\"sqlite:///optuna_dist.db\", host=f'jupyterhub.idris.fr{os.environ[\"JUPYTERHUB_SERVICE_PREFIX\"]}proxy/8080')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7370f-2d8a-4916-a83a-22248b31e99b",
   "metadata": {},
   "source": [
    "usage with InMemoryStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cfc8e-b576-453c-9991-83b56dcc9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -100, 100)\n",
    "    y = trial.suggest_categorical(\"y\", [-1, 0, 1])\n",
    "    return x**2 + y\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(storage=storage)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "run_server(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221ec23-cbe3-4cc4-bcb0-923194807120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.3.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.3.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
