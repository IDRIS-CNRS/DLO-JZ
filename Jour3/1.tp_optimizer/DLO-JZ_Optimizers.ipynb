{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DLO-JZ Optimizers and Large Batch - Day 3\n",
    "\n",
    "The cells in this *notebook* are not meant to be modified, except for rare exceptions indicated in the comments.\n",
    "\n",
    "*Notebook written by the IDRIS AI assistance team*\n",
    "* Translation to english, June 2024\n",
    "* Adaptation to the new format, February 2024\n",
    "* modification of lrfinder & cifar10, October 2023\n",
    "* addition of Lion, July 2023\n",
    "* creation, October 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is intended to be run from a front-end machine of Jean-Zay. The *hostname* should be jean-zay[1-5] or jean-zay-srv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *python* functions for SLURM queue management developed by IDRIS and the functions dedicated to the DLO-JZ training are to be imported.\n",
    "\n",
    "The environment module for the *jobs* and the image size are set for this *notebook*.\n",
    "\n",
    "**TODO:** choose a *nickname* (maximum 5 characters) to differentiate yourself in the SLURM queue and in collaborative tools during the training and competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools_tp import plot_accuracy, lrfind_plot, plot_accuracy_lr, model_vizu, update_vizu\n",
    "MODULE = 'pytorch-gpu/py3/2.3.0'\n",
    "account = 'for@a100'\n",
    "n_gpu = 1\n",
    "\n",
    "name = 'pseudo'  #TODO#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset and Model\n",
    "\n",
    "For this lab, we will use the CIFAR 10 database and the Resnet-18 model to be able to run complete trainings in a reasonable amount of time. The lab will be conducted by modifying the [cifar10.py](cifar10.py) code.\n",
    "\n",
    "### CIFAR 10\n",
    "\n",
    "#### Train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "        transforms.RandomHorizontalFlip(),   # Horizontal Flip - Data Augmentation\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor()                # Convert the PIL Image to a tensor\n",
    "        ])\n",
    "    \n",
    "    \n",
    "train_dataset = torchvision.datasets.CIFAR10(root=os.environ['ALL_CCFRSCRATCH']+'/CIFAR_10',\n",
    "                                             train=True, download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,    \n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print('X train batch, shape: {}, data type: {}, Memory usage: {} bytes'\n",
    "      .format(batch[0].shape, batch[0].dtype, batch[0].element_size()*batch[0].nelement()))\n",
    "print('Y train batch, shape: {}, data type: {}, Memory usage: {} bytes'\n",
    "      .format(batch[1].shape, batch[1].dtype, batch[1].element_size()*batch[1].nelement()))\n",
    "\n",
    "\n",
    "img = batch[0][0].numpy().transpose((1,2,0))\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "_ = plt.title('label class: {}'.format(batch[1][0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([transforms.ToTensor()])# convert the PIL Image to a tensor\n",
    "    \n",
    "val_dataset = torchvision.datasets.CIFAR10(root=os.environ['ALL_CCFRSCRATCH']+'/CIFAR_10',\n",
    "                                               train=False, download=False, transform=val_transform)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(num_classes=10)\n",
    "print('number of total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "print('number of trainable parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Description\n",
    "\n",
    "We will study 4 optimizers (SGD, AdamW, LAMB, LARS, and Lion).\n",
    "\n",
    "Each time, we will look at the case of **Small Batch** learning and **Large Batch** learning.\n",
    "\n",
    " * **Small Batch**: *Global Batch Size* of **256** on 1 GPU over **30** *epochs* (or **5880** *training steps*)\n",
    " * **Large Batch**: *Global Batch Size* of **8192** on 1 GPU over **75** *epochs* (or **525** *training steps*)\n",
    "\n",
    "**Note**:\n",
    "\n",
    "The *wrapped_optimizer* parameter is present in the code due to the specific implementation of LARS. This is because the *LR scheduler* must take the base *SGD optimizer* as input, not the *wrapped* one.\n",
    "\n",
    "For the other optimizers, it serves no purpose. But this trick allows us to switch to each type of *optimizer* easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_0: Constant *Learning Rate* (reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Rate Finder (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether with a constant *learning rate* or with a *Cycle Scheduler*, we first need to find the range of *learning rate* values that will have a positive effect on the model's learning.\n",
    "\n",
    "Before the lab, we ran a script performing a few iterations for various *learning rates*. If you're curious, you can look at the *lrfinder_cifar10.py* script.\n",
    "\n",
    "Below, you can observe the loss curves as a function of the *learning rate* for two batch sizes, knowing that the optimizer is *SGD*.\n",
    "\n",
    "From these curves, you should be able to identify a scale of *learning rates* that perform better than others.\n",
    "For \"similar\" performances with the *lr finder*, between two *learning rates*, the smaller one will probably have a more stable behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_SGD_256](images_lrfinder/lr_decorelated_SGD_256.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_SGD_8192](images_lrfinder/lr_decorelated_SGD_8192.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will launch a reference training with a constant *learning rate*.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen *learning rate* value**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 0. --lr {lr_smallb}', \n",
    "           f'cifar10.py -b 8192 -e 75 --wd 0. --lr {lr_largeb}']\n",
    "jobids_ref = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobids_ref = {jobids_ref}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobids_ref = ['400133', '400134']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobids=jobids_ref[0]\n",
    "plot_accuracy_lr(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobids=jobids_ref[1]\n",
    "plot_accuracy_lr(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_1 : *One Cycle Learning Rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now modify the code to replace the constant *learning rate* with a *One Cycle Scheduler* and compare the result with the reference training.\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Find the line declaring the *Learning Rate Scheduler*:\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1, total_iters=5)\n",
    "```\n",
    "* Replace it with a *One Cycle Scheduler*:\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.lr, steps_per_epoch=N_batch, epochs=args.epochs)\n",
    "```\n",
    "\n",
    "__Note__: PyTorch's *OneCycleLR* automatically calculates a minimum *learning rate* from the given maximum value.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen maximum *learning rate* values (Small Batch and Large Batch) using the *learning rate finder***\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 5e-4 --lr {lr_smallb}', f'cifar10.py -b 8192 -e 75 --wd 5e-4 --lr {lr_largeb}',\n",
    "           f'cifar10.py -b 256 -e 30 --wd 0. --lr {lr_smallb}', f'cifar10.py -b 8192 -e 75 --wd 0. --lr {lr_largeb}']\n",
    "jobid_sgd = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_sgd = {jobid_sgd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobid_sgd = ['153761', '153762', '153763', '153764']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that we have run trainings with and without weight decay.** So you can estimate the effect of *weigh decay* + *one cycle scheduler* on the *training* curve.\n",
    "\n",
    "Then, you can compare the *test accuracy* and *learning rate* curves with the reference training above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobids=[jobid_sgd[2],jobid_sgd[0]]\n",
    "plot_accuracy_lr(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobids=[jobid_sgd[3],jobid_sgd[1]]\n",
    "plot_accuracy_lr(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_2: *AdamW* Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now modify the optimizer to use *AdamW*.\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Find the line declaring the *Stochastic Gradient Descent* optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.mom, weight_decay=args.wd)\n",
    "```\n",
    "* Replace it with the *AdamW* optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(model.parameters(), args.lr, betas=(args.mom, 0.999), weight_decay=args.wd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Rate Finder (AdamW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the optimizer has changed, we need to determine the maximum *learning rate* value to provide as a parameter.\n",
    "\n",
    "As with the previous optimizers, we suggest the following *learning rate* exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_AdamW_256](images_lrfinder/lr_decorelated_AdamW_256.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_AdamW_8192](images_lrfinder/lr_decorelated_AdamW_8192.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training (AdamW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch the training with the *AdamW* optimizer.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen maximum *learning rate* values (Small Batch and Large Batch)**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 1e-2 --lr {lr_smallb}', \n",
    "           f'cifar10.py -b 8192 -e 75 --wd 1e-1 --lr {lr_largeb}']\n",
    "jobid_adamw = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_adamw = {jobid_adamw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobid_adamw = ['400755', '400757']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the *test accuracy* and *train accuracy* curves with the previous trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[0], jobid_adamw[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[1], jobid_adamw[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_3: *LAMB* Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now modify the optimizer to use *LAMB*.\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Replace the *AdamW* optimizer with the *LAMB* optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = apex.optimizers.FusedLAMB(model.parameters(), args.lr, betas=(args.mom, 0.999), weight_decay=args.wd)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Rate Finder (LAMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find the maximum *learning rate* value to provide as a parameter for this optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_FusedLAMB_256](images_lrfinder/lr_decorelated_FusedLAMB_256.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_FusedLAMB_8192](images_lrfinder/lr_decorelated_FusedLAMB_8192.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training (LAMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch the training with the LAMB optimizer.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen maximum *learning rate* values (Small Batch and Large Batch)**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 5e-2 --lr {lr_smallb}', \n",
    "           f'cifar10.py -b 8192 -e 75 --wd 5e-1 --lr {lr_largeb}']\n",
    "jobid_lamb = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_lamb = {jobid_lamb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobid_lamb = ['401241', '401282']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the *test accuracy* and *train accuracy* curves with the previous trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[0], jobid_adamw[0], jobid_lamb[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[1], jobid_adamw[1], jobid_lamb[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_4: *LARS* Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will try a large batch training with the LARS or LARC optimizer (Apex optimization of LARS).\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Replace the *LAMB* optimizer with the *LARC* optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = ...\n",
    "\n",
    "wrapped_optimizer = optimizer\n",
    "```\n",
    "with\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.mom, weight_decay=args.wd)\n",
    "    \n",
    "wrapped_optimizer = LARC(optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find the maximum *learning rate* value to provide as a parameter for this optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_LARC_256](images_lrfinder/lr_decorelated_LARC_256.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_LARC_8192](images_lrfinder/lr_decorelated_LARC_8192.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training (LARS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch the training with the LARS optimizer.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen maximum *learning rate* values (Small Batch and Large Batch)**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 5e-3 --lr {lr_smallb}', \n",
    "           f'cifar10.py -b 8192 -e 75 --wd 5e-3 --lr {lr_largeb}']\n",
    "jobid_lars = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_lars = {jobid_lars}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobid_lars = ['403084', '403085']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the *test accuracy* and *train accuracy* curves with the previous trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[0], jobid_adamw[0], jobid_lamb[0], jobid_lars[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[1], jobid_adamw[1], jobid_lamb[1], jobid_lars[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TP_opti_5: *LION* Optimizer (experimental ⚠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will test an optimizer from early 2023: LION.\n",
    "\n",
    "It is an optimizer from the publication: Symbolic Discovery of Optimization Algorithms, https://arxiv.org/abs/2302.06675\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Replace the *LARC* optimizer with the *LION* optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.mom, weight_decay=args.wd)\n",
    "\n",
    "wrapped_optimizer = LARC(optimizer)\n",
    "```\n",
    "with\n",
    "\n",
    "```python\n",
    "optimizer = Lion(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    \n",
    "wrapped_optimizer = optimizer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find the maximum *learning rate* value to provide as a parameter for this optimizer.\n",
    "\n",
    "**In addition to the *learning rate scheduler*, here are some indications from the authors of the paper:**\n",
    "> *Based on our experience, a suitable learning rate for Lion is typically 3-10x smaller than that for AdamW. Since the effective weight decay is lr * λ, the value of decoupled weight decay λ used for Lion is 3-10x larger than that for AdamW in order to maintain a similar strength.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_LARC_256](images_lrfinder/lr_decorelated_LARC_256.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![lr_decorelated_LARC_8192](images_lrfinder/lr_decorelated_LARC_8192.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training (LION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch the training with the LION optimizer.\n",
    "\n",
    "Job submission. **Be careful, you are requesting the compute nodes at this time**.\n",
    "\n",
    "To submit the job, please switch the following cell from `Raw NBConvert` mode to `Code` mode.\n",
    "\n",
    "**TODO: replace XXX with the chosen maximum *learning rate* values (Small Batch and Large Batch). \n",
    "You will also need to define the *weight decay* values for each batch size.**\n",
    "\n",
    "**For relatively small batches, you should be able to achieve good performance using the learning rate finder and the authors' recommendations.**\n",
    "\n",
    "**The case for large batches is not as straightforward. In our tests, we couldn't identify truly more effective parameters.**\n",
    "\n",
    "**You can start with the same *learning rate scheduler* as before, but we strongly encourage you to experiment with changes to it.**\n",
    "\n",
    "**TODO**: in the script [cifar10.py](cifar10.py):\n",
    "* Change the learning rate scheduler to a CosineAnnealingLR for which LION's behavior is more consistent.\n",
    "\n",
    "```python\n",
    "scheduler = ...\n",
    "```\n",
    "with\n",
    "\n",
    "```python\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                       T_max=N_batch*args.epochs, \n",
    "                                                       eta_min=args.lr/5)\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "lr_smallb = XXX\n",
    "lr_largeb = XXX\n",
    "command = [f'cifar10.py -b 256 -e 30 --wd 5e-1 --lr {lr_smallb}', \n",
    "           f'cifar10.py -b 8192 -e 75 --wd 3e-1 --lr {lr_largeb}']\n",
    "jobid_lion = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_lion = {jobid_lion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#jobid_lion = ['404951', '404955']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "You can compare the *test accuracy* and *train accuracy* curves with the previous trainings.\n",
    "\n",
    "#### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[0], jobid_adamw[0], jobid_lamb[0], jobid_lars[0], jobid_lion[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Large Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy([jobid_sgd[1], jobid_adamw[1], jobid_lamb[1], jobid_lars[1], jobid_lion[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "--------------\n",
    "\n",
    "## Appendix (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the optimizer of your choice (You will need to modify the code accordingly):\n",
    "\n",
    "You can perform additional tests by adjusting the different parameters:\n",
    "* Number of epochs\n",
    "* The value of *weight decay*\n",
    "* The value of *learning rate*\n",
    "* Batch size. Note: **on 2 GPUs**, so the *batch* size will be **multiplied by 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = \n",
    "weight_decay = \n",
    "lr = \n",
    "batch_size = \n",
    "command = f'cifar10.py -b {batch_size} -e {n_epoch} --wd {weight_decay} --lr {lr}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR finder (Optionnel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "command_lr = f'cifar10.py -b {batch_size} -e 20 --findlr --lr 5. --wd {weight_decay}'\n",
    "jobid_test_lrf = gpu_jobs_submitter(command_lr, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_test_lrf = {jobid_test_lrf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid_test_lrf ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfind_plot(jobid_test_lrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jobid_test = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00')\n",
    "print(f'jobid_test = {jobid_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid_test = ['428']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(jobid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.3.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.3.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
