{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLO-JZ Data parallism ZeRO et Pipeline parallelism - Jour 4\n",
    "\n",
    "Comparatif des différents types de parallèlisme sur un gros Vision Transformer **CoAtNet**.\n",
    "\n",
    "![Monstertruck](./images/MonsterTruck.png)\n",
    "\n",
    "\n",
    "## Objet du notebook\n",
    "\n",
    "Le but de ce *notebook* est d'optimiser un code d'apprentissage d'un modèle *CoAtNet-7* sur *Imagenet* pour Jean Zay en implémentant :\n",
    "* **TP 1** : Passage à CoAtNet\n",
    "* **TP 2** : Pipeline parallelism avec PyTorch\n",
    "* **TP 3** : Deepspeed - ZeRo Data Parallelism\n",
    "* **TP 4** : Deepspeed - Pipeline Parallelism et comparatif\n",
    "\n",
    "\n",
    "Les cellules dans ce *notebook* ne sont pas prévues pour être modifiées, sauf rares exceptions indiquées dans les commentaires. Les TP se feront en modifiant le code `dlojz.py`.\n",
    "\n",
    "Les directives de modification seront marquées par l'étiquette **TODO :** dans le *notebook* suivant.\n",
    " \n",
    "Les solutions sont présentes dans le répertoire `solutions/`.\n",
    "\n",
    "*Notebook rédigé par l'équipe assistance IA de l'IDRIS, juin 2023*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un module PyTorch doit avoir été chargé pour le bon fonctionnement de ce Notebook. **Nécessairement**, le module `pytorch-gpu/py3/1.11.0` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions *python* de gestion de queue SLURM dévelopées par l'IDRIS et les fonctions dédiées à la formation DLO-JZ sont à importer.\n",
    "\n",
    "Le module d'environnement pour les *jobs* et la taille des images sont fixés pour ce *notebook*.\n",
    "\n",
    "**TODO :** choisir un *pseudonyme* (maximum 5 caractères) pour vous différencier dans la queue SLURM et dans les outils collaboratifs pendant la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools import controle_technique, compare, GPU_underthehood, plot_accuracy, lrfind_plot, pipe_memory, turbo_profiler\n",
    "MODULE = 'pytorch-gpu/py3/1.13.0'\n",
    "image_size = 224\n",
    "account = 'for@v100'\n",
    "name = 'pseudo'   ## TODO Pseudonyme à choisir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion de la queue SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie permet d'afficher et de gérer la queue SLURM.\n",
    "\n",
    "Pour afficher toute la queue *utilisateur* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**: Cette fonction utilisée plusieurs fois dans ce *notebook* permet d'afficher la queue de manière dynamique, rafraichie toutes les 5 secondes. Cependant elle ne s'arrête que lorsque la queue est vide. Si vous désirez reprendre la main sur le *notebook*, il vous suffira d'arrêter manuellement la cellule avec le bouton *stop*. Cela a bien sûr aucun impact sur le *scheduler* SLURM. Les *jobs* ne seront pas arrêtés.\n",
    "\n",
    "Si vous voulez arrêter des *jobs* dans la queue :\n",
    "* Annuler tous vos *jobs* dans la queue (décommenter la ligne suivante)\n",
    "* Annuler un *job* dans votre queue (décommenter la ligne suivante et ajouter le numéro du *job* à la fin de la ligne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!scancel -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie *debug* permet d'afficher les fichiers de sortie et les fichiers d'erreur du *job*.\n",
    "\n",
    "Il est nécessaire dans la cellule suivante (en décommentant) d'indiquer le *jobid* correspondant sous le format suivant.\n",
    "\n",
    "***Remarque*** : dans ce notebook, lorsque vous soumettrez un job, vous recevrez en retour le numéro du job dans le format suivant : `jobid = ['123456']`. La cellule ci-dessous peut ainsi être facilement actualisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = ['2088207']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier de sortie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier d'erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Différence de scripts <a id='diff_scripts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le *debug* ou pour comparer son code avec les solutions mises à disposition, la fonction suivante permet d'afficher une page html contenant un différentiel de fichiers texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"dlojz.py\"\n",
    "s2 = \"./solutions/dlojz4_2.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le résultat du différentiel de fichiers sur la page suivante (attention au spoil !) :\n",
    "\n",
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "# TP4_0 : Préparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : copier-coller la solution `solutions/dlojz4_0.py` dans `dlojz.py` afin d'ajouter dans le code les 2 éléments suivants nécessaires pour la suite des TP :\n",
    "* utiliser une taille d'image équivalente pour la *validation* et le *training* car *CoatNet* n'a pas la même souplesse que *ResNet*, il nécessite une même taille d'image (multiple de 32).\n",
    "* afficher dans les *logs* l'empreinte mémoire de **tous** les GPU.\n",
    "\n",
    "**À noter** : Pendant tout le TP, nous utiliserons une taille d'image de 352 x 352, qui correspond à la taille classique utilisée pour ce modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour visualiser ces changements, veuillez utiliser le différentiel de fichiers suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"dlojz.py\"\n",
    "s2 = \"./solutions/dlojz4_0.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copier/coller la solution si nécessaire\n",
    "!cp solutions/dlojz4_0.py dlojz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4_1 : CoAtNet\n",
    "\n",
    "Ce TP consiste à lister les versions du modèle *CoATNet*, de l'appliquer à notre code et de juger des problématques liées aux gros modèles.\n",
    "\n",
    "### Liste des versions de CoAtNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 1\n",
    "batch_size = 32\n",
    "command = f'CoAtNet/coatnet.py'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'éviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1790096']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(name, contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoAtNet-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : dans le script `dlojz.py` :\n",
    "\n",
    "* Importer la description des architectures *CoAtNet*.\n",
    "\n",
    "```python\n",
    "from CoAtNet.coatnet import coatnet_6\n",
    "```\n",
    "* Remplacer :\n",
    "\n",
    "  `model = models.resnet50()` par `model = coatnet_6((args.image_size,args.image_size))`\n",
    "\n",
    "  et \n",
    "\n",
    "  `archi_model = 'Resnet-50'` par `archi_model = 'CoAtNet-6'`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "batch_size = 4\n",
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculler la cellule précédente en mode `Raw NBConvert`, afin d'éviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1790206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 1\n",
    "batch_size = [2, 4, 6, 8]\n",
    "command = [f'dlojz.py -b {b} --image-size {image_size} --test'\n",
    "          for b in batch_size]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['1790142', '1790143', '1790144', '1790146']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commentaires](images/cedez.png \"Assurez-vous que tout se passe bien avant de continuer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4_2 : Pipelined Parallelism de PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP consiste à implémenter le *Pipelined Parallelism* de PyTorch et de comparer cette solution avec les autres solutions.\n",
    "\n",
    "La principale contrainte induite est de structurer le modèle comme suit, avec des `torch.nn.Sequential` pour chaque section et pour le modèle entier :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline pytorch](images/pipeline2pytorch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Pipeline Parallelism* de PyTorch est de type standard **GPipe**.\n",
    "\n",
    "![G Pipe](images/gpipe.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À noter** : Le code modifié permettra de faire de l'*Hybrid Parallelism* (DP + PP). \n",
    "\n",
    "Chaque instance créée par *Data Parallelism* sera associée à une *task* Slurm, et chacune de ces instances pourra elle-même sollliciter plusieurs GPU pour tourner en mode *Pipelined Parallelism*.\n",
    "\n",
    "Dans notre cas, nous testerons le code seulement en mode *Pipelined Parallelism*, sur 1 *task* associée à 4 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : dans le script `dlojz.py`:\n",
    "\n",
    "* Importer les fonctions nécessaires.\n",
    "\n",
    "```python\n",
    "from torch.distributed.pipeline.sync import Pipe\n",
    "import tempfile\n",
    "from torch.distributed import rpc\n",
    "```\n",
    "* Ajouter l'argument `--chunks` (pour le nombre de *micro batches*) avant le *parser* les arguments.\n",
    "\n",
    "```python\n",
    "parser.add_argument('--chunks', default=1, type=int, help='number of chunks for Pipelined Parallelism')\n",
    "      \n",
    "args = parser.parse_args()\n",
    "\n",
    "```\n",
    "\n",
    "* Initialiser le *Framework RPC*, juste après la configuration de la distribution.\n",
    "\n",
    "```python\n",
    "# Initialize RPC Framework, Pipe depends on it\n",
    "tmpfile = tempfile.NamedTemporaryFile()\n",
    "rpc.init_rpc(\n",
    "    name=f'worker{idr_torch.rank}',\n",
    "    rank=0,\n",
    "    world_size=1,\n",
    "    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n",
    "        init_method=\"file://{}\".format(tmpfile.name),\n",
    "\t    # Specifying _transports and _channels is a workaround and we no longer\n",
    "        # will have to specify _transports and _channels for PyTorch \n",
    "        # versions >= 1.8.1 (Not True for Jean Zay)\n",
    "\t    # With Jean Zay, _transports must be equal to [\"shm\", \"uv\"] and not [\"ibv\", \"uv\"]\n",
    "        _transports=[\"shm\", \"uv\"],\n",
    "        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "* Structurer le modèle pour le *Pipelined Parallelism*.\n",
    "\n",
    "```python\n",
    "# define model\n",
    "model = coatnet_6((args.image_size,args.image_size))\n",
    "\n",
    "# How many sections\n",
    "nb_part = torch.cuda.device_count()//int(os.environ['SLURM_NTASKS_PER_NODE']) \n",
    "# device number where the first part of the model will run\n",
    "first_part = idr_torch.local_rank*nb_part\n",
    "# list of devices involved for pipelined Parallelism\n",
    "gpus = [g for g in range(first_part, first_part+nb_part)]\n",
    "\n",
    "class LambdaModule(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super().__init__()\n",
    "        assert isinstance(lambd, type(lambda x: x))\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "lambda_fc = LambdaModule(lambda x: x.view(-1, 2048))\n",
    "\n",
    "section0 = torch.nn.Sequential(*model.s0, *model.s1, *model.s2, *model.pres3).to(gpus[0])\n",
    "section1 = torch.nn.Sequential(*model.s3[:15]).to(gpus[1])\n",
    "section2 = torch.nn.Sequential(*model.s3[15:30]).to(gpus[2])\n",
    "section3 = torch.nn.Sequential(*model.s3[30:], *model.s4, model.pool, lambda_fc, model.fc).to(gpus[3])\n",
    "pipe_model = torch.nn.Sequential(*section0, *section1, *section2, *section3)\n",
    "\n",
    "# Pipe the model, chunks=n means that the batch (size according to batch size) will be shared to n micro batches (size = batch_size/chunks)\n",
    "model = Pipe(pipe_model, chunks=args.chunks, checkpoint=\"never\")\n",
    "\n",
    "archi_model = 'CoAtNet-6'\n",
    "```\n",
    "\n",
    "* Modifier la déclaration du `DistributedDataParallel` pour prendre en compte le fait qu'il y a plusieurs GPU associés à une seule *task* pour le *Pipelined Parallelism*, en indiquant simplement :\n",
    "\n",
    "```python\n",
    "model = DistributedDataParallel(model)\n",
    "```\n",
    "* Envoyer les métriques de *validation* au dernier *device* du *Pipe*.\n",
    "\n",
    "```python\n",
    "## Initialisation  \n",
    "if idr_torch.rank == 0: accuracies = []\n",
    "val_loss = torch.Tensor([0.]).to(gpus[-1])                  # send to GPU\n",
    "val_accuracy = torch.Tensor([0.]).to(gpus[-1])              # send to GPU\n",
    "```\n",
    "* Dans les boucles de *training* et de *validation*, envoyer les *Input*/images au premier GPU et les *labels* au dernier GPU.\n",
    "\n",
    "```python\n",
    "# distribution of images and labels to all GPUs\n",
    "images = images.to(gpus[0], non_blocking=args.non_blocking)\n",
    "labels = labels.to(gpus[-1], non_blocking=args.non_blocking)\n",
    "```\n",
    "   et \n",
    "\n",
    "```python\n",
    "# distribution of images and labels to all GPUs\n",
    "val_images = val_images.to(gpus[0], non_blocking=args.non_blocking)\n",
    "val_labels = val_labels.to(gpus[-1], non_blocking=args.non_blocking)\n",
    "```\n",
    "\n",
    "* La sortie du modèle *Pipelined* est au format `Rref`, il faudra utiliser la méthode `.local_value()` pour le transformer en tenseur pour le calcul de la *loss*, dans les boucles de *training* et de *validation*.\n",
    "\n",
    "```python\n",
    "# Runs the forward pass with autocasting.\n",
    "with autocast():\n",
    "    outputs = model(images).local_value()\n",
    "    loss = criterion(outputs, labels)\n",
    "```\n",
    "et\n",
    "\n",
    "```python\n",
    "# Runs the forward pass with no grade mode.\n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        val_outputs = model(val_images).local_value()\n",
    "        loss = criterion(val_outputs, val_labels)\n",
    "```\n",
    "\n",
    "* Ajouter pour les logs, la mesure de l'empreinte mémoire sur tous les GPU avec la ligne suivante après la boucle d'apprentissage.\n",
    "\n",
    "```python\n",
    "else:                                                                                                          #\n",
    "    print(f'MaxMemory for GPU:{idr_torch.rank} {torch.cuda.max_memory_allocated()} Bytes')                                   #\n",
    "#***************************************************************************************************************\n",
    "for g in gpus: print(f'MaxMemory for GPU:{g} {torch.cuda.max_memory_allocated(device=g)} Bytes') \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "batch_size = 32 \n",
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --chunks 4 --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name, n_gpu_per_task=4, \n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculler la cellule précédente en mode `Raw NBConvert`, afin d'éviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1790229']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_memory(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "chunks = [(16,2), (32,4), (48,6), (56,7), (64,8)]\n",
    "command = [f'dlojz.py -b {c[0]} --image-size {image_size} --chunks {c[1]} --test'\n",
    "          for c in chunks]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,  n_gpu_per_task=4, \n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['1790423', '1790425', '1790428', '1790429', '1790430']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids, calcul_memo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique([jobids[-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commentaires](images/cedez.png \"Assurez-vous que tout se passe bien avant de continuer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4_3 : Deepspeed\n",
    "\n",
    "### Préparation\n",
    "\n",
    "Il faut enlever le *Pipelined Parallelism* du fichier `dlojz.py`. Nous vous proposons de copier-coller la solution du TP4_1 pour revenir à l'état précédent.\n",
    "\n",
    "**TODO** :\n",
    "\n",
    "* Copier-coller la solution `solutions/dlojz4_1.py` dans le fichier `dlojz.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copier/coller la solution si nécessaire\n",
    "!cp solutions/dlojz4_1.py dlojz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation de deepspeed\n",
    "\n",
    "Ce TP consiste à implémenter *Deepspeed* pour intégrer l'optimisation **ZeRO** pour le *Data Parallelism*.\n",
    "\n",
    "\n",
    "**TODO** : dans le script `dlojz.py` :\n",
    "\n",
    "* Importer *Deepspeed*.\n",
    "\n",
    "```python\n",
    "import deepspeed\n",
    "```\n",
    "* Intégrer la configuration de *Deepspeed* par fichier de configuration *json* dans le *parser* d'arguments.\n",
    "\n",
    "```python\n",
    "# Include DeepSpeed configuration arguments\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "```\n",
    "* Remplacer le mécanisme de distribution de PyTorch par celui de *Deepspeed* :\n",
    "\n",
    "À la place de :\n",
    "```python\n",
    "# configure distribution method: define rank and initialise communication backend (NCCL)\n",
    "dist.init_process_group(backend='nccl', init_method='env://',\n",
    "                        world_size=idr_torch.size, rank=idr_torch.rank)\n",
    "...\n",
    "model = model.to(gpu)\n",
    "...\n",
    "model = DistributedDataParallel(model, device_ids=[idr_torch.local_rank])\n",
    "...\n",
    "\n",
    "```\n",
    "mettre :\n",
    "```python\n",
    "# Deepspeed initialization - force port number if several job run on the same node \n",
    "deepspeed.init_distributed(distributed_port=os.environ['MASTER_PORT'])\n",
    "model_engine, optimizer, _, scheduler = deepspeed.initialize(args=args,\n",
    "                                                     model=model, \n",
    "                                                     model_parameters=model.parameters()\n",
    "                                                     )\n",
    "```\n",
    "\n",
    "**À noter** : Nous garderons, comme indiqué dans la documentation de *Deepspeed*, la distinction entre le modèle PyTorch `model` et le modèle encapsulé avec *Deepspeed* `model_engine`.\n",
    "\n",
    "* Appliquer le nouveau modèle dans l'étape de *forward*.\n",
    "\n",
    "```python\n",
    "outputs = model_engine(images)\n",
    "\n",
    "```\n",
    "et\n",
    "\n",
    "```python\n",
    "val_outputs = model_engine(val_images)\n",
    "```\n",
    "\n",
    "* **Désactiver l'AMP**. \n",
    "\n",
    "En effet, l'optimisation ZeRO ne supporte pas l'*Automatic Mixed Precision*. À la place, on appliquera une précision `float16` à l'ensemble des paramètres du modèle (cela se fera dans la configuration *json*). \n",
    "\n",
    "Pour retrouver les lignes de code à modifier dans le script `dlojz.py`, vous pouvez utiliser [l'outil de différentiel de texte](#diff_scripts) entre la solution `dlojz1_1.py` et la solution `dlojz1_2.py`.\n",
    "\n",
    "\n",
    "* *Caster* les données d'entrée en `float16` afin qu'elles correspondent à la précision du modèle :\n",
    "\n",
    "```python\n",
    "images = images.half().to(gpu, non_blocking=args.non_blocking, memory_format=torch.channels_last)\n",
    "\n",
    "```\n",
    "et\n",
    "\n",
    "```python\n",
    "val_images = val_images.half().to(gpu, non_blocking=args.non_blocking, memory_format=torch.channels_last)\n",
    "```\n",
    "\n",
    "* Déléguer les étapes de *backward* et d'actualisation des poids à *Deepspeed* dans la boucle de *training* en remplaçant :\n",
    "\n",
    "```python\n",
    "# backward and optimize\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "par\n",
    "\n",
    "```python\n",
    "#runs backpropagation\n",
    "model_engine.backward(loss)\n",
    "\n",
    "#weight update\n",
    "model_engine.step()\n",
    "\n",
    "```\n",
    "\n",
    "* Effacer ou commenter le bloc suivant, puisque l'on utilisera le mécanisme de *Deepspeed* pour le *learning rate scheduler* :\n",
    "```python\n",
    "# scheduler update\n",
    "#scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration de ZeRO\n",
    "\n",
    "La configuration de *Deepspeed* se fait par fichier *JSON* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ds_config.json\n",
    "{ \"train_micro_batch_size_per_gpu\": 16,\n",
    "  \"gradient _accumulation_steps\": 1,\n",
    "  \n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"weight_decay\": 5e-4\n",
    "    }\n",
    "  },\n",
    " \n",
    "  \"scheduler\": {\n",
    "      \"type\": \"OneCycle\",\n",
    "      \"params\": {\n",
    "          \"cycle_min_lr\": 1e-6,\n",
    "          \"cycle_max_lr\": 1e-3,\n",
    "          \"decay_lr_rate\": 1e-6\n",
    "      }\n",
    "  },\n",
    " \n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "    },\n",
    " \n",
    " \"zero_optimization\": {\n",
    "    \"stage\": 2\n",
    " },\n",
    " \"zero_allow_untested_optimizer\": true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "batch_size = 16\n",
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --test --deepspeed --deepspeed_config ds_config.json'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1790826']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_memory(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "batch_size = [2, 4, 8, 16, 24, 32]\n",
    "for b in batch_size:\n",
    "    with open(\"ds_config.json\", \"r\") as jsonFile:\n",
    "        data = json.load(jsonFile)\n",
    "\n",
    "    data[\"train_micro_batch_size_per_gpu\"] = b\n",
    "\n",
    "    with open(f\"ds_config{b}.json\", \"w\") as jsonFile:\n",
    "        json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "command = [f'dlojz.py -b {b} --image-size {image_size} --test --deepspeed --deepspeed_config ds_config{b}.json'\n",
    "          for b in batch_size]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['169112', '169113', '169114', '169115', '169116', '169117']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique([jobids[-2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commentaires](images/cedez.png \"Assurez-vous que tout se passe bien avant de continuer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "# TP3_4 : Pipeline Parallelism avec Deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP consite à implémenter le *Pipeline Parallelism* de *Deepspeed* que l'on pourra ensuite utiliser en mode hybride avec le *Data Parallelism* + *ZeRO*.\n",
    "\n",
    "La version du *Pipelined Parallelism* de *Deepspeed* est optimisé pour économiser l'empreinte mémoire.\n",
    "\n",
    "![pipeline deepspeed](images/pipe-schedule.png)\n",
    "\n",
    "**À noter** : Avec *Deepspeed*, le *Pipelined Parallelism* comme le *Data Parallism* fonctionne toujours en *multi-task*, ainsi une *task* est associée à chaque *device*.\n",
    "\n",
    "L'implémentation du *Pipeline Parallelism* amenant trop de changements par rapport au code manipulé durant le TP, nous vous suggérons de copier-coller la solution `solutions/dlojz4_4.py` sur `dlojz.py`.\n",
    "\n",
    "**TODO** :\n",
    "* Copier-coller `solutions/dlojz4_4.py` sur `dlojz.py`.\n",
    "* Regarder le code. Notamment :\n",
    "\n",
    "```python\n",
    "# Define Pipeline Module\n",
    "deepspeed.init_distributed(distributed_port=os.environ['MASTER_PORT'])\n",
    "model = PipelineModule(layers = [\n",
    "                    *model.s0, *model.s1, *model.s2, *model.pres3, *model.s3, *model.s4,\n",
    "                     model.pool, lambda x: x.view(-1, 2048), model.fc],\n",
    "                     num_stages = args.nb_pipeline_stages,\n",
    "                     loss_fn=criterion,\n",
    "                     partition_method = 'parameters' if args.partition_param else 'uniform')\n",
    "\n",
    "# Deepspeed initialization - force port number if several job run on the same node \n",
    "model_engine, optimizer, _, scheduler = deepspeed.initialize(args=args,\n",
    "                                                     model=model, \n",
    "                                                     model_parameters=model.parameters(),\n",
    "                                                     training_data=train_dataset)\n",
    "...\n",
    "\n",
    "    loss = model_engine.train_batch()\n",
    "....    \n",
    "    \n",
    "    val_loss = model_engine.eval_batch(val_iter)\n",
    "\n",
    "```\n",
    "\n",
    "#### Configuration *JSON* :\n",
    "\n",
    "**À noter** : la configuration du *Pipeline Parallelism* se fait avec :\n",
    "* `train_micro_batch_size_per_gpu` correspondant à la taille du **micro batch**,\n",
    "* `gradient_accumulation_steps` correspondant au nombre de *tronçons* du *pipeline*.\n",
    "\n",
    "La taille du *mini batch* pour chaque itération d'apprentissage correspond donc à `train_micro_batch_size_per_gpu` x `gradient_accumulation_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ds_config.json\n",
    "{ \"train_micro_batch_size_per_gpu\": 24,\n",
    "  \"gradient_accumulation_steps\": 8,\n",
    "  \n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"weight_decay\": 5e-4\n",
    "    }\n",
    "  },\n",
    " \n",
    "  \"scheduler\": {\n",
    "      \"type\": \"OneCycle\",\n",
    "      \"params\": {\n",
    "          \"cycle_min_lr\": 1e-6,\n",
    "          \"cycle_max_lr\": 1e-3,\n",
    "          \"decay_lr_rate\": 1e-6\n",
    "      }\n",
    "  },\n",
    " \n",
    " \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    " \"zero_allow_untested_optimizer\": true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "command = f'dlojz.py --image-size {image_size} -p 4 --test --deepspeed --deepspeed_config ds_config.json'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['230538']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_memory(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation du *chunk number*\n",
    "\n",
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "chunks_numbers = [2, 4, 8, 16, 32, 40]\n",
    "for c in chunks_numbers:\n",
    "    with open(\"ds_config.json\", \"r\") as jsonFile:\n",
    "        data = json.load(jsonFile)\n",
    "\n",
    "    data[\"gradient_accumulation_steps\"] = c\n",
    "\n",
    "    with open(f\"ds_config{c}.json\", \"w\") as jsonFile:\n",
    "        json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "command = [f'dlojz.py --image-size {image_size} -p 4 --test --deepspeed --deepspeed_config ds_config{c}.json'\n",
    "           for c in chunks_numbers]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:20:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['239664', '239666', '239667', '239668', '239674', '239676']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids, calcul_memo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique([jobids[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essais et recherche du meilleur parallèlisme\n",
    "\n",
    "**TODO** : Trouver la meilleure architecture et configuration en terme de *Throughput*.\n",
    "\n",
    "* L'argument `-p` correspond au nombre de *stages* du pipeline. Sachant que l'on utilise 4 GPU, un *stage* de 4 correspond à un *pipeline parallelism* total sur 4 GPU, un *stage* de 2 correspond à un *hybrid parallelism* 2x2, un *stage* de 1 à un *Data Parallelism* complet.\n",
    "\n",
    "* Choisir un optimiseur accéléré comme : `Adam`, `AdamW`, `Lamb`, `OnebitAdam`, `OnebitLamb`, ou `ZeroOneAdam`.\n",
    "\n",
    "Configuration *JSON* :\n",
    "\n",
    "**À noter** :\n",
    "* Seul le *stage 1* de ZeRO marche en *hybrid parallelism* avec *Deepspeed*.\n",
    "* `OnebitAdam`, `OnebitLamb`, ou `ZeroOneAdam` ne marche pas avec ZeRO. Si vous utilisez un de ceux-ci, il faudra mettre le paramètre `freeze_step` comme ceci pour pouvoir mesurer son accélération dans notre test :\n",
    "```\n",
    "\"optimizer\": {\n",
    "    \"type\": \"OnebitAdam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"weight_decay\": 5e-4,\n",
    "      \"freeze_step\": 5\n",
    "      }\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ds_config.json\n",
    "{ \"train_micro_batch_size_per_gpu\": 16,\n",
    "  \"gradient_accumulation_steps\": 24,\n",
    "  \n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"weight_decay\": 5e-2\n",
    "    }\n",
    "  },\n",
    " \n",
    "  \"scheduler\": {\n",
    "      \"type\": \"OneCycle\",\n",
    "      \"params\": {\n",
    "          \"cycle_min_lr\": 1e-6,\n",
    "          \"cycle_max_lr\": 1e-3,\n",
    "          \"decay_lr_rate\": 1e-6\n",
    "      }\n",
    "  },\n",
    " \n",
    " \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 32,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "    },\n",
    " \n",
    " \n",
    " \"zero_optimization\": {\n",
    "    \"stage\": 1\n",
    " },\n",
    " \"zero_allow_untested_optimizer\": true\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 4\n",
    "command = f'dlojz.py --image-size {image_size} -p 2 --test --deepspeed --deepspeed_config ds_config.json'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:20:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller vos sorties `jobid = ['xxxxx']` dans la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['202876']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_memory(jobid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commentaires](images/cedez.png \"La suite correspond aux annexes, vous etes arrivé à bout du TP, BRAVO\")\n",
    "\n",
    "\n",
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-1.11.0_py3.9.12",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-1.11.0_py3.9.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
