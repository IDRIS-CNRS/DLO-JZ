{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302e8689-b596-4a8f-abab-b4eddaf2502a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **PyTorch JIT Optimization**\n",
    "License : CC BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06b861-30b0-4a1e-a425-ed0289fcedfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24f1d00-dd9a-43a3-923c-7df05b53a3a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Welcome to our lab on optimizing deep learning models with PyTorch's JIT compilation, focusing on TorchScript and `torch.compile`. Dive into hands-on applications of these core optimization tools to boost model performance efficiently.\n",
    "\n",
    "**Lab Objectives:**\n",
    "- Understand interpretation vs. compilation in Python and PyTorch.\n",
    "- Master TorchScript through tracing and scripting for JIT compilation.\n",
    "- Explore `torch.compile` to optimize PyTorch models effectively.\n",
    "- Practice exporting models for efficient deployment.\n",
    "\n",
    "\n",
    "**Why JIT?** JIT compilation optimizes PyTorch models by compiling Python code into a more efficient form at runtime, enhancing execution speed and resource efficiency. This is crucial for:\n",
    "- Accelerating execution and reducing resource demands.\n",
    "- Improving hardware utilization.\n",
    "- Simplifying deployment across diverse platforms.\n",
    "\n",
    "\n",
    "This lab explores JIT compilation in PyTorch, showing how it addresses Python's limitations to enhance deep learning model performance, and equips you with techniques to optimize your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eefb8-944b-4629-ac01-1b5d9619d28a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **Initial Setup and Instructions**\n",
    "This section prepares the notebook environment for our exercises and introduces key tools we'll use throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79873762-9104-4f98-9cf5-7590455170e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Understanding `ToDo`\n",
    "\n",
    "Throughout this notebook, you'll encounter placeholders marked with `ToDo`. This indicates a section where you, the student, need to complete or modify the code. The `ToDo` class is designed to remind you of tasks that need to be addressed to proceed with the exercises. It's a prompt for active participation in the learning process.\n",
    "\n",
    "Whenever you see a `ToDo` in the code, it's your cue to engage with the material by writing or modifying code. This interactive approach helps reinforce learning through practice.\n",
    "\n",
    "The following cell defines the `ToDo` class. Note that no action is required from you in this specific cell; it's simply a setup for the interactive tasks ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0223f46-01b5-40a8-9b3c-6fcf20aa0d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToDo:\n",
    "    def __init__(self, message=None):\n",
    "        if message is None:\n",
    "            self.message = \"This part of the code needs to be completed by the student.\"\n",
    "        else:\n",
    "            self.message = message\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(\"TODO: Complete this part of the code.\")\n",
    "        raise NotImplementedError(self.message)\n",
    "\n",
    "    # Example of an additional method that could be useful in the future\n",
    "    def hint(self):\n",
    "        print(f\"Hint: {self.message}\")\n",
    "\n",
    "# Usage\n",
    "todo = ToDo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c58413-3e49-41d5-9bc5-650469ee253f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Executing Lab Exercises: Interactive and Script-Based Tasks\n",
    "\n",
    "Throughout this lab, you'll engage in exercises directly within this notebook and work on script-based tasks identifiable by `%writefile` at the beginning of the cell. For deploying scripts on a computing cluster, leverage `srun` to execute your code on compute nodes. If you're equipped with a local GPU, run these scripts using `!python code.py`. This dual approach ensures a thorough learning experience, combining interactive exploration with practical script execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd044c0f-a6ba-48a0-9268-95e2941be38c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Auto-Reloading\n",
    "\n",
    "To ensure any changes in imported modules are automatically updated in the notebook, we enable auto-reloading. This is particularly useful during development, as it saves time that would otherwise be spent restarting the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadd0d2-96b5-4512-8088-93dfd16aae77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81971782-f2ec-47bd-a883-0f62a291208a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Initial Setup\n",
    "\n",
    "Before starting, create a `generated_files` directory to ensure all generated script files are neatly stored in one place.\n",
    "\n",
    "Also we import utilitary functions to run jobs.\n",
    "\n",
    "> **Task:** Define a name you will use to identify your jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615833dc-39a4-4d5c-a504-19ce81288b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p generated_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be39fab-dbde-4431-bc23-9a8a50ee4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools import controle_technique, compare, GPU_underthehood, plot_accuracy, lrfind_plot, imagenet_starter, turbo_profiler\n",
    "MODULE = 'pytorch-gpu/py3/2.1.1'\n",
    "account = 'for@a100'\n",
    "name = todo()   ## Pseudonyme Ã  choisir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649b5c7-5cb5-4b2b-851e-fcae290a73c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **Python's Execution Model Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98835beb-68c6-405a-80a7-9cc5f909940a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Python, an interpreted language, processes code line by line, which is straightforward for development but can slow down execution. Unlike compiled languages like C, which convert code directly into machine code for faster performance, Python's execution involves an extra step that can introduce performance bottlenecks.\n",
    "\n",
    "### CPython's Hybrid Approach\n",
    "\n",
    "CPython, Python's standard interpreter, compiles code to bytecode before interpreting it, striking a balance between direct interpretation and full compilation. This method reduces some overhead but doesn't achieve the full speed of compiled languages.\n",
    "\n",
    "### Boosting Performance with Cython\n",
    "\n",
    "Cython enhances Python by allowing static typing and compilation to C, significantly speeding up execution. This approach bridges the gap between Python's ease of use and the performance of compiled languages, albeit with the complexity of managing Cython code and the limitations in handling dynamic operations.\n",
    "\n",
    "### JIT in PyTorch\n",
    "\n",
    "PyTorch addresses Python's performance issues with JIT compilation, converting code into an optimized, hardware-specific format at runtime. This method ensures efficient execution of dynamic deep learning models, leveraging the best of both Python's flexibility and the efficiency of compiled execution.\n",
    "\n",
    "Through JIT, PyTorch marries Python's development advantages with the performance benefits of compiled languages, optimizing deep learning model performance and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc943264-ae6e-4d86-a027-886b0ed7e939",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **Performance: Python vs. C**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ba0d0-08dd-4933-b2ff-6bd645486615",
   "metadata": {},
   "source": [
    "\n",
    "Python runs code directly, which is beneficial for rapid development but can result in slower execution times. Conversely, C requires a compilation process that translates code into a binary executable, typically leading to faster execution after this initial setup. The C compilation process involves:\n",
    "\n",
    "1. **Preprocessing:** Macro expansion and inclusion of header files.\n",
    "2. **Compilation:** Conversion of the preprocessed source code to assembly code.\n",
    "3. **Assembly:** Translation of assembly code into machine code, producing an object file (.o).\n",
    "4. **Linking:** Combining object files and libraries into a final executable binary.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "> **Task 1 (Python)**: Create a Python script to sum numbers from 1 to 1 million. Then, time the script's execution.\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "Here's one way to solve the exercise using a loop, encapsulated within a Python script file stored in the `generated_files` directory:\n",
    "\n",
    "```python\n",
    "%%writefile generated_files/sum_python.py\n",
    "total = 0\n",
    "for i in range(1, 1000001):\n",
    "    total += i\n",
    "print(f\"Sum: {total}\")\n",
    "```\n",
    "\n",
    "Alternatively, for a more concise solution using Python's built-in `sum()` function, also stored in the `generated_files` directory:\n",
    "\n",
    "```python\n",
    "%%writefile generated_files/sum_python.py\n",
    "total = sum(range(1, 1000001))\n",
    "print(f\"Sum: {total}\")\n",
    "```\n",
    "\n",
    "Both scripts, when executed, will calculate the sum of numbers from 1 to 1,000,000 and are stored in the `generated_files` directory for organization.\n",
    "\n",
    "</details>\n",
    "\n",
    "> **Task 2 (C)**: Create a C program to sum numbers from 1 to 1 million. Compile and time the program's execution.\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "Here's a solution for summing numbers from 1 to 1,000,000 using a loop, encapsulated within a C program file stored in the `generated_files` directory:\n",
    "\n",
    "```c\n",
    "%%writefile generated_files/sum_c.c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    long total = 0;\n",
    "    for (int i = 1; i <= 1000000; i++) {\n",
    "        total += i;\n",
    "    }\n",
    "\n",
    "    printf(\"Sum: %ld\\n\", total);\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "This script, when compiled and executed, will calculate the sum of numbers from 1 to 1,000,000 and is stored in the `generated_files` directory for tidy organization.\n",
    "\n",
    "</details>\n",
    "\n",
    "> **Task 3 (Comparison)**: Compare the execution times of both programs. Reflect on the differences and discuss the impact of Python's direct execution versus the compilation and execution process of C.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "\n",
    "Observations from the execution times:\n",
    "\n",
    "1. **Python's Execution**: Directly interpreted, resulting in slower execution due to real-time interpretation overhead.\n",
    "```\n",
    "real\t0m0.073s\n",
    "user\t0m0.039s\n",
    " sys\t0m0.020s\n",
    "```\n",
    "\n",
    "2. **C's Execution**: Involves compilation and running. The compilation is a one-time overhead, but the resulting executable runs significantly faster than the Python script.\n",
    "```\n",
    "real\t0m0.377s\n",
    "user\t0m0.034s\n",
    " sys\t0m0.082s\n",
    "\n",
    "real\t0m0.013s\n",
    "user\t0m0.004s\n",
    " sys\t0m0.009s\n",
    "```\n",
    "\n",
    "    \n",
    "### Conclusion:\n",
    "\n",
    "The key difference lies in Python's runtime interpretation versus C's upfront compilation. C's compiled executable runs faster, showcasing the efficiency of compiled programs despite the initial compile time.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "These tasks illustrate performance differences between Python and C. If you're less familiar with these languages, it's okay to focus on the conceptual insights. **The key is understanding, not perfect execution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eedf7b-723d-4ddb-bee8-2f024dc38f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile generated_files/sum_python.py\n",
    "# Python code to sum numbers from 1 to 1,000,000 using a loop\n",
    "total = 0\n",
    "# Add your code here to sum numbers from 1 to 1,000,000\n",
    "# Hint: Use a for loop and range(1, 1000001)\n",
    "todo()\n",
    "print(f\"Sum: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df61f9-41ec-43d2-a892-b4a2fa4d5f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile generated_files/sum_c.c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    long total = 0;\n",
    "    // Add your code here to sum numbers from 1 to 1,000,000\n",
    "    // Hint: Use a for loop starting from 1 to 1,000,000 and add each number to 'total'\n",
    "\n",
    "    printf(\"Sum: %ld\\n\", total);\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf997c0-932d-44fb-bcae-38ed4ebe396d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Python performance:\")\n",
    "!time python generated_files/sum_python.py\n",
    "print(\"\\n\\nC performance compilation:\")\n",
    "!time gcc generated_files/sum_c.c -o generated_files/sum_c\n",
    "print(\"\\n\\nC performance execution:\")\n",
    "!time ./generated_files/sum_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead24e4e-c9de-4234-baad-fc85b6ba91ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **TorchScript**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e524430-7ba6-4c53-abbb-30320b7e809c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### JIT (Just-In-Time) Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5835e-efa2-4665-9212-c99e8fd24dc0",
   "metadata": {},
   "source": [
    "JIT compilation optimizes PyTorch model execution, translating Python code to a more efficient format that enhances performance, particularly during inference. It combines Python's development simplicity with the execution speed closer to compiled languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115d919b-c052-41c5-8f8d-836d6208272c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TorchScript Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc44f9-d938-4149-bad8-37a7e6601eed",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "TorchScript is a tool within PyTorch for creating serializable and optimizable models. It enables models to run independently of Python, facilitating deployment in non-Python environments.\n",
    "\n",
    "- **Mechanism**: Converts PyTorch models to TorchScript through **tracing** or **scripting**.\n",
    "  \n",
    "  - **Tracing** records operations on provided inputs, generating a computation graph.\n",
    "  \n",
    "  - **Scripting** directly translates Python code to TorchScript, incorporating complex control flows.\n",
    "\n",
    "#### Advantages\n",
    "- **Improved Performance**: Models execute faster, especially in production.\n",
    "- **Increased Portability**: Enables model execution in environments without Python.\n",
    "- **Enhanced Flexibility**: Supports models with dynamic behaviors and varying inputs.\n",
    "\n",
    "TorchScript is pivotal for JIT in PyTorch, improving models' speed, deployability, and flexibility.\n",
    "\n",
    "Before diving into specifics, let's outline our approach: we'll first explore TorchScript's key featuresâtracing and scriptingâto understand how they convert Python functions for optimized execution in PyTorch. After grasping these functionalities, we will then evaluate the performance gains they offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea04e0-285b-457f-a57a-3e3e4df546bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploring TorchScript Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6e684-7ceb-4e78-b4f7-e0c86727079b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "TorchScript tracing transforms Python functions into TorchScript by executing the function with a sample input and capturing the operations executed. This creates a static computation graph reflecting the input-specific behavior. While tracing suits straightforward functions well, it may not accurately handle dynamic behavior due to its static nature.\n",
    "\n",
    "\n",
    "> **Task: Implement Tracing with TorchScript for `add_ten` and `conditional_add` by using torch.jit.trace.** Refer to the [torch.jit.trace documentation](https://pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace) for more details on the parameters required.\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "Unlike tracing, scripting does not require an example input. Scripting analyzes the code to capture control flows and the overall logic.\n",
    " \n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Define the functions\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "def conditional_add(x):\n",
    "    if x < 10:\n",
    "        x = x + 10\n",
    "    else:\n",
    "        x = x + 100\n",
    "    return x\n",
    "\n",
    "# Apply tracing\n",
    "example_input = torch.tensor(1)\n",
    "traced_add_ten = torch.jit.trace(add_ten, example_input)\n",
    "traced_conditional_add = torch.jit.trace(conditional_add, example_input)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4063bbe-ef6f-476a-bb6d-dc1a6d5012f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the functions\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "def conditional_add(x):\n",
    "    if x < 10:\n",
    "        x = x + 10\n",
    "    else:\n",
    "        x = x + 100\n",
    "    return x\n",
    "\n",
    "# Apply tracing\n",
    "example_input = torch.tensor(1)\n",
    "traced_add_ten = todo()\n",
    "traced_conditional_add = todo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95b82a-d4ff-4767-87f6-dd96f9dce98d",
   "metadata": {},
   "source": [
    "\n",
    "> **Task: Observe Results and Discuss Tracing Limitations for `conditional_add`**. Experiment with `traced_add_ten` and `traced_conditional_add` using various inputs. Specifically, adjust the example input for `traced_conditional_add` and note the changes.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```\n",
    "Traced 'add_ten' result: tensor(10)\n",
    "Traced 'add_ten' result: tensor(25)\n",
    "Traced 'conditional_add' with input less than 10: tensor(10)\n",
    "Traced 'conditional_add' with input greater than 10: tensor(25)\n",
    "```\n",
    "    \n",
    "`traced_add_ten` consistently adds ten to any input, reflecting its straightforward logic. In contrast, `traced_conditional_add` may fail to adapt to inputs not matched by the example input used for tracing. This is because tracing captures the function's execution path based on that specific input, missing other conditional branches.\n",
    "\n",
    "Changing the example input for tracing `conditional_add` directly impacts which conditional path is captured. For example, tracing with a value less than 10 captures the \"less than 10\" path, making the function inaccurate for inputs greater than 10, and vice versa.\n",
    "\n",
    "This demonstrates tracing's limitation with dynamic behaviors, emphasizing the need for a representative example input to ensure accurate behavior across all inputs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362c015-7570-4805-9fa3-34ccb83b35c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the traced functions\n",
    "test_input_less = torch.tensor(0)\n",
    "test_input_more = torch.tensor(15)\n",
    "\n",
    "print(\"Traced 'add_ten' result:\", traced_add_ten(test_input_less))\n",
    "print(\"Traced 'add_ten' result:\", traced_add_ten(test_input_more))\n",
    "print(\"Traced 'conditional_add' with input less than 10:\", traced_conditional_add(test_input_less))\n",
    "print(\"Traced 'conditional_add' with input greater than 10:\", traced_conditional_add(test_input_more))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bd26b-79b1-41cf-a4c4-f5b7a3b5a060",
   "metadata": {},
   "source": [
    "> **Task: Compare the Graphs for `add_ten` and `conditional_add` Tracing** generated using the `.graph` attribute of the traced objects. This attribute provides a textual representation of the operations and tensors within the traced function, showcasing the flow of data and the operations applied.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```\n",
    "Graph for 'add_ten':\n",
    " graph(%x : Long(requires_grad=0, device=cpu)):\n",
    "  %1 : Long(requires_grad=0, device=cpu) = prim::Constant[value={10}]() # /tmp/ipykernel_3336442/1385163837.py:5:0\n",
    "  %2 : int = prim::Constant[value=1]() # /tmp/ipykernel_3336442/1385163837.py:5:0\n",
    "  %3 : Long(requires_grad=0, device=cpu) = aten::add(%x, %1, %2) # /tmp/ipykernel_3336442/1385163837.py:5:0\n",
    "  return (%3)\n",
    "\n",
    "\n",
    "Graph for 'conditional_add':\n",
    " graph(%x : Long(requires_grad=0, device=cpu)):\n",
    "  %3 : Long(requires_grad=0, device=cpu) = prim::Constant[value={10}]() # /tmp/ipykernel_3336442/1385163837.py:9:0\n",
    "  %4 : int = prim::Constant[value=1]() # /tmp/ipykernel_3336442/1385163837.py:9:0\n",
    "  %5 : Long(requires_grad=0, device=cpu) = aten::add(%x, %3, %4) # /tmp/ipykernel_3336442/1385163837.py:9:0\n",
    "  return (%5)\n",
    "```\n",
    "\n",
    "    \n",
    "The traced graphs for `add_ten` and `conditional_add` are structurally similar, highlighting tracing's static nature. Despite `conditional_add`'s conditional logic, its graph shows a straightforward addition, like `add_ten`, underscoring tracing's limitation in capturing dynamic behavior based solely on the provided example input.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865a68e-d24c-4afb-ac38-ed03e4997d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Graph for 'add_ten':\\n\", traced_add_ten.graph)\n",
    "print(\"\\nGraph for 'conditional_add':\\n\", traced_conditional_add.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365aa5e-a362-4d21-b97e-3726cad62ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploring TorchScript Scripting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836129f5-73ed-43c7-b008-72e636325bb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "After exploring TorchScript's tracing mechanism, let's delve into scripting, another powerful method for converting Python functions and models into TorchScript.\n",
    "\n",
    "Scripting performs a static analysis of your Python code to create a TorchScript version. Unlike tracing, which records operations performed on example inputs, scripting examines the code itself, capturing its structure, control flows (like loops and if-statements), and the overall logic.\n",
    "\n",
    "**Advantages Over Tracing:**\n",
    "\n",
    "- **Dynamic Behavior**: Scripting accurately captures dynamic behaviors, making it more suitable for functions and models with conditional logic or varying execution paths.\n",
    "- **Complete Translation**: It translates the entire function or model, including all potential execution paths, not just the ones executed with a particular set of example inputs.\n",
    "\n",
    "**Weaknesses:**\n",
    "- **Compatibility**: Not all Python code is directly convertible to TorchScript via scripting. Certain Python features, especially those involving dynamic typing or reflection, might not be supported.\n",
    "- **Complexity**: Understanding errors and debugging the scripted code can be more challenging due to the static analysis and the need to adhere strictly to TorchScript's supported features.\n",
    "\n",
    "To solidify your understanding of TorchScript scripting, you'll convert functions to TorchScript using scripting, observe their behavior, and identify both the strengths and limitations of scripting.\n",
    "\n",
    "\n",
    "> **Task: Convert the previously defined `add_ten` and `conditional_add` functions to TorchScript using torch.jit.script.** Refer to the [torch.jit.script documentation](https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script) for more details on the parameters required.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "Unlike tracing, scripting does not require an example input. Scripting analyzes the code to capture control flows and the overall logic.\n",
    "\n",
    "```python\n",
    "# Apply scripting\n",
    "scripted_add_ten = torch.jit.script(add_ten)\n",
    "scripted_conditional_add = torch.jit.script(conditional_add)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead961b-6bb0-4e9d-83fa-445171574452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "scripted_add_ten = todo()\n",
    "scripted_conditional_add = todo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3e2e6-3e43-4b71-adbc-60a23f75494e",
   "metadata": {},
   "source": [
    "\n",
    "> **Task: Test and Analyze Scripted Functions** with different inputs. Observe how the scripted versions handle different scenarios compared to their traced counterparts, especially for `conditional_add`.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```\n",
    "Scripted 'conditional_add' with input less than 10: tensor(15)\n",
    "Scripted 'conditional_add' with input greater than 10: tensor(115)\n",
    "```\n",
    "    \n",
    "Testing the scripted functions with various inputs demonstrates that they accurately reflect the original Python logic, producing correct results for all scenarios, unlike their traced counterparts which may not capture dynamic behaviors effectively.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32180e98-59eb-4e60-ae1a-549acc41c2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Scripted 'conditional_add' with input less than 10:\", scripted_conditional_add(torch.tensor(5)))\n",
    "print(\"Scripted 'conditional_add' with input greater than 10:\", scripted_conditional_add(torch.tensor(15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11df0e-59fc-44a3-94fc-a3c398401794",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Task: Compare the Graphs for `add_ten` and `conditional_add` Scripting** generated using the `.graph` attribute of the scripted objects. This attribute provides a textual representation of the operations and tensors within the scripted function, showcasing the flow of data and the operations applied. Reflect on scripting's ability to accurately represent the original Python function, regardless of input variability. Contrast this with your observations from tracing, particularly for the `conditional_add` function.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "    \n",
    "The `.graph` attribute of the scripted `add_ten` and `conditional_add` functions reveals TorchScript's ability to closely mirror the original Python code's logic. Unlike tracing, scripting captures conditional branches and dynamic behaviors accurately, ensuring the scripted functions behave as intended across different inputs. This direct representation highlights scripting's superiority in handling complex, input-dependent logic, a stark contrast to the limitations observed with tracing.\n",
    "\n",
    "```\n",
    "Graph for 'add_ten':\n",
    " graph(%x.1 : Tensor):\n",
    "  %3 : int = prim::Constant[value=1]()\n",
    "  %2 : int = prim::Constant[value=10]() # /tmp/ipykernel_3336442/1385163837.py:5:15\n",
    "  %4 : Tensor = aten::add(%x.1, %2, %3) # /tmp/ipykernel_3336442/1385163837.py:5:11\n",
    "  return (%4)\n",
    "\n",
    "\n",
    "Graph for 'conditional_add':\n",
    " graph(%x.1 : Tensor):\n",
    "  %7 : int = prim::Constant[value=1]()\n",
    "  %2 : int = prim::Constant[value=10]() # /tmp/ipykernel_3336442/1385163837.py:8:11\n",
    "  %11 : int = prim::Constant[value=100]() # /tmp/ipykernel_3336442/1385163837.py:11:16\n",
    "  %3 : Tensor = aten::lt(%x.1, %2) # /tmp/ipykernel_3336442/1385163837.py:8:7\n",
    "  %5 : bool = aten::Bool(%3) # /tmp/ipykernel_3336442/1385163837.py:8:7\n",
    "  %x : Tensor = prim::If(%5) # /tmp/ipykernel_3336442/1385163837.py:8:4\n",
    "    block0():\n",
    "      %x.7 : Tensor = aten::add(%x.1, %2, %7) # /tmp/ipykernel_3336442/1385163837.py:9:12\n",
    "      -> (%x.7)\n",
    "    block1():\n",
    "      %x.13 : Tensor = aten::add(%x.1, %11, %7) # /tmp/ipykernel_3336442/1385163837.py:11:12\n",
    "      -> (%x.13)\n",
    "  return (%x)\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc84b2a-cd97-4602-92c5-dcf04552e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Graph for 'add_ten':\\n\", scripted_add_ten.graph)\n",
    "print(\"\\nGraph for 'conditional_add':\\n\", scripted_conditional_add.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c680a03-6bf6-430d-929c-185c64b494a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Applying TorchScript to a Model: ResNet50 Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adb43a-60cb-43a4-8ec0-334500c3df71",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Applying TorchScript's tracing and scripting to ResNet50, a widely-used model for image classification, demonstrates its effectiveness in optimizing deep learning models for production. \n",
    "\n",
    "This example highlights how TorchScript prepares complex models for efficient deployment, showcasing its utility in real-world applications.\n",
    "\n",
    "\n",
    "> **Task: Convert the ResNet50 model to TorchScript using scripting (`torch.jit.script`) or tracing (`torch.jit.trace`).** We then perform inference with a dummy input to verify the model's functionality and speed.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "Tracing requires an example input and might not capture the model's full behavior, but it can be faster for straightforward models without dynamic control flows.    \n",
    "      \n",
    "Unlike tracing, scripting does not require an example input and provides a comprehensive conversion capturing the model's control flows, making it generally preferred for complex models.\n",
    "\n",
    "```python\n",
    "# For scripting:\n",
    "torchscript_model = torch.jit.script(model)\n",
    "\n",
    "# OR for tracing:\n",
    "torchscript_model = torch.jit.trace(model, input_tensor)\n",
    "```\n",
    "After testing the scripted model, you'll notice it behaves identically to the original pre-trained ResNet50 model, demonstrating TorchScript's effectiveness in preserving the model's functionality. This process not only makes the model portable but also optimizes its performance, particularly in environments where Python is not available or when leveraging specialized hardware accelerators.\n",
    "You could get performance like this using a gpu and `scripting`\n",
    "\n",
    "```\n",
    "Original model output (first 5 scores): [ 0.38384837  0.803145   -0.7964321   0.42339647  0.56345093]\n",
    "TorchScript model output (first 5 scores): [ 0.38384837  0.803145   -0.7964321   0.42339647  0.56345093]\n",
    "Mean Original model inference time: 0.075557 seconds\n",
    "Mean TorchScript model inference time: 0.023330 seconds\n",
    "Mean Performance Gain: 3.24x faster with compiled version    \n",
    "```    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1fbf6-98e7-4ff9-867f-5bb7fa262984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile generated_files/resnet_torchscript.py\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from utils import todo\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a dummy input tensor for inference and move it to the defined device\n",
    "input_tensor = torch.rand(1, 3, 224, 224).to(device, non_blocking=True)\n",
    "\n",
    "# Load a pre-trained ResNet50 model\n",
    "model = models.resnet50(pretrained=False).to(device)\n",
    "\n",
    "# Convert to TorchScript via scripting or tracing\n",
    "torchscript_model = todo()\n",
    "\n",
    "nbr_iter = 100\n",
    "\n",
    "# Time inference for the original model\n",
    "start.record()\n",
    "for _ in range(nbr_iter):\n",
    "    original_output = model(input_tensor)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "mean_original_inference_time = start.elapsed_time(end)/nbr_iter/10\n",
    "\n",
    "\n",
    "# Time inference for the TorchScript model\n",
    "start.record()\n",
    "for _ in range(nbr_iter):\n",
    "    scripted_output = torchscript_model(input_tensor)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "mean_scripted_inference_time = start.elapsed_time(end)/nbr_iter/10\n",
    "\n",
    "\n",
    "# Compare the outputs for consistency (example by checking the first 5 scores)\n",
    "print(\"Original model output (first 5 scores):\", original_output[0][:5].detach().cpu().numpy())\n",
    "print(\"TorchScript model output (first 5 scores):\", scripted_output[0][:5].detach().cpu().numpy())\n",
    "\n",
    "# Compare the performance\n",
    "mean_performance_gain = mean_original_inference_time / mean_scripted_inference_time\n",
    "print(f\"Mean Original model inference time: {mean_original_inference_time:.6f} seconds\")\n",
    "print(f\"Mean TorchScript model inference time: {mean_scripted_inference_time:.6f} seconds\")\n",
    "print(f\"Mean Performance Gain: {mean_performance_gain:.2f}x faster with compiled version\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a4574-c633-4a4e-81f1-5b6736a3be2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = f'generated_files/resnet_torchscript.py'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu=1, module=MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00')\n",
    "print(f'jobid = {jobid}')\n",
    "#!python generated_files/resnet_torchscript.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d71d3f-599f-48c8-8968-08c4b11981ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ceb50f-1c41-458f-850f-d117884520a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nOut file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stdout'][0]}\n",
    "print(\"\\nError file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607695ea-81ce-4f8e-a4d4-23d946132937",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Running TorchScript Models in C++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9be0c5-151d-4496-ba5c-e001e1cf4ae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "Exporting a model with TorchScript not only optimizes it for production but also makes it accessible for inference within a C++ environment. This capability is particularly valuable for deploying deep learning models in performance-critical and resource-constrained settings where Python may not be available.\n",
    "\n",
    "#### Loading and Using TorchScript Models in C++\n",
    "\n",
    "To use a TorchScript-exported model in C++, you need to load the model using the TorchScript C++ API. The following code snippet provides an example of how this can be achieved:\n",
    "\n",
    "```cpp\n",
    "#include <torch/script.h> // One-stop header.\n",
    "#include <iostream>\n",
    "#include <memory>\n",
    "\n",
    "int main() {\n",
    "    // Load the scripted model\n",
    "    auto model = torch::jit::load(\"resnet50_ts.pt\");\n",
    "\n",
    "    // Create a dummy input tensor\n",
    "    auto input = torch::rand({1, 3, 224, 224});\n",
    "\n",
    "    // Perform inference\n",
    "    at::Tensor output = model.forward({input}).toTensor();\n",
    "\n",
    "    // Print the top 5 predictions\n",
    "    std::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\\n';\n",
    "}\n",
    "```\n",
    "\n",
    "This C++ code snippet demonstrates loading a TorchScript model (e.g., `resnet50_ts.pt`), preparing input data, and performing inference. Note that this example assumes the presence of a model file `resnet50_ts.pt`, which should be generated from the Python environment as shown in previous sections.\n",
    "\n",
    "#### Documentation and Resources\n",
    "\n",
    "For a comprehensive guide on integrating TorchScript models into C++ applications, the following resources are invaluable:\n",
    "\n",
    "- **PyTorch C++ API**: Explore the detailed [PyTorch C++ API documentation](https://pytorch.org/cppdocs/) for a deeper understanding of using TorchScript models in C++.\n",
    "- **Loading a TorchScript Model in C++**: For more specifics on loading and running models, refer to the [Loading a TorchScript Model](https://pytorch.org/tutorials/advanced/cpp_export.html) tutorial.\n",
    "- **LibTorch**: The PyTorch C++ library, known as LibTorch, is essential for C++ integration and can be downloaded from the [PyTorch website](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "By leveraging TorchScript, you can efficiently bridge Python-based model training with C++ based model deployment, enhancing the portability and scalability of deep learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608d4c4-e4fe-474c-b1c5-8366dc672ed0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Conclusion on TorchScript\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa64616-0b7d-4111-a93f-33abe732c3ca",
   "metadata": {},
   "source": [
    "TorchScript bridges the gap between PyTorch research and production, allowing for model serialization and optimized execution in non-Python environments. It's ideal for deploying stable models across various platforms, offering improved inference speed.\n",
    "\n",
    "**Use Cases:**\n",
    "- Deploying models without Python dependency.\n",
    "- Serializing models for consistent deployment.\n",
    "\n",
    "**Limitations:**\n",
    "- Struggles with highly dynamic models due to static computation graphs.\n",
    "- Limited capture of Python's dynamic features.\n",
    "\n",
    "While TorchScript enhances model portability and performance, its limitations with dynamic models highlight the need for more adaptive solutions. Enter `torch.compile()`, PyTorch's answer to achieving hardware-specific optimizations, marking a significant advancement in model deployment and optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bcb06-77fd-434a-a637-60674b1a841a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "## **Torch.compile()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118248b3-31b2-468c-8e58-22d83f1bae5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc5f2a-ea13-43d4-8bbc-a69477abe356",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Torch Compile propels PyTorch forward by compiling Python into an optimized execution format, significantly enhancing performance, particularly during inference. It leverages TorchDynamo for dynamic graph capture and TorchInductor, the default backend compiler, to translate models into code that runs efficiently across a variety of hardware. This synergy between Python's simplicity and the efficiency of compiled languages optimizes model execution without sacrificing PyTorch's ease of use.\n",
    "\n",
    "#### Operational Mechanism\n",
    "\n",
    "- **TorchDynamo**: Acts as an on-the-fly Python bytecode interpreter, capturing the execution graph of models dynamically at runtime.\n",
    "- **TorchInductor**: Serves as the primary backend, converting the graph into optimized code suitable for multiple accelerators, ensuring flexibility and broad compatibility.\n",
    "\n",
    "#### Core Features and Backend Diversity\n",
    "\n",
    "- **Enhanced Performance**: Dramatically reduces execution times across training and inference phases, optimizing models for speed.\n",
    "- **Backend Flexibility**: Offers a range of backends tailored for specific phasesâTorchInductor and cudagraphs for training, with the latter optimized for static batch sizes, and TensorRT and OpenVINO for inference, focusing on high-speed execution.\n",
    "- **Dynamic Code Optimization**: Efficiently manages dynamic Python code, including models with conditional logic or variable input sizes, ensuring comprehensive optimization.\n",
    "\n",
    "Torch Compile marks a significant leap in enhancing PyTorch's execution speed, flexibility, and efficiency. It introduces a seamless pathway for model optimization, catering to diverse computational needs and hardware environments, thus broadening the scope of PyTorch's applicability and performance capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23559385-b635-44d0-a0e0-d971e4d18470",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utilizing Torch Compile for Dynamic Python Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb19e96-673e-445f-854c-5f93ca2c57ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Torch Compile significantly enhances PyTorch model performance by adeptly handling dynamic Python code, including conditional statements and loops, optimizing models that exhibit dynamic behavior due to varying inputs or internal states.\n",
    "\n",
    "> **Task: Optimize a dynamically behaving function with Torch Compile.** Use `torch.compile` on the `conditional_add` function, which alters its output based on the input value, to showcase Torch Compile's capability in handling dynamic Python code. For more details on using `torch.compile`, refer to the [official documentation](https://pytorch.org/docs/stable/torch.compiler.html).\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Define the conditional add function\n",
    "def conditional_add(x):\n",
    "    return x + 10 if x < 10 else x + 100\n",
    "\n",
    "# Apply Torch Compile with a specified backend\n",
    "compiled_conditional_add = torch.compile(conditional_add, backend='cudagraphs')  # or try 'inductor' with various modes as an alternative\n",
    "\n",
    "# Test the compiled function\n",
    "print(compiled_conditional_add(torch.tensor(5)))  # Expected output: 15\n",
    "print(compiled_conditional_add(torch.tensor(15))) # Expected output: 115\n",
    "```\n",
    "</details>\n",
    "\n",
    "This example underscores Torch Compile's capability to adeptly manage conditional logic and complex control flows within Python functions, optimizing models with dynamic behaviors. It suggests selecting a backend like 'inductor' or 'cudagraphs' for specific optimizations and exploring the \"mode\" parameter, primarily compatible with the 'inductor' backend, to further tailor optimizations to your model's needs and hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425aa928-f9a9-45c8-9761-b4f74638cbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch._dynamo.reset()\n",
    "# Define the conditional add function\n",
    "def conditional_add(x):\n",
    "    if x < 10:\n",
    "        return x + 10\n",
    "    else:\n",
    "        return x + 100\n",
    "\n",
    "# Apply Torch Compile\n",
    "compiled_conditional_add = todo()\n",
    "\n",
    "# Test the compiled function\n",
    "print(compiled_conditional_add(torch.tensor(5)))  # Expected output: 15\n",
    "print(compiled_conditional_add(torch.tensor(15))) # Expected output: 110"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b821a3-5c69-4c84-a583-3b0ebc147a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrapping NumPy Code to C++/CUDA with Torch Compile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e07150-4398-4b90-aba8-4257cb872950",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "The `torch.compile` feature in PyTorch 2.1 enables the compilation of NumPy code into efficient, fused vectorized C++ or CUDA code without modifying the original scripts. This capability significantly enhances performance by utilizing PyTorchâs compiler backend, facilitating NumPy code execution on CUDA with minimal syntax changes.\n",
    "\n",
    "> **Task:** Optimize `kmeans` with `torch.compile` and compare CPU vs. GPU speeds. Set `device` to `cpu` or `cuda` to specify execution hardware. The `kmeans` function is commonly used in clustering algorithms to calculate the nearest cluster center for each point in a dataset.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the kmeans function\n",
    "def kmeans(X, means):\n",
    "    return np.argmin(np.linalg.norm(X - means[:, None], axis=2), axis=0)\n",
    "\n",
    "# Prepare synthetic dataset\n",
    "npts = 10_000_000\n",
    "X = np.repeat([[5, 5], [10, 10]], [npts, npts], axis=0)\n",
    "X = X + np.random.randn(*X.shape)  # Creating two distinct \"blobs\"\n",
    "means = np.array([[5, 5], [10, 10]])\n",
    "\n",
    "# Convert NumPy arrays to tensors\n",
    "X_tensor = torch.tensor(X)\n",
    "means_tensor = torch.tensor(means)\n",
    "\n",
    "# Set the device to CUDA if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_tensor = X_tensor.to(device)\n",
    "means_tensor = means_tensor.to(device)\n",
    "\n",
    "# Wrap the kmeans function with torch.compile for optimization\n",
    "compiled_kmeans = torch.compile(kmeans)\n",
    "\n",
    "# Execute the compiled function and measure performance\n",
    "compiled_pred = compiled_kmeans(X_tensor, means_tensor)\n",
    "```\n",
    "    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Bonus Solution (click to reveal)</summary>\n",
    "\n",
    "**Bonus Solution for Full GPU Execution:**\n",
    "\n",
    "This approach fully leverages GPU for computation, showcasing massive performance gains (up to 4000x on an A100 GPU for the computing part). However, due to communication costs between CPU and GPU, the overall speedup might reduce (to about 10x). This solution is ideal when running on a GPU.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Kmeans function\n",
    "def kmeans(X, means):\n",
    "    return np.argmin(np.linalg.norm(X - means[:, None], axis=2), axis=0)\n",
    "\n",
    "@torch.compile\n",
    "def tensor_fn(X, means):\n",
    "    X, means = X.numpy(), means.numpy()\n",
    "    ret = kmeans(X, means)\n",
    "    return torch.from_numpy(ret)\n",
    "\n",
    "def compiled_kmeans(X, means):\n",
    "    with torch.device(\"cuda\"):\n",
    "        return tensor_fn(X, means)\n",
    "\n",
    "# Set the device to CUDA if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare synthetic dataset and convert to tensors on the GPU\n",
    "X_tensor = torch.tensor(X).to(device)\n",
    "means_tensor = torch.tensor(means).to(device)\n",
    "\n",
    "# Execute the function fully on GPU and observe massive performance gains\n",
    "cuda_result = compiled_kmeans(X_tensor, means_tensor)\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "> **Task:** Observe the performance evaluation code and note any differences in execution time between the first run and subsequent runs. Notice also the behavior differences in function of the `device` and the `backend`.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "You will likely notice that the first run takes longer than the following ones. This behavior is attributed to the initial compilation process that `torch.compile` undergoes during the first execution. Once the code is compiled, PyTorch caches the optimized version, leading to faster execution times in subsequent runs as it bypasses the compilation step and directly executes the optimized code.\n",
    "With the backend inductor, you could achieve with 1 A100 and  such performances :\n",
    "```\n",
    "On GPU (A100) :\n",
    "First Run Compiled Time: 7.7172 seconds\n",
    "Original Execution Time: 1.7764 seconds\n",
    "Mean Compiled Execution Time: 0.0237 seconds\n",
    "Performance Gain: 74.85x faster with compiled version\n",
    "\n",
    "On CPU (8 cores):\n",
    "First Run Compiled Time: 26.3685 seconds\n",
    "Original Execution Time: 1.7909 seconds\n",
    "Mean Compiled Execution Time: 0.0182 seconds\n",
    "Performance Gain: 98.67x faster with compiled version\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2322d801-18d4-4512-a993-9a58221b34d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile generated_files/compile_numpy.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
    "from utils import todo\n",
    "\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Kmeans function\n",
    "def kmeans(X, means):\n",
    "    return np.argmin(np.linalg.norm(X - means[:, None], axis=2), axis=0)\n",
    "\n",
    "# Prepare synthetic dataset\n",
    "npts = 10_000_000\n",
    "X = np.repeat([[5, 5], [10, 10]], [npts, npts], axis=0)\n",
    "X = X + np.random.randn(*X.shape)  # Creating two distinct \"blobs\"\n",
    "means = np.array([[5, 5], [10, 10]])\n",
    "\n",
    "# Set the device to CUDA if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert NumPy arrays to tensors on device\n",
    "X_tensor = torch.tensor(X).to(device)\n",
    "means_tensor = torch.tensor(means).to(device)\n",
    "\n",
    "# Wrap the kmeans function with torch.compile for optimization\n",
    "compiled_kmeans = todo()\n",
    "\n",
    "# Performance evaluation \n",
    "nbr_iter = 10\n",
    "\n",
    "# Execute a first time the compiled function and measure performance\n",
    "start.record()\n",
    "_ = compiled_kmeans(X_tensor, means_tensor)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "first_run_compiled_time = start.elapsed_time(end)/nbr_iter/10\n",
    "print(f\"First Run Compiled Time: {first_run_compiled_time:.4f} seconds\")\n",
    "\n",
    "# Original kmeans function execution\n",
    "start.record()\n",
    "for _ in range(nbr_iter):\n",
    "    original_pred = kmeans(X, means)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "mean_original_execution_time = start.elapsed_time(end)/nbr_iter/10\n",
    "print(f\"Original Execution Time: {mean_original_execution_time:.4f} seconds\")\n",
    "\n",
    "# Compiled kmeans function execution\n",
    "start.record()\n",
    "for _ in range(nbr_iter):\n",
    "    compiled_pred = compiled_kmeans(X_tensor, means_tensor)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "mean_compiled_execution_time = start.elapsed_time(end)/nbr_iter/10\n",
    "print(f\"Mean Compiled Execution Time: {mean_compiled_execution_time:.4f} seconds\")\n",
    "\n",
    "# Compare the performance\n",
    "performance_gain = mean_original_execution_time / mean_compiled_execution_time\n",
    "print(f\"Performance Gain: {performance_gain:.2f}x faster with compiled version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945ac5b-cdb7-4271-9697-1ef7b7995d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "command = f'./generated_files/compile_numpy.py'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu=1, module=MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00')\n",
    "print(f'jobid = {jobid}')\n",
    "#!python {command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d33acc-76cb-45fd-a8fd-f7cfed7694e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5809bfb-a9b2-4204-b8af-dfe2ab2da549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nOut file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stdout'][0]}\n",
    "print(\"\\nError file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dbfba2-4158-4dda-9d2d-f32d7ba8a1c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Accelerating Model Training with Torch.compile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95c9c0fa-644e-4681-b35a-14fa1374ea4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Torch.compile's primary advantage lies in its ability to significantly boost the training efficiency of deep learning models. This powerful feature enables the use of optimized backends like 'cudagraphs' for GPU acceleration and 'inductor' for broad compatibility across CPUs and GPUs, streamlining the training process.\n",
    "\n",
    "To optimize your model's training with Torch.compile, simply wrap your model instance as demonstrated below:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from your_model import YourModel  # Replace with your actual model class\n",
    "\n",
    "# Initialize your model\n",
    "model_raw = YourModel()\n",
    "\n",
    "# Optimize with Torch.compile\n",
    "model = torch.compile(model_raw, backend='inductor')  # 'cudagraphs' is also an option for GPU optimization\n",
    "```\n",
    "\n",
    "The reference performance of the `train_pipeline.py` script without torch.compile() is : **1170**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1af567-37e9-4e98-99a6-99400e27abd8",
   "metadata": {},
   "source": [
    ">**Task:** Modify the `train_pipeline.py` script, which outlines a standard training routine for a PyTorch model. In the model setup section, incorporate Torch.compile to enhance training efficiency. This adjustment is aimed at expediting training times and improving model performance across your hardware setup. After making the changes, return and run the code in the cell below to observe the improvements.\n",
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "    model = torch.compile(model_raw, backend=\"inductor\")\n",
    "```\n",
    "You should achieve a throughput around 1300. As for now, on Jean Zay for this code, only the inductor backend will work with mode default or reduce-overhead.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d7302-b8a8-4f11-8bc2-776938aa2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f'train_pipeline.py -b 512 --image-size 224 --test'\n",
    "jobid = gpu_jobs_submitter(command, n_gpu=1, module=MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13693154-9935-411d-ac18-5cf75cb0fb1a",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule prÃ©cÃ©dente en mode `Raw NBConvert`, afin d'Ã©viter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88995d13-d999-4c14-a676-fd168685eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052d886-2c6b-493b-b1c3-7c5c64dae1d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nOut file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stdout'][0]}\n",
    "print(\"\\nError file:\\n\")\n",
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b2c37-d03d-4277-a003-9add498588cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe291c2-50c5-4ebe-a8e1-f2bddf030136",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Streamlining Training with Advanced Torch.compile Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3c4a4-63a6-477e-890f-4a197ff42c13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "When enhancing your training setup with Torch.compile, incorporating it with Distributed Data Parallel (DDP) and the optimizer involves precise steps for optimal performance:\n",
    "\n",
    "#### Integrating DDP with Torch.compile\n",
    "\n",
    "Ensure DDP is applied before Torch.compile to maintain proper functionality across multiple GPUs:\n",
    "\n",
    "```python\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "model_ddp = DDP(model_raw)  # Wrap model with DDP\n",
    "model = torch.compile(model_ddp, backend='inductor')  # Then compile\n",
    "```\n",
    "\n",
    "#### Compiling the Optimizer\n",
    "\n",
    "With PyTorch 2.2.0+, optimizers can be compiled in an experimental feature to enhance the training loop. Wrap the optimizer step in a function and compile it for execution optimization:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "model = YourModel()  # Initialize your model\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)  # Define the optimizer\n",
    "\n",
    "@torch.compile(fullgraph=False)\n",
    "def optimized_step():\n",
    "    opt.step()  # Compile the optimizer step\n",
    "```\n",
    "\n",
    "> **Task:** After applying Torch.compile to your model in `train_pipeline.py`, try compiling the optimizer step as shown. This advanced, experimental technique aims to refine the training process further.\n",
    "\n",
    "Leveraging these advanced Torch.compile strategies can significantly enhance the efficiency and speed of your model training, offering a cutting-edge approach to deep learning optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57647413-ca8a-48ec-9e9f-94065be2cfe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Enhancing Inference with Torch.compile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d260f-b38f-46fb-8be2-45782311ce4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Torch.compile now offers an experimental feature that integrates with NVIDIA's TensorRT, a leading deep learning inference optimizer. This collaboration aims to boost inference performance on NVIDIA GPUs, utilizing TensorRT's capabilities for neural network optimization, including layer fusion and precision calibration. The goal is to markedly improve inference speed and efficiency, a crucial advantage for deployment in performance-sensitive production settings.\n",
    "\n",
    "The synergy between TensorRT and Torch.compile simplifies achieving optimal inference speeds, making it easier to enhance model performance in critical production environments.\n",
    "\n",
    "Additionally, it's important to select a backend that aligns with your specific hardware capabilities. For instance, Intel architectures can benefit from using the Intel Extension for PyTorch (IPEX), which optimizes performance on Intel CPUs. This tailored approach ensures that you leverage the best possible optimization techniques for your hardware, further enhancing inference efficiency and model responsiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-1.11.0_py3.9.12",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-1.11.0_py3.9.12"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
