{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLO-JZ Optimisation de l'apprentissage - Jour 1 \n",
    "\n",
    "Optimisation système d'une boucle d'apprentissage *Resnet-50*.\n",
    "\n",
    "![car](./images/optimisation.png)\n",
    "\n",
    "## Objet du notebook\n",
    "\n",
    "Le but de ce *notebook* est d'optimiser un code d'apprentissage d'un modèle *Resnet-50* sur *Imagenet* pour Jean Zay en implémentant :\n",
    "* **TP 1** : l'accélération GPU\n",
    "* **TP 2** : l'*Automatic Mixed Precision*\n",
    "* **TP 3** : le *Channels Last Memory Format*\n",
    "\n",
    "Les cellules dans ce *notebook* ne sont pas prévues pour être modifiées, sauf rares exceptions indiquées dans les commentaires. Les TP se feront en modifiant le code `dlojz.py`.\n",
    "\n",
    "Les directives de modification seront marquées par l'étiquette **TODO :**, dans le *notebook* suivant.\n",
    " \n",
    "Les solutions sont présentes dans le répertoire `solutions`.\n",
    "\n",
    "*Notebook rédigé par l'équipe assistance IA de l'IDRIS, juin 2023*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un module PyTorch doit avoir été chargé pour le bon fonctionnement de ce Notebook. **Nécessairement**, le module `pytorch-gpu/py3/1.11.0` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions *python* de gestion de queue SLURM dévelopées par l'IDRIS et les fonctions dédiées à la formation DLO-JZ sont à importer.\n",
    "\n",
    "Le module d'environnement pour les *jobs* et la taille des images sont fixés pour ce *notebook*.\n",
    "\n",
    "**TODO :** choisir un *pseudonyme* (maximum 5 caractères) pour vous différencier dans la queue SLURM et dans les outils collaboratifs pendant la formation et la compétition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "from dlojz_tools import controle_technique, compare, GPU_underthehood, plot_accuracy, lrfind_plot, imagenet_starter\n",
    "MODULE = 'pytorch-gpu/py3/1.11.0'\n",
    "image_size = 176\n",
    "account = 'for@v100'\n",
    "name = 'pseudo'   ## Pseudonyme à choisir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation d'un repertoire `checkpoints` si cela n'a pas déjà été fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion de la queue SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie permet d'afficher et de gérer la queue SLURM.\n",
    "\n",
    "Pour afficher toute la queue *utilisateur* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque**: Cette fonction utilisée plusieurs fois dans ce *notebook* permet d'afficher la queue de manière dynamique, rafraichie toutes les 5 secondes. Cependant elle ne s'arrête que lorsque la queue est vide. Si vous désirez reprendre la main sur le *notebook*, il vous suffira d'arrêter manuellement la cellule avec le bouton *stop*. Cela a bien sûr aucun impact sur le *scheduler* SLURM. Les *jobs* ne seront pas arrêtés.\n",
    "\n",
    "Si vous voulez arrêter des *jobs* dans la queue:\n",
    "* Annuler tous vos *jobs* dans la queue (décommenter la ligne suivante) : `!scancel -u $USER`\n",
    "* Annuler un *job* dans votre queue (décommenter la ligne suivante et ajouter le numéro du *job* à la fin de la ligne)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!scancel -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie *debug* permet d'afficher les fichiers de sortie et les fichiers d'erreur du *job*.\n",
    "\n",
    "Il est nécessaire dans la cellule suivante (en décommentant) d'indiquer le *jobid* correspondant sous le format suivant.\n",
    "\n",
    "***Remarque*** : dans ce notebook, lorsque vous soumettrez un *job*, vous recevrez en retour le numéro du job dans le format suivant : `jobid = ['123456']`. La cellule ci-dessous peut ainsi être facilement actualisée.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = ['1493206']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier de sortie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichier d'erreur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat {search_log(contains=jobid[0], with_err=True)['stderr'][0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "### Différence entre deux scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le *debug* ou pour comparer son code avec les solutions mises à disposition, la fonction suivante permet d'afficher une page html contenant un différentiel de fichiers texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"dlojz.py\"\n",
    "s2 = \"./solutions/dlojz1_3.py\"\n",
    "compare(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir le résultat du différentiel de fichiers sur la page suivante (attention au spoil !) :\n",
    "\n",
    "[compare.html](compare.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "## Dataset et modèle\n",
    "\n",
    "Cette partie permet de visualiser les caractéristiques du *dataset* et du modèle utilisés.\n",
    "\n",
    "### Imagenet\n",
    "\n",
    "#### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "        transforms.RandomResizedCrop(176),             # Random resize - Data Augmentation\n",
    "        transforms.RandomHorizontalFlip(),              # Horizontal Flip - Data Augmentation\n",
    "        transforms.ToTensor(),                          # convert the PIL Image to a tensor\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "    \n",
    "    \n",
    "train_dataset = torchvision.datasets.ImageNet(root=os.environ['ALL_CCFRSCRATCH']+'/imagenet',\n",
    "                                                  transform=transform)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,    \n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True)\n",
    "batch = next(iter(train_loader))\n",
    "print('X train batch, shape: {}, data type: {}, Memory usage: {} bytes'\n",
    "      .format(batch[0].shape, batch[0].dtype, batch[0].element_size()*batch[0].nelement()))\n",
    "print('Y train batch, shape: {}, data type: {}, Memory usage: {} bytes'\n",
    "      .format(batch[1].shape, batch[1].dtype, batch[1].element_size()*batch[1].nelement()))\n",
    "\n",
    "img = batch[0][0].numpy().transpose((1,2,0))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "labels_cls, labels_id = torch.load(os.environ['ALL_CCFRSCRATCH']+'/imagenet/meta.bin')\n",
    "label = labels_cls[np.unique(labels_id)[batch[1][0].numpy()]]\n",
    "_ = plt.title('label class: {}'.format(label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "                                    transforms.Resize((256, 256)),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),   # convert the PIL Image to a tensor\n",
    "                                    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                    std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "val_dataset = torchvision.datasets.ImageNet(root=os.environ['ALL_CCFRSCRATCH']+'/imagenet', split='val',\n",
    "                        transform=val_transform)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnet50()\n",
    "print('number of total parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "print('number of trainable parameters: {}'.format(sum([p.numel() for p in model.parameters() if p.requires_grad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1_0 : Baseline CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce TP consiste à appliquer le code *baseline* pour prendre en main les fonctionnalités de test et découvrir le code.\n",
    "\n",
    "**TODO :**\n",
    "1. Exécuter les cellules suivantes (le *job* prend 7 minutes environ)\n",
    "2. Puis, ouvrir le fichier `dlojz.py`\n",
    "\n",
    "Remarque :\n",
    "* l'option *test* lance un apprentissage de 50 itérations.\n",
    "* les chronomètres mesurent les temps de la 10e à la 50e itération et restitue un temps moyen par itération.\n",
    "* les parties `DON'T MODIFY` dans le script ne doivent pas être modifiées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --test'\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:20:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1493206']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code *baseline* `dlo-jz.py` a été exécuté sur le CPU (contrairement à ce qui est indiqué par le contrôle technique) en mode *test*, soit sur 50 itérations.\n",
    "\n",
    "Dans le prochain exercice nous verrons ensemble l'accélération sur 1 GPU.\n",
    "\n",
    "\n",
    "![Garage](images/stop.png \"Arrêtez-vous ici! Une présentation vous attend avant le prochain TP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1_1 : Accélération GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : dans le script `dlojz.py` :\n",
    "* Définir la variable `gpu` et envoyer le modèle dans la mémoire du GPU.\n",
    "```python\n",
    "    # define model\n",
    "    gpu = torch.device(\"cuda\")\n",
    "    model = models.resnet50()\n",
    "    model = model.to(gpu)\n",
    "```\n",
    "\n",
    "* Envoyer les *batches* d'images d'entrée et les *labels* associés sur le GPU, pour **les étapes de *training* et de *validation***. *Remarque* : `non_blocking=args.non_blocking` sera utile plus tard lors de la définition d'un DataLoader distribué.\n",
    "\n",
    "```python\n",
    "    # distribution of images and labels to all GPUs\n",
    "    images = images.to(gpu, non_blocking=args.non_blocking)\n",
    "    labels = labels.to(gpu, non_blocking=args.non_blocking)\n",
    "```\n",
    "et\n",
    "\n",
    "```python\n",
    "    # distribution of images and labels to all GPUs\n",
    "    val_images = val_images.to(gpu, non_blocking=args.non_blocking)\n",
    "    val_labels = val_labels.to(gpu, non_blocking=args.non_blocking)\n",
    "```\n",
    "\n",
    "* Envoyer les tenseurs pour les métriques de validation sur le GPU.\n",
    "```python\n",
    "    ## Initialisation  \n",
    "    if idr_torch.rank == 0: accuracies = []\n",
    "    val_loss = torch.Tensor([0.]).to(gpu)                  # send to GPU\n",
    "    val_accuracy = torch.Tensor([0.]).to(gpu)              # send to GPU\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "batch_size = 128\n",
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --test'\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1493317']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 1\n",
    "batch_size = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "command = [f'dlojz.py -b {b} --image-size {image_size} --test'\n",
    "          for b in batch_size]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['1493424', '1493432', '1493433', '1493434', '1493435', '1493436', '1493438', '1493440']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dernier *job* a atteint le seuil *CUDA Out Of Memory* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique([jobids[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Garage](images/stop.png \"Arrêtez-vous ici! Une présentation vous attend avant le prochain TP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1_2 : Automatic Mixed Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : dans le script `dlojz.py` :\n",
    "* Importer les fonctionnalités liées à l'*Automatic Mixed Precision*.\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "```\n",
    "\n",
    "* Initialiser le *scaler*.\n",
    "```python\n",
    "    # Creates a GradScaler once at the beginning of training.\n",
    "    scaler = GradScaler()\n",
    "```    \n",
    "\n",
    "* Implémenter l'*autocasting* (le changement de précision, FP32 à FP16) dans le *forward* , avec la ligne `with autocast():` dans la boucle de *training* **et** la boucle de validation.\n",
    "\n",
    "```python\n",
    "    ...\n",
    "    # Runs the forward pass with autocasting.\n",
    "    with autocast():\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "    ...\n",
    "```\n",
    "\n",
    "* Implémenter le *gradient scaling* pour la seule boucle de *training*. **Note**: À la place des lignes `loss.backward()` et `optimizer.step()`.\n",
    "\n",
    "```python\n",
    "    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "    # Backward passes under autocast are not recommended.\n",
    "    # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "    # otherwise, optimizer.step() is skipped.\n",
    "    scaler.step(optimizer)\n",
    "\n",
    "    # Updates the scale for next iteration.\n",
    "    scaler.update()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "batch_size = 128\n",
    "command = f'dlojz.py -b {batch_size} --image-size {image_size} --test'\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobid = ['xxxxx']` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1493501']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 1\n",
    "batch_size = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "command = [f'dlojz.py -b {b} --image-size {image_size} --test'\n",
    "          for b in batch_size]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['1493508', '1493509', '1493511', '1493512', '1493513', '1493515', '1493516', '1493517']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement de taille de batch\n",
    "\n",
    "**TODO :** Choisir pour la suite du TP une taille de batch par GPU qui vous semble la plus pertinente selon le test précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choisir un batch size optimal\n",
    "bs_optim = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test'\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1493563']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commentaires](images/cedez.png \"Assurez-vous que tout se passe bien avant de continuer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1_3 : Channels Last Memory Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : dans le script `dlojz.py` :\n",
    "* Lors de l'envoie du modèle au GPU, configurer le paramètre `memory_format` avec `torch.channels_last`.\n",
    "```python\n",
    "    # define model\n",
    "    gpu = torch.device(\"cuda\")\n",
    "    model = models.resnet50()\n",
    "    model = model.to(gpu, memory_format=torch.channels_last)\n",
    "```\n",
    "\n",
    "* Lors de l'envoie des images d'entrée au GPU, configurer le paramètre `memory_format` avec `torch.channels_last`.\n",
    "\n",
    "```python\n",
    "    # distribution of images and labels to all GPUs\n",
    "    images = images.to(gpu, non_blocking=args.non_blocking, memory_format=torch.channels_last)\n",
    "    labels = labels.to(gpu, non_blocking=args.non_blocking)\n",
    "```\n",
    "et\n",
    "\n",
    "```python\n",
    "    # distribution of images and labels to all GPUs\n",
    "    val_images = val_images.to(gpu, non_blocking=args.non_blocking, memory_format=torch.channels_last)\n",
    "    val_labels = val_labels.to(gpu, non_blocking=args.non_blocking)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = 1\n",
    "command = f'dlojz.py -b {bs_optim} --image-size {image_size} --test'\n",
    "command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "jobid = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                    account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobid = {jobid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobid = ['1493619']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controle_technique(jobid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'occupation mémoire\n",
    "\n",
    "Afin de mesurer l'impact de la taille de batch sur l'occupation mémoire et sur le *throughput*, la cellule suivante permet de soumettre plusieurs *jobs* avec des tailles de *batch* croissantes. Dans les cas où la mémoire est saturée et dépasse la capacité du GPU, le système renverra une erreur *CUDA Out of Memory*.\n",
    "\n",
    "Soumission du *job*. **Attention vous sollicitez les noeuds de calcul à ce moment-là**.\n",
    "\n",
    "Pour soumettre le job, veuillez basculer la cellule suivante du mode `Raw NBConvert` au mode `Code`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_gpu = 1\n",
    "batch_size = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
    "command = [f'dlojz.py -b {b} --image-size {image_size} --test'\n",
    "          for b in batch_size]\n",
    "jobids = gpu_jobs_submitter(command, n_gpu, MODULE, name=name,\n",
    "                   account=account, time_max='00:10:00', constraint='v100-32g')\n",
    "print(f'jobids = {jobids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copier-coller la sortie `jobids = ['xxxxx', ...]` dans la cellule suivante.\n",
    "\n",
    "Puis, rebasculer la cellule précédente en mode `Raw NBConvert`, afin d'eviter de relancer un job par erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobids = ['1493654', '1493656', '1493657', '1493658', '1493659', '1493660', '1493661', '1493662']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_slurm_queue(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_underthehood(jobids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Garage](images/stop.png \"Arrêtez-vous ici! Une présentation vous attend avant le prochain TP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.0.0_py3.10.9",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.0.0_py3.10.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
