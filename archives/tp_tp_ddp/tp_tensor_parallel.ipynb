{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4b4224-50bd-43f4-8227-fe03befc5cde",
   "metadata": {},
   "source": [
    "# DLO-JZ Implémentation du Tensor Parallelism\n",
    "\n",
    "## Vocabulaire\n",
    "\n",
    "* TS: Tensor Slicing, autre nom du Tensor Parallelism (pour ne pas avoir de collisions avec TP = Travaux Pratiques :D)\n",
    "* PP: Pipeline Parallelism\n",
    "* DDP: Distributed Data Parallelism\n",
    "\n",
    "## Objet du notebook\n",
    "\n",
    "Dans ce notebook, vous serez guidé pour implémenter du tensor parallelism (aussi appelé tensor slicing) avec du Pytorch Natif.\n",
    "\n",
    "Le modèle utilisé que vous devrez paralléliser est un transformer de type BERT simplifié (par exemple il n'a pas de PositionalEncoding, mais c'est pas bien grave). En effet, les modèles de computer vision de type convolution ne sont pas assez larges généralement pour justifier l'usage du TS.\n",
    "Certains modèles sophistiqués comme CoatNet utilisent à la fois des convolutions et des couches d'attention pour résoudre des problèmes de traitement d'images. Mais cela exigerait de paralléliser plus de types de couches. Pour simplifier les choses je suis parti sur un apprentissage de NLP avec une seule pile d'encodeurs (un BERT donc).\n",
    "\n",
    "Un schéma du réseau que j'ai implémenté est disponible dans [cet article](https://arxiv.org/pdf/1909.08053.pdf) (Figure 2).\n",
    "\n",
    "La tâche à résoudre est un dataset de reviews d'IMDb. Vous pouvez observer les csv dans le sous-dossier `data` et constater que l'on a du texte de revues de films, ainsi qu'un label qui dit si la critique est plutôt positive ou négative. On va utiliser un transformer pour automatiser cela. Normalement la tâche est suffisamment simple pour être résolu avec un petit transformer. Mais pour faire apparaître les besoins de tensor slicing, on va utiliser BERT-Base puis BERT-Large.\n",
    "\n",
    "On ne fera pas d'apprentissage complet. Au début on voudra juste tester nos modifications de code, donc on ne fera que quelques steps.\n",
    "\n",
    "Dans un second temps, après que le TS sera fonctionnel, je propose des parties _Pour aller plus loin_ qui viseront à remettre de la DDP par dessus, pour avoir un parallélisme 2D. Un parallélisme 3D exigerait également l'ajout de _Pipeline Parallelism_ mais cela demanderait plus de temps, de légèrement retravailler le notebook et les scripts et c'est conceptuellement bien plus difficile que le TS, donc le PP est au-delà des objectifs de ce TP.\n",
    "\n",
    "## Scripts à disposition\n",
    "\n",
    "Vous devriez avoir dans ce dossier 7 scripts python :\n",
    "\n",
    "* dataset.py : Chargement des datasets et instanciation des dataloaders. Vous n'aurez aucun changement à y faire (sauf dans les parties _Pour aller plus loin_).\n",
    "* model.py : Implémentation du modèle transformer non-distribué. Vous n'aurez aucun changement à y faire.\n",
    "* pcomm.py (pour process communication car la bibliothèque comm existe déjà :( ): Implémentation des couches de communication nécessaires au TS. Vous travaillerez dessus.\n",
    "* setup.py : Mes petits outils internes, vous n'avez pas besoin de les consulter, encore moins de les modifier.\n",
    "* test.py : De courts scripts pour tester chacune des couches que vous implémenterez sans vous embêtez à lancer un entraînement. Chaque test consiste en une forward (comparaison des outputs), une backward (comparaison des gradients) et une step d'optimisation (comparaison des poids après optimisations). Vous n'avez pas besoin de besoin de les consulter, encore moins de les modifier.\n",
    "* tp_model.py : Une quasi-copie de `model.py`, c'est là que vous travaillerez le plus pour convertir votre transformer avec du TS.\n",
    "* tp_tensor_parallelism.py : Boucle d'apprentissage et une validation simples. Elle est très similaire au TP sur la DDP. Vous remarquerez dans ce script que j'ai ajouté la précision mixte. Les modifications de base (par exemple `dist.init_process_group` ou `torch.cuda.set_device`) sont déjà incluses car vous avez déjà travaillé sur ces aspects. Vous n'aurez aucun changement  à y faire (sauf dans les parties _Pour aller plus loin_)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d3feb-6be7-4044-904b-9ec41622cf89",
   "metadata": {},
   "source": [
    "## Initialisation et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2e837-d20d-44f2-8047-134c92857a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import datetime\n",
    "from idr_pytools import display_slurm_queue, gpu_jobs_submitter, search_log\n",
    "\n",
    "from setup import read_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6d70b-a421-4063-8554-84f1cac33bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"pseudo\"\n",
    "account = \"for@a100\"\n",
    "module = \"pytorch-gpu/py3/2.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256fd3c-c971-46f1-9503-bab99f23f413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute(ntasks: int = 2, epochs: int = 1, samples: int = 1024, layers: int = 6, hidden_dim: int = 768, heads: int = 1, batch_size: int = 1, tp: int = 1) -> None:\n",
    "    \"\"\"L'argument `tp` fait référence au degré de tensor parallelism. Il n'est pertinent que dans les sous-parties \"Pour aller plus loin\" \"\"\"\n",
    "    execid = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    command = f\"python3 tp_tensor_parallel.py --execid={execid} --epochs={epochs} --samples={samples} --layers={layers} --dim={hidden_dim} --heads={heads} --bsz={batch_size} --tp={tp}\"\n",
    "    jobid = gpu_jobs_submitter(\n",
    "        command, ntasks, module, name=name, account=account, time_max=\"00:10:00\", qos=\"qos_gpu-dev\", constraint=\"a100\"\n",
    "    )\n",
    "    display_slurm_queue(name)\n",
    "    read_metrics(execid)\n",
    "\n",
    "def test(classname: str):\n",
    "    command = f\"python3 test.py {classname}\"\n",
    "    jobid = gpu_jobs_submitter(\n",
    "        command, 2, module, name=name, account=account, time_max=\"00:10:00\", qos=\"qos_gpu-dev\", constraint=\"a100\"\n",
    "    )\n",
    "    display_slurm_queue(name)\n",
    "    %cat {search_log(name, contains=jobid[0])[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314031d9-c3b0-4e16-a321-f5d648b45e49",
   "metadata": {},
   "source": [
    "## I. Première exécution mono-GPU\n",
    "\n",
    "_Note: Avec la config ci-dessous (Bert-Base) ça doit rentrer un GPU V100-16GB, mais si vous utilisez un BERT-Large (Layers=24, Heads=16, Hidden_dim=1024), vous devriez avoir un OOM sur un V100-32GB. Dans la suite, on va naviguer entre les deux configurations pour stresser le code sans faire d'OOM_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866b294-e1a8-4a6b-933c-ff35180624a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute(ntasks=1, epochs=1, samples=1024, layers=12, hidden_dim=768, heads=8, batch_size=32, tp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a8a61-db2e-4a98-9de7-96330dc76adc",
   "metadata": {},
   "source": [
    "## II. Communication en Tensor Parallelism\n",
    "\n",
    "Note : Pour tout ce qui suit, lorsque vous aurez besoin de faire référence au _rank_ ou _world_size_, vous n'appelerez **pas** directement _idr_torch_. Ces variables seront stockées dans _pcomm.tp_rank_ et _pcomm.tp_degree_. En procédant ainsi, cela simplifiera les parties _Pour aller plus loin_, puisqu'il suffira de changer l'initialisation de ces variables plutôt que de changer chacune de vos modifications. Cette initialisation est faite dans le fichier `pcomm.py` (fonction `init`).\n",
    "\n",
    "Pour distribuer notre Transformer avec du TS, nous pouvons nous appuyer sur [cet article](https://arxiv.org/pdf/1909.08053.pdf). La figure 3 nous donne toutes les informations dont nous avons besoin pour avancer dans le TP.\n",
    "\n",
    "Notamment dans ce schéma, les auteurs ont introduit deux opérations de communication qu'ils ont appelées _f_ et _g_. Par soucis de clarté, je les ai respectivement appelées _Duplication_ et _AllReduce_, puisque c'est ce que font effectivement ces couches dans la forward.\n",
    "\n",
    "**TODO:** Balise 1 -- Dans `pcomm.py`, implémentez les forwards et backwards des fonctions _Duplication_ et _AllReduce_. La fonction _AllGather_ vous est déjà fournie.\n",
    "\n",
    "**HINT:** Toutes les informations nécessaires sont disponibles dans l'article fourni, le code est donné juste avant la figure.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise1.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa64366-9854-4615-b083-524b6795ecfc",
   "metadata": {},
   "source": [
    "## III. Paralléliser chaque couche\n",
    "\n",
    "On va utiliser nos 3 fonctions _Duplication_, _AllGather_ et _AllReduce_ pour distribuer notre transformer.\n",
    "\n",
    "### Couche linéaire\n",
    "\n",
    "Commençons par la couche linéaire. Il y a deux manières de découper une couche linéaire (selon les lignes ou selon les colonnes). Chaque manière implique des communications différentes. Dans ce TP, pour simplifier les choses, toutes les couches linéaires sont sans biais, et on va se focaliser sur un découpage **le long des colonnes**.\n",
    "\n",
    "**TODO:** Balise 2 -- Dans `tp_model.py`, complétez la classe _ColWiseLinear_. On appelera un découpage le long des colonnes le fait de découper selon la dimension de sortie de la couche. Vous pourrez vous aider des formules dans les slides de cours.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise2.py`\n",
    "\n",
    "Vous pouvez tester cette classe via le script suivant (qui fait un forward, compare les outputs, un backward, qui compare les gradients et une step, qui compare les poids après la step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2731c-e1ad-4bd4-852a-79eb2edbb65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(\"ColWiseLinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f800b93-5c94-4968-8b76-3e64d65580a7",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "**TODO:** Balise 3 -- Faire la même chose avec la couche Embedding\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise3.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bcd68-af66-4557-ae29-9d5af109f236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(\"Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94b7a6-3dbe-4521-a705-f67d9355a246",
   "metadata": {},
   "source": [
    "### MultiHeadSelfAttention\n",
    "\n",
    "**TODO:** Balise 4 -- Faire pareil avec la couche MultiHeadSelfAttention\n",
    "\n",
    "**HINT:** Aidez vous de la figure suivante\n",
    "![megatron_attention](images/megatron_attention.png)\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise4.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae181fe-878e-4690-97a8-302ea9b6bc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(\"MultiHeadSelfAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89343ca8-5785-40c9-bcba-9dfcafea19f3",
   "metadata": {},
   "source": [
    "### Assemblons notre transformer\n",
    "\n",
    "Nos couches _Embedding_ et _MultiHeadSelfAttention_ ont bien été transformées. La couche _ColWiseLinear_ est supposée remplacée une couche linéaire classique.\n",
    "Notre module _FeedForwardBlock_ continue d'appeler une couche linéaire standard, il faut changer ça.\n",
    "\n",
    "**TODO:** Balise 5 -- Remplacez les couches linéaires par des couches de type _ColWiseLinear_.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise5.py`\n",
    "\n",
    "_Note: Ce module est utilisé à la fois par tous les blocs de Transformer (class Block dans `tp_model.py`) mais je l'ai aussi ré-utilisé dans le classifier final (class Classifier) car j'avais envie de faire un classifier à deux couches (mais j'aurais pu décider autrement)._\n",
    "\n",
    "_Note: la consommation mémoire devrait avoir fortement baissé. Un BERT-Large doit normalement pouvoir passer sur les V100-16GB._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1914f-ca85-4d8e-bbdb-4d5a18b66bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute(ntasks=2, epochs=1, samples=1024, layers=12, hidden_dim=768, heads=8, batch_size=32, tp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34466b06-86c4-4bda-842d-b24866d45507",
   "metadata": {},
   "source": [
    "## IV. Optimisation des communications\n",
    "\n",
    "Là on a un petit problème. Chaque couche linéaire fait une communication. Or, quand on a deux couches linéaires consécutives, ce n'est pas forcément obligatoire, on pourrait se contenter d'une seule communication.\n",
    "\n",
    "**TODO:** Balise 6 -- Implémentez la couche _ColRowLinearPair_.\n",
    "\n",
    "**HINT:** Aidez-vous de la figure suivante. ![megatron_mlp](images/megatron_mlp.jpeg)\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise6.py`\n",
    "\n",
    "Testez la:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff90d73-272d-4259-bfa9-646b2794d330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(\"ColRowLinearPair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195e523-7486-4ad5-a7c9-d6fa1c4f88b7",
   "metadata": {},
   "source": [
    "### Assemblons notre nouveau transformer\n",
    "\n",
    "Notre module _ColRowLinearPair_ doit remplacer le _FeedForwardBlock_ dans les couches _Block_ et _Classifier_.\n",
    "\n",
    "**TODO:** Balise 7 -- Faire ces modifications.\n",
    "\n",
    "_Note: Normalement la conso mémoire diminue d'un chouia, et vous allez un peu plus vite, vu que vous faites moins de comm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd034-4f75-4f9f-befd-d6128dec6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(ntasks=2, epochs=1, samples=1024, layers=12, hidden_dim=768, heads=8, batch_size=32, tp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91929da-c6f5-408c-ad44-0df729c8eae1",
   "metadata": {},
   "source": [
    "**FIN pour le Tensor Slicing** Ce notebook vous a montré comment on peut faire du Tensor Slicing sur un transformer pour mieux comprendre son fonctionnement. On se rend compte qu'il y a assez peu de modifications à introduire, mais qu'elle nécessite de changer directement les couches du réseau, ce qui peut être compliqué dans certains cas.\n",
    "\n",
    "Si vous êtes encore motivé et que vous avez encore du temps, vous pouvez faire les parties _Pour aller plus loin_. Elles vous guideront pour remettre de la DDP afin d'avoir un parallélisme 2D.\n",
    "\n",
    "## V. Pour aller plus loin - Parallélisme 2D\n",
    "\n",
    "Vu que cette fois, on va mettre en place de la DDP en plus du TS, nos communications collectives n'impliqueront plus tout le monde.\n",
    "\n",
    "Chaque communication collective permet de ne pas impliquer tous les processus, mais seulement un sous-groupe, réuni dans un communicateur. Avant de mettre le data parallélisme en place,  il faut donc modifier nos communications TS.\n",
    "\n",
    "**TODO:** Balise 8 -- Créer un sous-communicateur pour le TS. Vous pourrez le stocker dans la variable `tp_grp`. Ce sous-communicateur ne contient pas tous les processus. Le degré de TS est donné par l'argument _tp_ dans la fonction _init_. Le communicateur ne contient tous les processus que si _tp_ = _ntasks_\n",
    "\n",
    "**HINT:** Elle est cool [cette doc](https://pytorch.org/docs/stable/distributed.html#groups).\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise8.py`\n",
    "\n",
    "-----------\n",
    "\n",
    "**TODO:** Balise 9 -- Changez les communications dans `pcomm.py` pour utiliser ce sous-communicateur.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise9.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5d7bd-4ab8-4f53-ac7e-f9f0b94be452",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(ntasks=4, epochs=1, samples=1024, layers=24, hidden_dim=1024, heads=16, batch_size=32, tp=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5696f9e-aa71-48f8-b11d-2b985609b44f",
   "metadata": {},
   "source": [
    "**TODO**: Balise 10 -- Créez un sous-communicateur pour la DDP. Attention, pour les deux sous-communicateurs (_tp_grp_ et _dp_grp_), vous ne pouvez plus leur donner tous les processus, il faut faire un partitionnement.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise10.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071eaffd-f7a4-4e70-951c-d3921f9b1774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute(ntasks=4, epochs=1, samples=1024, layers=12, hidden_dim=768, heads=8, batch_size=32, tp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaeea34-019b-49b1-bcf5-727c1ad27859",
   "metadata": {},
   "source": [
    "Ça a l'air de tourner mais là on a simplement les communicateurs. On ne fait pas le data parallélisme correctement.\n",
    "\n",
    "**TODO:** Balise 11 -- Mettre la DDP : fichier `dataset.py` et fichier `tp_tensor_parallélisme.py`.\n",
    "\n",
    "**SOLUTION:** Dans le fichier `solutions/balise11.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490c645-e318-4b1f-9feb-12ba24268c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute(ntasks=4, epochs=1, samples=1024, layers=12, hidden_dim=768, heads=8, batch_size=32, tp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c9735-ba85-47ea-b790-dba0946c8ec9",
   "metadata": {},
   "source": [
    "**TODO:** Pour éviter les problèmes avec NCCL, il est crucial que vous mettiez l'argument `use_local_synchronization=True` dans les créations de nouveaux communicateurs quand vous en avez plusieurs, sinon ça va crasher. Modifiez la balise 8 et la balise 10 en conséquence si ce n'est pas déjà fait.\n",
    "\n",
    "Testez votre parallélisme 2D :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bbce3-ef60-443d-a3d0-0a25e54fa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(ntasks=4, epochs=1, samples=1024, layers=24, hidden_dim=1024, heads=16, batch_size=32, tp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4ef8c-1212-47c8-9e07-36d20f7fc433",
   "metadata": {},
   "source": [
    "# FINI !!!!!!!!!!\n",
    "\n",
    "Félicitations vous connaissez maintenant le 2D parallélisme. Comme vous pouvez le constater le TS est un peu fastidieux car il exige d'aller au plus profond du code (pas toujours simple). Mais conceptuellement c'est pas si compliqué et on a pu le mettre en place en seulement une dizaine de balises. Le 3D parallélisme demande l'ajout de PP. C'est possible, il faudrait revoir la manière dont nos `Block` fonctionnent et y faire des communications. Il faudrait aussi faire un nouveau sous-communicateur pour le PP. Le PP est conceptuellement beaucoup plus compliqué à implémenter mais le principe général est le même.\n",
    "\n",
    "Maintenant si vous le voulez, vous pouvez jouer avec les paramètres pour vérifier que la conso mémoire diminue bien, constater les temps de calcul (vous devriez normalement trouver que le data parallélisme est plus avantageux quand c'est possible), voire même tester que l'accuracy augmente bien en faisant un apprentissage un peu plus long :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062165ee-8ec9-4e27-ba08-052b2498d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute(ntasks=4, epochs=12, samples=1024, layers=24, hidden_dim=1024, heads=16, batch_size=32, tp=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dd781-49fb-46b7-be27-d6283a3d2c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.3.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.3.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
