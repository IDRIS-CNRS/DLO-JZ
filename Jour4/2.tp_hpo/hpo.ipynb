{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9d9624",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **TP HPO : Hyperparameter Optimization in Deep Learning**\n",
    "\n",
    "* Author : IDRIS-CNRS, Guerda.K, Hunout.L\n",
    "* Version : 2025-01\n",
    "* License : CC BY-NC-SA 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5206ac",
   "metadata": {},
   "source": [
    "## **Initial Setup and Instructions**\n",
    "This section prepares the notebook environment for our exercises and introduces key tools we'll use throughout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45606e98",
   "metadata": {},
   "source": [
    "### Auto-Reloading\n",
    "\n",
    "To ensure any changes in imported modules are automatically updated in the notebook, we enable auto-reloading. This is particularly useful during development, as it saves time that would otherwise be spent restarting the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7bfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018aec11",
   "metadata": {},
   "source": [
    "### Understanding `ToDo`\n",
    "\n",
    "Throughout this notebook, you'll encounter placeholders marked with `ToDo`. This indicates a section where you, the student, need to complete or modify the code. The `ToDo` class is designed to remind you of tasks that need to be addressed to proceed with the exercises. It's a prompt for active participation in the learning process.\n",
    "\n",
    "Whenever you see a `ToDo` in the code, it's your cue to engage with the material by writing or modifying code. This interactive approach helps reinforce learning through practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6076720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToDo:\n",
    "    def __init__(self, message=None):\n",
    "        if message is None:\n",
    "            self.message = \"This part of the code needs to be completed by the student.\"\n",
    "        else:\n",
    "            self.message = message\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print(\"TODO: Complete this part of the code.\")\n",
    "        raise NotImplementedError(self.message)\n",
    "\n",
    "    # Example of an additional method that could be useful in the future\n",
    "    def hint(self):\n",
    "        print(f\"Hint: {self.message}\")\n",
    "\n",
    "# Usage\n",
    "todo = ToDo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076bc1d",
   "metadata": {},
   "source": [
    "## **Introduction to Hyperparameter Optimization**\n",
    "\n",
    "Hyperparameter Optimization (HPO) is a critical process in developing deep learning models. It involves finding the most effective combination of hyperparameters, which are the configuration settings used to structure the learning process of a model. Unlike model parameters, which are learned during training, hyperparameters are set prior to the learning process and can significantly impact the performance of the model.\n",
    "\n",
    "### Goals of HPO\n",
    "- **Maximize Performance**: Optimize model accuracy, precision, recall, or any other relevant metric.\n",
    "- **Efficiency**: Reduce training time and computational resources without compromising performance.\n",
    "- **Generalization**: Enhance the model's ability to perform well on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3e6de",
   "metadata": {},
   "source": [
    "## Overview of HPO Frameworks\n",
    "\n",
    "In the realm of machine learning and deep learning, several frameworks are available for hyperparameter optimization (HPO). Each of these frameworks offers unique features and capabilities. Among the most notable are Optuna, Ray Tune, Hyperopt, and Scikit-Optimize. However, in this notebook, we will primarily focus on Optuna and Ray Tune due to their specific advantages and relevance in different scenarios.\n",
    "\n",
    "Optuna\n",
    "\n",
    "- **Overview**: Optuna is a versatile and user-friendly open-source optimization framework specifically tailored for machine learning. Known for its efficiency and ease of use, Optuna is particularly well-suited for individuals and teams starting their journey with HPO. \n",
    "- **Why We Focus on Optuna**: We choose Optuna for its intuitive API, efficient optimization algorithms, and excellent visualization capabilities, making it ideal for educational purposes and straightforward HPO tasks.\n",
    "- [Optuna Website](https://optuna.org/)\n",
    "\n",
    "Ray Tune\n",
    "\n",
    "- **Overview**: Ray Tune is a powerful component of the Ray ecosystem, designed for distributed hyperparameter tuning. It is especially useful for handling large-scale, computationally intensive HPO tasks.\n",
    "- **Why Ray Tune is an Interesting Choice**: We include Ray Tune due to its scalability, ability to leverage distributed computing resources effectively, and integration with various machine learning frameworks, making it suitable for more advanced, large-scale HPO scenarios.\n",
    "- [Ray Tune Website](https://docs.ray.io/en/latest/tune/)\n",
    "\n",
    "Other Notable Frameworks\n",
    "\n",
    "- **Hyperopt**: A popular tool for optimizing over awkward search spaces with real-valued, discrete, and conditional dimensions. [Hyperopt Website](http://hyperopt.github.io/hyperopt/)\n",
    "- **Scikit-Optimize**: A library for sequential model-based optimization that is built on top of Scikit-Learn. It's particularly straightforward for those already familiar with the Scikit-Learn ecosystem. [Scikit-Optimize Website](https://scikit-optimize.github.io/stable/)\n",
    "\n",
    "While there are other excellent frameworks available, Optuna and Ray Tune stand out for their distinct advantages in specific use cases. Optuna's user-friendly nature makes it an excellent teaching tool for understanding the basics and intricacies of HPO, while Ray Tune's scalability and advanced features make it a robust choice for tackling more complex, resource-intensive HPO tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a928c4",
   "metadata": {},
   "source": [
    "## Installing the Frameworks\n",
    "\n",
    "Before diving into practical examples, you may need to install Optuna and Ray Tune. These libraries can be easily installed using pip. However, they might already be installed in your environment, especially if you are using a pre-configured setup like Google Colab or a managed Jupyter environment.\n",
    "\n",
    "To check if these libraries are already installed, you can import them in your notebook. If they are not installed, you can install them using the following pip commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b19adc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna is already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import optuna\n",
    "    print(\"Optuna is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Optuna is not installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1c828",
   "metadata": {},
   "source": [
    "If you find that Optuna or Ray Tune is not installed, you can install them using the following commands:\n",
    "\n",
    "\n",
    "Remember to restart the kernel of your Jupyter notebook after installing the libraries to ensure that the changes take effect.\n",
    "\n",
    "```bash\n",
    "# To install Optuna\n",
    "!pip install optuna --user\n",
    "\n",
    "# To install Ray Tune\n",
    "!pip install ray[tune] --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d72610",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## **Synthetic Optimization with a Simple Function**\n",
    "\n",
    "To grasp the fundamentals of hyperparameter optimization (HPO) without the complexity of training models, we'll start with a synthetic example. This approach involves a straightforward function that simulates loss based on two hyperparameters and a modeâ€”either 'simple' or 'complex'. This synthetic function facilitates exploration of the HPO process efficiently.\n",
    "\n",
    "### Setting Up the Simulation\n",
    "\n",
    "Our goal with the `synthetic_loss_function` is to explore how different combinations of hyperparameters affect the loss, depending on the mode of operation. This function demonstrates the impact of hyperparameters on a model's loss landscape in a simplified manner.\n",
    "\n",
    "### Implementing the Synthetic Loss Function\n",
    "\n",
    "The task involves implementing a single `synthetic_loss_function` capable of operating in two distinct modes. This design allows us to examine different types of loss landscapes: one straightforward with a single minimum and another more intricate with multiple minima.\n",
    "\n",
    "> **Task**: Implement the `synthetic_loss_function` that behaves differently based on the `mode` parameter:\n",
    "- The `mode='simple'` loss, will be a variant of the famous [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) (a=b=1) :\n",
    "  $$\\text{f(x,y)} = (a - x)^2 + b(y - x^2)^2$$\n",
    "  \n",
    "- The `mode='complex'` loss, we will use the [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function). It's featuring multiple minima and a more dynamic landscape :\n",
    "  $$\\text{f(x,y)} = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bf8f6-7c9e-4604-8166-99fbfda02ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def synthetic_loss_function(hyperparam1, hyperparam2, mode='simple'):\n",
    "    # Implementation based on mode\n",
    "    if mode == 'simple':\n",
    "        # Calculate the loss for the simple mode\n",
    "        return todo() #Implement Rosenbrock\n",
    "    \n",
    "    elif mode == 'complex':\n",
    "        # Calculate the loss for the complex mode\n",
    "        return todo() #Implement Himmelblaus\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified. Choose either 'simple' or 'complex'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ea15a",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "Here's the completed `synthetic_loss_function`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def synthetic_loss_function(hyperparam1, hyperparam2, mode='simple'):\n",
    "    # Implementation based on mode\n",
    "    if mode == 'simple':\n",
    "        # Calculate the loss for the simple mode\n",
    "        return ((1 - hyperparam1)**2 + (hyperparam2 - hyperparam1**2)**2)\n",
    "    elif mode == 'complex':\n",
    "        # Calculate the loss for the complex mode\n",
    "        return (hyperparam1**2+hyperparam2-11)**2+(hyperparam1+hyperparam2**2-7)**2\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified. Choose either 'simple' or 'complex'.\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7100",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizing a Synthetic Loss Landscape\n",
    "\n",
    "In practice, the loss landscape of deep learning models is intricate and high-dimensional, making direct visualization challenging. For this exercise, we simplify the concept by using a synthetic loss function. This visualization serves as a conceptual tool to illustrate the effects of hyperparameter adjustments on model performance, rather than a practical approach to navigating real-world loss landscapes.\n",
    "\n",
    "This simplified exercise aims to provide insight into the optimization process in a visual and intuitive manner.\n",
    "\n",
    "> **Task**: Generate a meshgrid for parameter values and use `synthetic_loss_function` to calculate the loss. Then, create a contour plot to visualize the loss landscape. Ensure the range for parameters is broad enough to effectively visualize the landscapes for both modes (-5 to 5 both axes).\n",
    "\n",
    "\n",
    "> **Task**: Apply the `synthetic_loss_function` to calculate the loss across the meshgrid for both 'simple' and 'complex' modes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcce4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the range for both parameters using np.linspace and that accommodates both 'simple' and 'complex' modes\n",
    "hyperparam1_range = todo()\n",
    "hyperparam2_range = todo()\n",
    "\n",
    "# Create a meshgrid for the parameter values\n",
    "hyperparam1, hyperparam2 =  np.meshgrid(hyperparam1_range,hyperparam2_range)\n",
    "\n",
    "# Calculate the loss for each combination of param1 and param2 for both modes\n",
    "# Hint: Use the synthetic_loss_function you defined earlier with 'simple' and 'complex' modes\n",
    "loss_simple = todo()\n",
    "loss_complex = todo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c827b",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Define the range for both parameters using np.linspace and that accommodates both 'simple' and 'complex' modes\n",
    "hyperparam1_range = np.linspace(-5,5,100)\n",
    "hyperparam2_range = np.linspace(-5,5,100)\n",
    "\n",
    "\n",
    "# Create a meshgrid for the parameter values\n",
    "hyperparam1, hyperparam2 =  np.meshgrid(hyperparam1_range,hyperparam2_range)\n",
    "\n",
    "# Calculate the loss for each combination of param1 and param2 for both modes\n",
    "# Hint: Use the synthetic_loss_function you defined earlier with 'simple' and 'complex' modes\n",
    "loss_simple = synthetic_loss_function(hyperparam1,hyperparam2)\n",
    "loss_complex = synthetic_loss_function(hyperparam1,hyperparam2,mode='complex')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46d201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting the loss landscape for 'simple' mode\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_simple, levels=50, cmap='viridis') # add norm=colors.LogNorm() to see global minimum\n",
    "plt.colorbar()\n",
    "plt.title('Simple Mode Loss Landscape')\n",
    "plt.xlabel('Hyperparameter 1')\n",
    "plt.ylabel('Hyperparameter 2')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_simple, levels=50, cmap='viridis', norm=colors.LogNorm())\n",
    "plt.colorbar()\n",
    "\n",
    "# Plotting the loss landscape for 'complex' mode\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_complex, levels=50, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Complex Mode Loss Landscape')\n",
    "plt.xlabel('Hyperparameter 1')\n",
    "plt.ylabel('Hyperparameter 2')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.contourf(hyperparam1, hyperparam2, loss_complex, levels=50, cmap='viridis', norm=colors.LogNorm())\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc9eb5-8144-4199-86f0-7eefb8eddcd6",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## **HPO with Optuna**\n",
    "\n",
    "Hyperparameter Optimization (HPO) plays a crucial role in enhancing the performance of machine learning models by efficiently finding the best set of hyperparameters. It bridges the gap between theoretical understanding and practical application, moving beyond trial-and-error to systematic and automated search strategies. Optuna stands out in the HPO landscape, offering a user-friendly interface and efficient algorithms for exploring complex hyperparameter spaces. Its versatility makes it suitable for a wide range of applications, from tuning simple models to optimizing sophisticated deep learning architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee498f-b05a-4575-9e2e-6ba9169c92f7",
   "metadata": {},
   "source": [
    "### Define the Objective Function\n",
    "\n",
    "The cornerstone of using Optuna for HPO is the objective function. This function evaluates how well a set of hyperparameters performs against a predefined metric, typically the loss or accuracy of a model. Within Optuna, a `trial` object suggests hyperparameters, allowing the objective function to be dynamically adjusted based on the trial's performance. \n",
    "\n",
    "> **Task**: Implement an `objective` function for Optuna, designed to work with both 'simple' and 'complex' modes of a synthetic loss function. This entails defining hyperparameter ranges suitable for both modes and incorporating a mode selector. Utilize Optuna's `suggest_float` for continuous hyperparameters and `suggest_categorical` for selecting the operational mode. This exercise will introduce you to defining hyperparameter spaces and optimizing them within the Optuna framework. For detailed guidance, consult the documentation for [`suggest_float`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_float) and [`suggest_categorical`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html#optuna.trial.Trial.suggest_categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f703f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Use trial.suggest_float for hyperparam1. Think about the appropriate range.\n",
    "    hyperparam1 = todo()\n",
    "    \n",
    "    # Use trial.suggest_float for hyperparam2. Consider what range might be best.\n",
    "    hyperparam2 = todo()\n",
    "    \n",
    "    # Use trial.suggest_categorical to choose between 'simple' and 'complex' modes.\n",
    "    mode = todo()\n",
    "    \n",
    "    return synthetic_loss_function(hyperparam1, hyperparam2, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67864e27",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Use trial.suggest_float for hyperparam1. Think about the appropriate range.\n",
    "    hyperparam1 = trial.suggest_float('hyperparam1',-5,5)\n",
    "    \n",
    "    # Use trial.suggest_float for hyperparam2. Consider what range might be best.\n",
    "    hyperparam2 = trial.suggest_float('hyperparam1',-5,5)\n",
    "    \n",
    "    # Use trial.suggest_categorical to choose between 'simple' and 'complex' modes.\n",
    "    mode = trial.suggest_categorical(\"mode\",[\"simple\",\"complex\"])\n",
    "    \n",
    "    return synthetic_loss_function(hyperparam1, hyperparam2, mode)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8d315",
   "metadata": {},
   "source": [
    "### Exploring Optimization Strategies with Optuna\n",
    "\n",
    "Optuna supports a variety of hyperparameter optimization strategies, each offering distinct advantages. This section will guide you through the creation and optimization of studies using three key strategies: Grid Search, Bayesian Optimization (TPE), and Random Sampling.\n",
    "\n",
    "#### Grid Search\n",
    "> **Task**: Initialize an Optuna study with the [`GridSampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.GridSampler.html) to perform an exhaustive search over a predefined grid of hyperparameter values. This method is thorough, ensuring no potential combination is overlooked, though it may be computationally demanding.\n",
    "\n",
    "#### Bayesian Optimization (TPE)\n",
    "> **Task**: Employ the default [`TPESampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.TPESampler.html) for Bayesian Optimization. This approach uses a probabilistic model to intelligently propose hyperparameter sets, optimizing in complex, high-dimensional spaces efficiently.\n",
    "\n",
    "#### Random Sampling\n",
    "> **Task**: Utilize the [`RandomSampler`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.samplers.RandomSampler.html) for a stochastic exploration of the hyperparameter space. Random Sampling provides a baseline by selecting hyperparameters without prior assumptions, offering a chance to identify good parameters early in less complex spaces.\n",
    "\n",
    "For each strategy:\n",
    "- **Setting Up the Study**: Create an Optuna study specifying the chosen sampler. This setup defines the optimization strategy for your hyperparameter search.\n",
    "  \n",
    "- **Running the Optimization**: Optimize your study by invoking the `optimize` method with your objective function. The number of trials can be adjusted to balance thorough exploration with computational resource constraints.\n",
    "\n",
    "- **Analyzing the Results**: Upon completion, use `study.best_params` to review the most effective hyperparameters identified through each strategy, providing insights into their performance and suitability for your specific problem.\n",
    "\n",
    "This exploration provides a hands-on comparison of different optimization strategies within Optuna, demonstrating their utility and effectiveness across a range of optimization scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d69cfb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "\n",
    "# Adjust Optuna's logging level to WARN to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "random_study = todo()  # Initialize Random Sampling study\n",
    "tpe_study = todo()  # Initialize TPE study\n",
    "\n",
    "# Specify the number of trials\n",
    "n_trials = todo()\n",
    "\n",
    "# Optimize the studies\n",
    "grid_study.optimize(objective, n_trials=n_trials)\n",
    "todo()  # Optimize Random Sampling study\n",
    "todo()  # Optimize TPE study\n",
    "\n",
    "# Print the best parameters found by each study\n",
    "print(f\"Grid Search Best parameters found: {grid_study.best_params}\")\n",
    "todo()  # Print best parameters for Random Sampling study\n",
    "todo()  # Print best parameters for TPE study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ff694",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Basic Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "\n",
    "# Adjust Optuna's logging level to WARN to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Assuming 'objective' is defined elsewhere in your notebook\n",
    "# Example objective function\n",
    "def objective(trial):\n",
    "    hyperparam1 = trial.suggest_float(\"hyperparam1\", -5, 5)\n",
    "    hyperparam2 = trial.suggest_float(\"hyperparam2\", -5, 5)\n",
    "    mode = trial.suggest_categorical(\"mode\", [\"simple\", \"complex\"])\n",
    "    # Example evaluation logic\n",
    "    return (hyperparam1 - 1)**2 + (hyperparam2 - 2)**2\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "tpe_study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='TPESamplerStudy')\n",
    "random_study = optuna.create_study(direction='minimize', sampler=RandomSampler(), study_name='RandomSamplerStudy')\n",
    "\n",
    "# Number of trials\n",
    "n_trials = 100  # Specify the number of trials for demonstration\n",
    "\n",
    "# Optimize the studies\n",
    "grid_study.optimize(objective, n_trials=n_trials)\n",
    "tpe_study.optimize(objective, n_trials=n_trials)\n",
    "random_study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# Print the best parameters found by each study\n",
    "print(f\"Grid Search Best parameters found: {grid_study.best_params}\")\n",
    "print(f\"TPE Best parameters found: {tpe_study.best_params}\")\n",
    "print(f\"Random Sampling Best parameters found: {random_study.best_params}\")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>Advanced Solution with Progress Bar and less print (click to reveal)</summary>\n",
    "\n",
    "This advanced solution incorporates `tqdm` for progress tracking, providing visual feedback during the optimization process.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, GridSampler, RandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Adjust Optuna's logging level to reduce output verbosity\n",
    "logging.getLogger(\"optuna\").setLevel(logging.WARNING)\n",
    "\n",
    "# Search space for Grid Search\n",
    "grid_search_space = {\n",
    "    'hyperparam1': [-3, -1, 0, 1, 3],\n",
    "    'hyperparam2': [-3, -1, 0, 1, 3],\n",
    "    'mode': ['simple', 'complex']\n",
    "}\n",
    "\n",
    "# Initialize studies with different samplers\n",
    "grid_study = optuna.create_study(direction='minimize', sampler=GridSampler(grid_search_space), study_name='GridSearchStudy')\n",
    "tpe_study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='TPESamplerStudy')\n",
    "random_study = optuna.create_study(direction='minimize', sampler=RandomSampler(), study_name='RandomSamplerStudy')\n",
    "\n",
    "# Define a callback for the progress bar update\n",
    "def progress_bar_callback(study, trial):\n",
    "    pbar.update(1)\n",
    "\n",
    "# Number of trials\n",
    "n_trials = 50\n",
    "\n",
    "# Initialize tqdm progress bar for each study and optimize\n",
    "for study in [grid_study, tpe_study, random_study]:\n",
    "    with tqdm(total=n_trials, desc=f\"Optimizing {study.study_name}\", unit=\"trial\") as pbar:\n",
    "        study.optimize(objective, n_trials=n_trials, callbacks=[progress_bar_callback])\n",
    "        pbar.close()  # Ensure the progress bar is closed after optimization\n",
    "\n",
    "    # Print the best parameters found by each study\n",
    "    print(f\"{study.study_name} Best parameters found: {study.best_params}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb9713",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Visualizing Comparisons of Optimization Strategies with Optuna\n",
    "\n",
    "Optuna's visualization module provides a comprehensive toolkit for analyzing the outcomes of hyperparameter optimization experiments. These visual tools offer deep insights into the performance and dynamics of different optimization strategies, including Grid Search, TPE (Bayesian Optimization), and Random Sampling, highlighting the nuances of each approach in navigating the hyperparameter space.\n",
    "\n",
    "- **[`plot_parallel_coordinate`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_parallel_coordinate.html)**: Illustrates the interplay between hyperparameters and objective values, useful for comparing the influence of parameters across optimization strategies.\n",
    "\n",
    "- **[`plot_optimization_history`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_optimization_history.html)**: Chronicles the progression of objective value improvements across trials, offering insights into the efficiency and exploration depth of each strategy.\n",
    "\n",
    "- **[`plot_param_importances`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_param_importances.html)**: Highlights which hyperparameters significantly impact the objective value, aiding in strategy refinement for future optimizations.\n",
    "\n",
    "- **[`plot_contour`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_contour.html)**: Explores the synergy between pairs of hyperparameters and their collective effect on the objective, facilitating the identification of optimal parameter interactions.\n",
    "\n",
    "- **[`plot_edf`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_edf.html)**: Displays the empirical distribution of objective values across all trials, providing a macroscopic view of the optimization landscape influenced by different sampling strategies.\n",
    "\n",
    "- **[`plot_intermediate_values`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_intermediate_values.html)**: Reveals the trajectory of intermediate objective values, shedding light on the iterative progress within trials.\n",
    "\n",
    "- **[`plot_slice`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_slice.html)**: Demystifies how variations in individual hyperparameters impact the objective, highlighting parameter sensitivity.\n",
    "\n",
    "- **[`plot_timeline`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_timeline.html)**: Visualizes the timeline of trial execution, underscoring the operational efficiency and parallelization capabilities of each strategy.\n",
    "\n",
    "- **[`plot_rank`](https://optuna.readthedocs.io/en/stable/reference/visualization/generated/optuna.visualization.plot_rank.html)**: Assesses the comparative performance of trials, accentuating the effectiveness of different optimization strategies.\n",
    "\n",
    "> **Task**: Examine the visualizations to discern the distinct characteristics and efficacy of **Grid Search**, **Random Sampling** , and **TPE** strategies. Utilize insights from plots like hyperparameter importance and optimization history to devise nuanced future tuning approaches and enhance model optimization outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f2f9c-d656-4d08-b9b7-2326cc4b9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import optuna.visualization as vis\n",
    "from plotly.io import to_html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_optuna_visualizations(study):\n",
    "    \"\"\"\n",
    "    Display a \"beautiful\" series of Optuna visualization plots for a given study.\n",
    "\n",
    "    Parameters:\n",
    "    - study: The Optuna study to visualize.\n",
    "    \"\"\"\n",
    "    # List of visualization functions to display, with a brief description as a comment\n",
    "    visualization_functions = [\n",
    "        vis.plot_parallel_coordinate,  # Hyperparameter Relationship Plot\n",
    "        vis.plot_optimization_history, # Optimization History\n",
    "        vis.plot_param_importances,    # Hyperparameter Importance\n",
    "        vis.plot_contour,              # Contour Plot of Parameter Interactions\n",
    "        vis.plot_edf,                  # EDF (Empirical Distribution Function)\n",
    "        vis.plot_intermediate_values,  # Intermediate Values of All Trials\n",
    "        vis.plot_slice,                # Slice Plot in a Study\n",
    "        vis.plot_timeline,             # Timeline of a Study\n",
    "        vis.plot_rank                  # Rank Plot\n",
    "    ]\n",
    "    grid_html = \"<div style='display: grid; grid-template-columns: repeat( 3 , 1fr); gap: 10px;'>\"\n",
    "    for viz in visualization_functions:\n",
    "        content = to_html(viz(study).update_layout(autosize=True), full_html=False, include_plotlyjs='cdn')\n",
    "        grid_html += \"<div>\" + content + \"</div>\"\n",
    "    grid_html += \"</div>\"\n",
    "    display(HTML(grid_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78d2f6-bd80-461f-a689-53da8188656b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>WebGL Fix Firefox (click to reveal)</summary>\n",
    "\n",
    "You can have a WebGL error on the training computer, they don't have any gpu.\n",
    "Force WebGL on CPU :\n",
    "* go to about:config\n",
    "* search for webgl.force-enabled and make sure this preference is set to true\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b7a2ce-cdae-473a-8ff7-df730f4aad2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Alternative with matplotlib (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "import sys, io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna.visualization.matplotlib as vis #note .matplotlib\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def display_optuna_visualizations(study):\n",
    "    \"\"\"\n",
    "    Display a series of Optuna visualization plots for a given study.\n",
    "\n",
    "    Parameters:\n",
    "    - study: The Optuna study to visualize.\n",
    "    \"\"\"\n",
    "    # List of visualization functions to display, with a brief description as a comment\n",
    "    visualization_functions = [\n",
    "        vis.plot_parallel_coordinate,  # Hyperparameter Relationship Plot\n",
    "        vis.plot_optimization_history, # Optimization History\n",
    "        vis.plot_param_importances,    # Hyperparameter Importance\n",
    "        vis.plot_contour,              # Contour Plot of Parameter Interactions\n",
    "        vis.plot_edf,                  # EDF (Empirical Distribution Function)\n",
    "        vis.plot_intermediate_values,  # Intermediate Values of All Trials\n",
    "        vis.plot_slice,                # Slice Plot in a Study\n",
    "        vis.plot_timeline,             # Timeline of a Study\n",
    "        vis.plot_rank                  # Rank Plot\n",
    "    ]\n",
    "    \n",
    "    # Create a figure and axes for the grid of plots\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(25, 20))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Redirect stdout to a buffer\n",
    "    stdout_buffer = io.StringIO()\n",
    "    cell_stdout = sys.stdout\n",
    "    sys.stdout = stdout_buffer\n",
    "\n",
    "    # Call your plotting functions\n",
    "    for i, ax in enumerate(axs):\n",
    "        visualization_functions[i](study)\n",
    "\n",
    "        # Capture the plot generated by the function\n",
    "        captured_fig = plt.gcf()\n",
    "        captured_ax = plt.gca()\n",
    "        # Close the current figure to clear the plot\n",
    "        plt.close(captured_fig)\n",
    "\n",
    "        # Convert the captured plot to an image array\n",
    "        buffer_ = io.BytesIO()\n",
    "        captured_fig.savefig(buffer_, format='png', bbox_inches='tight')\n",
    "        buffer_.seek(0)\n",
    "        image = Image.open(buffer_)\n",
    "        image= image.crop((0, 0, image.width, min(image.width,image.height)))\n",
    "\n",
    "        # Plot the captured plot->image onto the current subplot\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    sys.stdout = cell_stdout \n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f4798",
   "metadata": {},
   "source": [
    "#### Grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(grid_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c751c6e",
   "metadata": {},
   "source": [
    "#### Random search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c167a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(random_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f549f5",
   "metadata": {},
   "source": [
    "#### TPE search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(tpe_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de77833",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Leveraging Optuna Pruners for Efficient Optimization\n",
    "\n",
    "Optuna pruners enhance optimization by terminating unpromising trials early, conserving valuable computational resources.\n",
    "#### Pruners in Action\n",
    "\n",
    "- **[`SuccessiveHalvingPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html)**: An implementation of **ASHA**. This pruner evaluates trials at intervals, halving less promising ones, focusing resources on those with the most short potential.\n",
    "- **[`HyperbandPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyperbandPruner.html)**: An implementation of **BOHB**. This pruner evaluate trials at different step intervals, halving less promissing ones, focusing resources on those with the most potential.\n",
    "\n",
    "#### Objective Function Adjustments for Pruning\n",
    "\n",
    "To utilize pruning:\n",
    "- **Intermediate Reporting**: The objective function must periodically report interim results using `trial.report()`.\n",
    "- **Checkpoints for Pruning**: Incorporate `trial.should_prune()` checks after reporting, halting trials early with `optuna.TrialPruned()` if deemed non-promising.\n",
    "\n",
    "#### Why Adapt Our Synthetic Function\n",
    "\n",
    "Adjusting our synthetic function to report intermediate results allows pruners to make informed decisions on trial continuation. This step is crucial for pruning to effectively reduce computational load and improve the optimization process's overall efficiency.\n",
    "\n",
    "Pruning with Optuna signifies a strategic layer to hyperparameter optimization, ensuring a smarter allocation of computational effort towards the most promising trials.\n",
    "\n",
    "> **Note**: Effective use of pruners like `SuccessiveHalvingPruner` or `HyberbandPruner` necessitates modifications for intermediate evaluations within the objective function, enabling a dynamic and resource-efficient optimization workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def objective_with_pruning(trial):\n",
    "    # Define hyperparameters\n",
    "    hyperparam1 = trial.suggest_float(\"hyperparam1\", -5, 5)\n",
    "    hyperparam2 = trial.suggest_float(\"hyperparam2\", -5, 5)\n",
    "    mode = trial.suggest_categorical(\"mode\", [\"simple\", \"complex\"])\n",
    "    \n",
    "    # Simulate a step-wise evaluation process\n",
    "    accumulated_loss = 0\n",
    "    steps = 10 \n",
    "    best_loss = float('+inf')\n",
    "    for step in range(1, steps + 1):\n",
    "        \n",
    "        end_loss = synthetic_loss_function(hyperparam1, hyperparam2 , mode)\n",
    "        intermediate_loss =  end_loss * (1.1-np.tanh(step/3-1))\n",
    "        \n",
    "        if intermediate_loss < best_loss : best_loss = intermediate_loss\n",
    "        \n",
    "        # Report intermediate objective value\n",
    "        trial.report(intermediate_loss, step)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac465be5",
   "metadata": {},
   "source": [
    "> **Task**: Create and optimize an Optuna study with the [`SuccessiveHalvingPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.SuccessiveHalvingPruner.html). Initialize the study to minimize the objective, using `objective_with_pruning`. Observe the pruning effect on trial completions and optimization efficiency. Analyze the best parameters found. Refer to Optuna's documentation on [creating a study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.create_study.html) and [optimizing it](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize) for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c46bc6-0024-4f97-a427-04de9047cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "asha_pruner = todo()\n",
    "\n",
    "asha_study = optuna.create_study(direction='minimize', \n",
    "                                 pruner=asha_pruner)\n",
    "\n",
    "start_asha = time.time()\n",
    "asha_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_asha = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf7b98-428a-427d-b74f-25681972f923",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "asha_pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource='auto', \n",
    "                                                     reduction_factor=4,  \n",
    "                                                     min_early_stopping_rate=0, \n",
    "                                                     bootstrap_count=0)\n",
    "\n",
    "asha_study = optuna.create_study(direction='minimize', \n",
    "                                 pruner=asha_pruner)\n",
    "\n",
    "start_asha = time.time()\n",
    "asha_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_asha = time.time()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659734ea-021e-4399-b4ec-5c14f80d7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ASHA,\\n Best result : {asha_study.best_trial.value}\\n Best config : {asha_study.best_params}, in {end_asha-start_asha}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e954da-4eee-489b-8a00-8e874b7aa72f",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Task**: Create and optimize an Optuna study with the [`HyberbandPruner`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.HyberbandPruner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35bae6-b749-4ccc-883e-4efcc8a03ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bohb_pruner = todo()\n",
    "\n",
    "bohb_study = optuna.create_study(direction=\"minimize\",\n",
    "                                 pruner=bohb_pruner)\n",
    "\n",
    "start_bohb = time.time()\n",
    "bohb_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_bohb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b1d05-6588-41c1-8076-fe5dbbf5b581",
   "metadata": {
    "tags": []
   },
   "source": [
    "<details>\n",
    "<summary>Solution (click to reveal)</summary>\n",
    "\n",
    "This basic solution focuses on setting up and running the optimization without additional libraries for progress tracking.\n",
    "\n",
    "```python\n",
    "bohb_pruner = optuna.pruners.HyperbandPruner(min_resource=1, \n",
    "                                             max_resource=\"auto\", \n",
    "                                             reduction_factor=3)\n",
    "\n",
    "bohb_study = optuna.create_study(direction=\"minimize\",\n",
    "                                 pruner=bohb_pruner)\n",
    "\n",
    "start_bohb = time.time()\n",
    "bohb_study.optimize(objective_with_pruning, n_trials=400)\n",
    "end_bohb = time.time()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f71d3-3bac-45e5-96f1-e1d56a561c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BOHB,\\n Best result : {bohb_study.best_trial.value}\\n Best config : {bohb_study.best_params}, in {end_bohb-start_bohb}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaf1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_optuna_visualizations(asha_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bd389-9ef3-4bd7-a9c9-7a24d2bca6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_optuna_visualizations(bohb_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af1bd7-854e-467a-bf7d-4ce4b0a1569a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6076c2-a0d7-485c-8ea9-8118d30a8257",
   "metadata": {
    "tags": []
   },
   "source": [
    "Optuna distributed throught Dask (Bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91cd0c6-ea40-4a68-acf1-5b76cb67050d",
   "metadata": {},
   "source": [
    "https://medium.com/optuna/running-distributed-hyperparameter-optimization-with-optuna-distributed-17bb2f7d422d\n",
    "\n",
    "https://github.com/xadrianzetx/optuna-distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75d156b-40ea-4875-9ba3-67d7ea710532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "import optuna_distributed\n",
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c6a4f2-7e1e-467e-844e-6c9aec4321bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -100, 100)\n",
    "    y = trial.suggest_categorical(\"y\", [-1, 0, 1])\n",
    "    # Some expensive model fit happens here...\n",
    "    time.sleep(1)\n",
    "        \n",
    "    return x**2 + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6ca8656-4dc6-4a9f-b735-5723a08a8cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://127.0.0.1:38251' processes=10 threads=10, memory=18.63 GiB>\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(\n",
    "    n_workers=8,\n",
    "    threads_per_worker=1,\n",
    "    processes=True,\n",
    "    memory_limit='2GB'\n",
    ")\n",
    "client = Client(cluster)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089b94e3-f891-44ce-b385-81c402fe2d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 16:06:53,457] A new study created in memory with name: no-name-eb51f070-afec-4bb8-85fc-1e1dff80a246\n",
      "[I 2025-01-13 16:06:54,464] Trial 0 finished with value: 5239.596731374947 and parameters: {'x': -72.38505875783308, 'y': 0}. Best is trial 0 with value: 5239.596731374947.\n",
      "[I 2025-01-13 16:06:55,465] Trial 1 finished with value: 504.24511607924745 and parameters: {'x': 22.433125419326828, 'y': 1}. Best is trial 1 with value: 504.24511607924745.\n",
      "[I 2025-01-13 16:06:56,465] Trial 2 finished with value: 5796.7259480477005 and parameters: {'x': -76.14279971243309, 'y': -1}. Best is trial 1 with value: 504.24511607924745.\n",
      "[I 2025-01-13 16:06:57,466] Trial 3 finished with value: 411.3364443883128 and parameters: {'x': -20.256762929656674, 'y': 1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:06:58,467] Trial 4 finished with value: 1368.5712197037524 and parameters: {'x': -37.007718380140005, 'y': -1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:06:59,467] Trial 5 finished with value: 5141.741642068896 and parameters: {'x': 71.71291126477084, 'y': -1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:07:00,468] Trial 6 finished with value: 1684.1854682531077 and parameters: {'x': 41.05101056311656, 'y': -1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:07:01,469] Trial 7 finished with value: 3766.0434376196513 and parameters: {'x': 61.37624489669966, 'y': -1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:07:02,469] Trial 8 finished with value: 1308.4305611057957 and parameters: {'x': -36.15840927233658, 'y': 1}. Best is trial 3 with value: 411.3364443883128.\n",
      "[I 2025-01-13 16:07:03,470] Trial 9 finished with value: 20.338064096274582 and parameters: {'x': -4.50977428440433, 'y': 0}. Best is trial 9 with value: 20.338064096274582.\n",
      "[I 2025-01-13 16:07:04,481] Trial 10 finished with value: 9740.61947217651 and parameters: {'x': 98.69457671106609, 'y': 0}. Best is trial 9 with value: 20.338064096274582.\n",
      "[I 2025-01-13 16:07:05,485] Trial 11 finished with value: 287.7586809132798 and parameters: {'x': -16.963451326698817, 'y': 0}. Best is trial 9 with value: 20.338064096274582.\n",
      "[I 2025-01-13 16:07:06,489] Trial 12 finished with value: 3.6051395724996422 and parameters: {'x': 1.8987205093166404, 'y': 0}. Best is trial 12 with value: 3.6051395724996422.\n",
      "[I 2025-01-13 16:07:07,493] Trial 13 finished with value: 547.2787100503937 and parameters: {'x': 23.39398875887551, 'y': 0}. Best is trial 12 with value: 3.6051395724996422.\n",
      "[I 2025-01-13 16:07:08,497] Trial 14 finished with value: 26.362792025729235 and parameters: {'x': 5.134470958699565, 'y': 0}. Best is trial 12 with value: 3.6051395724996422.\n",
      "[I 2025-01-13 16:07:09,501] Trial 15 finished with value: 2397.398155235309 and parameters: {'x': -48.963232687755706, 'y': 0}. Best is trial 12 with value: 3.6051395724996422.\n",
      "[I 2025-01-13 16:07:10,505] Trial 16 finished with value: 0.6900868035069891 and parameters: {'x': 0.8307146342198319, 'y': 0}. Best is trial 16 with value: 0.6900868035069891.\n",
      "[I 2025-01-13 16:07:11,509] Trial 17 finished with value: 751.9701580366335 and parameters: {'x': 27.422074283989414, 'y': 0}. Best is trial 16 with value: 0.6900868035069891.\n",
      "[I 2025-01-13 16:07:12,513] Trial 18 finished with value: 2.0482170215545703 and parameters: {'x': 1.4311593278019643, 'y': 0}. Best is trial 16 with value: 0.6900868035069891.\n",
      "[I 2025-01-13 16:07:13,517] Trial 19 finished with value: 8407.185089189159 and parameters: {'x': -91.69070339565053, 'y': 0}. Best is trial 16 with value: 0.6900868035069891.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "\n",
    "sequential_start = time.time()\n",
    "study.optimize(objective, n_trials=20)\n",
    "sequential_duration = time.time() - sequential_start\n",
    "sequential_best_value = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f91015-8600-4509-b2e3-0e5b18e950ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:07:14] INFO     Trial 20 finished with value: 2640.369620710627 and         \n",
      "                    parameters: {'x': 51.374795578285536, 'y': 1}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 21 finished with value: 3286.3254036439535 and        \n",
      "                    parameters: {'x': 57.31775818752818, 'y': 1}. Best is trial \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:15] INFO     Trial 22 finished with value: 13.21832292274366 and         \n",
      "                    parameters: {'x': 3.635701159713716, 'y': 0}. Best is trial \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 23 finished with value: 4.870517480030508 and         \n",
      "                    parameters: {'x': 2.2069248922495093, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:16] INFO     Trial 24 finished with value: 361.6361833276242 and         \n",
      "                    parameters: {'x': -19.016734297129574, 'y': 0}. Best is     \n",
      "                    trial 16 with value: 0.6900868035069891.                    \n",
      "           INFO     Trial 25 finished with value: 228.25312512918921 and        \n",
      "                    parameters: {'x': -15.108048356064698, 'y': 0}. Best is     \n",
      "                    trial 16 with value: 0.6900868035069891.                    \n",
      "[16:07:17] INFO     Trial 26 finished with value: 354.4804649865027 and         \n",
      "                    parameters: {'x': 18.8276516057235, 'y': 0}. Best is trial  \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 27 finished with value: 1251.0683679870692 and        \n",
      "                    parameters: {'x': 35.370444837280026, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:18] INFO     Trial 28 finished with value: 2770.9965950755163 and        \n",
      "                    parameters: {'x': -52.64025641156696, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 29 finished with value: 2154.203831476001 and         \n",
      "                    parameters: {'x': -46.41340142109821, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:19] INFO     Trial 30 finished with value: 125.26365350030446 and        \n",
      "                    parameters: {'x': 11.192124619584275, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 31 finished with value: 239.25743655095002 and        \n",
      "                    parameters: {'x': 15.46794868594249, 'y': 0}. Best is trial \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:20] INFO     Trial 32 finished with value: 33.26144885829924 and         \n",
      "                    parameters: {'x': -5.7672739538103475, 'y': 0}. Best is     \n",
      "                    trial 16 with value: 0.6900868035069891.                    \n",
      "           INFO     Trial 33 finished with value: 32.96961580228301 and         \n",
      "                    parameters: {'x': -5.741917432555349, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:21] INFO     Trial 34 finished with value: 802.5410800501687 and         \n",
      "                    parameters: {'x': -28.32915600666862, 'y': 0}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 35 finished with value: 871.7413747706053 and         \n",
      "                    parameters: {'x': -29.525266718026533, 'y': 0}. Best is     \n",
      "                    trial 16 with value: 0.6900868035069891.                    \n",
      "[16:07:22] INFO     Trial 36 finished with value: 1236.4718157279833 and        \n",
      "                    parameters: {'x': 35.14927902145339, 'y': 1}. Best is trial \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 37 finished with value: 951.7076943232148 and         \n",
      "                    parameters: {'x': 30.83354819548368, 'y': 1}. Best is trial \n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "[16:07:23] INFO     Trial 38 finished with value: 77.66544608915297 and         \n",
      "                    parameters: {'x': 8.869354321998472, 'y': -1}. Best is trial\n",
      "                    16 with value: 0.6900868035069891.                          \n",
      "           INFO     Trial 39 finished with value: 3853.5975677859446 and        \n",
      "                    parameters: {'x': -62.08540543304799, 'y': -1}. Best is     \n",
      "                    trial 16 with value: 0.6900868035069891.                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "distributed_study = optuna_distributed.from_study(study, client=client)\n",
    "\n",
    "distributed_start = time.time()\n",
    "distributed_study.optimize(objective, n_trials=20, n_jobs=2)\n",
    "distributed_duration = time.time() - distributed_start\n",
    "distributed_best_value = distributed_study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8e8e9f-3a97-4ecf-9432-7394913a8fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential optimization took 20.06 seconds with best value of 0.6901\n",
      "Asynchronous optimization took 10.21 seconds with best value of 0.6901\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sequential optimization took {sequential_duration:.2f} seconds with best value of {sequential_best_value:.4f}\")\n",
    "print(f\"Asynchronous optimization took {distributed_duration:.2f} seconds with best value of {distributed_best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8dafe-12e9-4df4-9043-bb00f7c4719b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Optuna distributed throught database (Bonus)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f36c1-b6d0-4439-9269-1908681f5841",
   "metadata": {},
   "source": [
    "We create a new database with **SQLite**, easy to use, possible to use but bad performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595ce8e-743f-48c5-848a-85c3d9d31934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sqlite3 optuna_dist.db \".databases\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af93c1-eb6e-44ce-86b5-a49e3faefe02",
   "metadata": {},
   "source": [
    "Now, we have to create a new study to initialize our trials database. Here we are using cmdline, we can do it directly with python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bb36b-fa47-419d-a5d0-491663cef3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!optuna create-study --study-name \"distributed-example\" --storage \"sqlite:///optuna_dist.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e29fd9-2a99-451e-a2e1-d9b0608c63f3",
   "metadata": {},
   "source": [
    "For this demonstration, we use a toy (and useless XD) HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaae7b8-6c57-4e33-9562-649cbe5c02f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile optuna_worker.py\n",
    "import optuna\n",
    "import time\n",
    "import random\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -10, 10)\n",
    "    time.sleep(random.uniform(1, 3))\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.load_study(\n",
    "        study_name=\"distributed-example\", storage=\"sqlite:///optuna_dist.db\"\n",
    "    )\n",
    "    start = time.time()\n",
    "    study.optimize(objective, n_trials=60)\n",
    "    print(f\"process run for {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaaa87-f530-4596-a6ce-afaee2a97297",
   "metadata": {},
   "source": [
    "We have everythings to run multiple process for our HPO. This line generate 4 processes which run the same HPO code. Each output color line mean a different process. \n",
    "\n",
    "*Note that we are going to do 4x60 = 240 trials here, each process doesn't know about the others.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa1b2a-1d32-4870-8023-ece54a59d184",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!seq 4 | xargs -I{} -P 4 sh -c 'python optuna_worker.py 2>&1 | while read -r line; do if [[ \"$line\" == *[* ]]; then echo -e \"\\033[3{}m${line}\\033[0m\"; else echo \"$line\"; fi; done'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864feec8-fa05-4d5a-82e9-cf5399f6daa6",
   "metadata": {},
   "source": [
    "Since we used a sqlite database for this part, we can visualize the HPO with this website : https://optuna.github.io/optuna-dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee66c5-d992-4ecd-ae56-f7944def6558",
   "metadata": {
    "tags": []
   },
   "source": [
    "Optuna Visualisation time ! (doesn't work right now on jupyterhub)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772fa71-1aba-4924-acce-cb920d2c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!~/.local/bin/optuna-dashboard --port=\"8080\"  --server='wsgiref' sqlite:///optuna_dist.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a888e83-b65a-4c69-be86-10cb41cae95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#f'jupyterhub.idris.fr{os.environ[\"JUPYTERHUB_SERVICE_PREFIX\"]}proxy/8080'\n",
    "import optuna_dashboard\n",
    "optuna_dashboard.run_server(storage=\"sqlite:///optuna_dist.db\", host=f'jupyterhub.idris.fr{os.environ[\"JUPYTERHUB_SERVICE_PREFIX\"]}proxy/8080')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7370f-2d8a-4916-a83a-22248b31e99b",
   "metadata": {},
   "source": [
    "usage with InMemoryStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cfc8e-b576-453c-9991-83b56dcc9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna_dashboard import run_server\n",
    "\n",
    "def objective(trial):\n",
    "    x = trial.suggest_float(\"x\", -100, 100)\n",
    "    y = trial.suggest_categorical(\"y\", [-1, 0, 1])\n",
    "    return x**2 + y\n",
    "\n",
    "storage = optuna.storages.InMemoryStorage()\n",
    "study = optuna.create_study(storage=storage)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "run_server(storage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.3.0_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.3.0_py3.11.5"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
